"use strict";(globalThis.webpackChunkai_native_textbook_docusaurus=globalThis.webpackChunkai_native_textbook_docusaurus||[]).push([[521],{996:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"part-3-simulation/chapter-11-isaac-sim-platform","title":"NVIDIA Isaac Sim Platform","description":"11.1 Isaac Sim Architecture Overview","source":"@site/docs/part-3-simulation/chapter-11-isaac-sim-platform.mdx","sourceDirName":"part-3-simulation","slug":"/part-3-simulation/chapter-11-isaac-sim-platform","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-11-isaac-sim-platform","draft":false,"unlisted":false,"editUrl":"https://github.com/NaumanNavaid/ai-native-textbook-docusaurus/tree/main/docs/part-3-simulation/chapter-11-isaac-sim-platform.mdx","tags":[],"version":"current","frontMatter":{"title":"NVIDIA Isaac Sim Platform","part":3,"chapter":11,"difficulty":"advanced","prerequisites":["chapter-10-physics-simulations","chapter-7-gazebo-physics-simulation"],"estimatedTime":60,"objectives":["Master NVIDIA Isaac Sim architecture and capabilities","Implement photorealistic simulation with RTX rendering","Develop synthetic data generation pipelines","Create domain randomization for robust AI training"]},"sidebar":"chaptersSidebar","previous":{"title":"Physics Simulations for Robotics","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-10-physics-simulations"},"next":{"title":"Digital Twin Development","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-12-digital-twin-development"}}');var t=r(4848),i=r(8453);const s={title:"NVIDIA Isaac Sim Platform",part:3,chapter:11,difficulty:"advanced",prerequisites:["chapter-10-physics-simulations","chapter-7-gazebo-physics-simulation"],estimatedTime:60,objectives:["Master NVIDIA Isaac Sim architecture and capabilities","Implement photorealistic simulation with RTX rendering","Develop synthetic data generation pipelines","Create domain randomization for robust AI training"]},o="Chapter 11: NVIDIA Isaac Sim Platform",l={},c=[{value:"11.1 Isaac Sim Architecture Overview",id:"111-isaac-sim-architecture-overview",level:2},{value:"11.1.1 Introduction to Isaac Sim",id:"1111-introduction-to-isaac-sim",level:3},{value:"11.1.2 Core Platform Components",id:"1112-core-platform-components",level:3},{value:"Omniverse Foundation",id:"omniverse-foundation",level:4},{value:"Physics Engine Integration",id:"physics-engine-integration",level:4},{value:"11.2 Advanced Rendering and Visual Fidelity",id:"112-advanced-rendering-and-visual-fidelity",level:2},{value:"11.2.1 RTX Path Tracing",id:"1121-rtx-path-tracing",level:3},{value:"11.2.2 Material Definition Language (MDL)",id:"1122-material-definition-language-mdl",level:3},{value:"11.3 Synthetic Data Generation",id:"113-synthetic-data-generation",level:2},{value:"11.3.1 Domain Randomization",id:"1131-domain-randomization",level:3},{value:"11.3.2 Automated Data Capture Pipeline",id:"1132-automated-data-capture-pipeline",level:3},{value:"11.4 AI Training Integration",id:"114-ai-training-integration",level:2},{value:"11.4.1 Ground Truth Data Generation",id:"1141-ground-truth-data-generation",level:3},{value:"11.4.2 Training Pipeline Integration",id:"1142-training-pipeline-integration",level:3},{value:"11.5 Advanced Isaac Sim Features",id:"115-advanced-isaac-sim-features",level:2},{value:"11.5.1 Distributed Simulation",id:"1151-distributed-simulation",level:3},{value:"11.5.2 Cloud Integration",id:"1152-cloud-integration",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:3},{value:"Practical Implementations",id:"practical-implementations",level:3},{value:"Next Steps",id:"next-steps",level:3},{value:"Glossary Terms",id:"glossary-terms",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 11.1: RTX Scene Setup",id:"exercise-111-rtx-scene-setup",level:3},{value:"Exercise 11.2: Domain Randomization Pipeline",id:"exercise-112-domain-randomization-pipeline",level:3},{value:"Exercise 11.3: Synthetic Data Generation",id:"exercise-113-synthetic-data-generation",level:3},{value:"Exercise 11.4: Cloud Deployment",id:"exercise-114-cloud-deployment",level:3},{value:"Exercise 11.5: Digital Twin Integration",id:"exercise-115-digital-twin-integration",level:3}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-11-nvidia-isaac-sim-platform",children:"Chapter 11: NVIDIA Isaac Sim Platform"})}),"\n",(0,t.jsx)(n.h2,{id:"111-isaac-sim-architecture-overview",children:"11.1 Isaac Sim Architecture Overview"}),"\n",(0,t.jsx)(n.h3,{id:"1111-introduction-to-isaac-sim",children:"11.1.1 Introduction to Isaac Sim"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac Sim represents the cutting edge of robotics simulation, built on NVIDIA's Omniverse platform. It combines physically accurate simulation with photorealistic rendering to create digital twins that bridge the gap between virtual testing and real-world deployment."}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"Isaac Sim leverages NVIDIA's RTX technology for real-time ray tracing, enabling unprecedented visual fidelity in robotics simulations. This makes it particularly valuable for computer vision applications where visual realism directly impacts training effectiveness."})}),"\n",(0,t.jsx)(n.h3,{id:"1112-core-platform-components",children:"11.1.2 Core Platform Components"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim's architecture consists of several interconnected systems:"}),"\n",(0,t.jsx)(n.h4,{id:"omniverse-foundation",children:"Omniverse Foundation"}),"\n",(0,t.jsx)(n.p,{children:"The Omniverse platform provides the foundation with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"USD (Universal Scene Description)"})," for scene composition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MDL (Material Definition Language)"})," for physically-based materials"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nucleus"})," for collaborative data management"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kit SDK"})," for application development"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Isaac Sim Python API Integration\r\nimport asyncio\r\nfrom omni.isaac.kit import SimulationApp\r\n\r\n# Initialize Isaac Sim\r\nsimulation_app = SimulationApp({\r\n    "headless": False,  # Set to True for headless mode\r\n    "width": 1280,\r\n    "height": 720,\r\n    "renderer": "RayTracedLighting"  # RTX rendering\r\n})\r\n\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.objects import DynamicSphere\r\nfrom omni.isaac.core.materials import PreviewSurface\r\n\r\nclass IsaacSimEnvironment:\r\n    def __init__(self):\r\n        self.world = World()\r\n        self.objects = {}\r\n        self.materials = {}\r\n\r\n    async def initialize(self):\r\n        """Initialize the simulation environment"""\r\n        await self.world.initialize_simulation_async()\r\n        self._setup_lighting()\r\n        self._create_materials()\r\n\r\n    def _setup_lighting(self):\r\n        """Configure realistic lighting for the scene"""\r\n        from omni.isaac.core import Light\r\n\r\n        # Dome light for global illumination\r\n        dome_light = self.world.scene.add(\r\n            Light(\r\n                prim_path="/World/DomeLight",\r\n                light_type="dome",\r\n                intensity=1000,\r\n                color=(1.0, 1.0, 1.0, 1.0),\r\n                texture_file="https://assets.omniverse.nvidia.com/EnvHDR/Environments/studio_small_01_4k.hdr"\r\n            )\r\n        )\r\n\r\n        # Directional light for shadows\r\n        directional_light = self.world.scene.add(\r\n            Light(\r\n                prim_path="/World/DirectionalLight",\r\n                light_type="distant",\r\n                intensity=5000,\r\n                color=(1.0, 0.95, 0.8, 1.0),\r\n                rotation=(60, 0, 0)\r\n            )\r\n        )\r\n\r\n    def _create_materials(self):\r\n        """Create physically-based materials"""\r\n        # Metallic material\r\n        self.materials["metal"] = PreviewSurface(\r\n            prim_path="/World/Materials/Metal",\r\n            metallic=1.0,\r\n            roughness=0.3,\r\n            base_color=(0.7, 0.7, 0.8, 1.0)\r\n        )\r\n\r\n        # Plastic material\r\n        self.materials["plastic"] = PreviewSurface(\r\n            prim_path="/World/Materials/Plastic",\r\n            metallic=0.0,\r\n            roughness=0.7,\r\n            base_color=(0.2, 0.4, 0.8, 1.0)\r\n        )\r\n\r\n        # Glass material\r\n        self.materials["glass"] = PreviewSurface(\r\n            prim_path="/World/Materials/Glass",\r\n            metallic=0.0,\r\n            roughness=0.0,\r\n            transmission=1.0,\r\n            base_color=(0.9, 0.95, 1.0, 1.0)\r\n        )\n'})}),"\n",(0,t.jsx)(n.h4,{id:"physics-engine-integration",children:"Physics Engine Integration"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim integrates multiple physics engines through PhysX:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Configure physics simulation\r\nfrom omni.isaac.core.physics_context import PhysicsContext\r\n\r\nclass IsaacPhysicsConfig:\r\n    def __init__(self):\r\n        self.physics_context = PhysicsContext(\r\n            prim_path="/World/physicsContext",\r\n            gravity=(0.0, -9.81, 0.0),\r\n            enable_gpu_dynamics=True,  # GPU acceleration\r\n            num_threads=8,\r\n            solver_type="TGS",  # Temporal Gauss-Seidel\r\n            max_position_iterations=50,\r\n            max_velocity_iterations=10\r\n        )\r\n\r\n    def configure_advanced_physics(self):\r\n        """Configure advanced physics features"""\r\n        # Enable contact reporting\r\n        self.physics_context.enable_ccd(True)  # Continuous collision detection\r\n        self.physics_context.enable_stabilization(True)\r\n\r\n        # Configure friction model\r\n        self.physics_context.set_friction_model("patch")\r\n        self.physics_context.set_restitution_threshold(2.0)\r\n\r\n        # Enable advanced features\r\n        self.physics_context.enable_enhanced_determinism(True)\r\n        self.physics_context.set_bounce_threshold_velocity(0.2)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"112-advanced-rendering-and-visual-fidelity",children:"11.2 Advanced Rendering and Visual Fidelity"}),"\n",(0,t.jsx)(n.h3,{id:"1121-rtx-path-tracing",children:"11.2.1 RTX Path Tracing"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim's RTX renderer enables physically accurate light transport:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class RTXConfiguration:\r\n    def __init__(self):\r\n        self.settings = {\r\n            "renderer": "RayTracedLighting",\r\n            "samples_per_pixel": 256,\r\n            "max_bounces": 8,\r\n            "max_ray_depth": 64,\r\n            "enable_dlss": True,  # NVIDIA DLSS upscaling\r\n            "dlss_quality": "Performance",\r\n            "enable_reflections": True,\r\n            "enable_transparent_refractions": True,\r\n            "enable_subsurface_scattering": True\r\n        }\r\n\r\n    def apply_settings(self):\r\n        """Apply RTX rendering settings"""\r\n        import carb.settings\r\n\r\n        settings = carb.settings.get_settings()\r\n\r\n        # Core RTX settings\r\n        settings.set("/rtx/raytracing/spp", self.settings["samples_per_pixel"])\r\n        settings.set("/rtx/raytracing/maxBounces", self.settings["max_bounces"])\r\n        settings.set("/rtx/raytracing/maxDepth", self.settings["max_ray_depth"])\r\n\r\n        # DLSS configuration\r\n        settings.set("/rtx/dlss/execMode", "performance:dlss")\r\n        settings.set("/rtx/dlss/optLevel", self.settings["dlss_quality"])\r\n\r\n        # Advanced features\r\n        settings.set("/rtx/indirectLighting/enabled", True)\r\n        settings.set("/rtx/shadows/enabled", True)\r\n        settings.set("/rtx/softShadows/enabled", True)\r\n        settings.set("/rtx/screenSpaceReflections/enabled", True)\r\n        settings.set("/rtx/ambientOcclusion/enabled", True)\r\n\r\n    def capture_high_quality_image(self, camera_path, output_path):\r\n        """Capture high-fidelity image for ML training"""\r\n        from omni.isaac.synthetic_utils import capture_images\r\n\r\n        capture_settings = {\r\n            "width": 1920,\r\n            "height": 1080,\r\n            "color": True,\r\n            "depth": True,\r\n            "instance_segmentation": True,\r\n            "semantic_segmentation": True,\r\n            "bounding_box_2d": True,\r\n            "bounding_box_3d": True,\r\n            "normals": True,\r\n            "motion_vectors": True\r\n        }\r\n\r\n        # Capture with high quality settings\r\n        await capture_images(\r\n            camera_prim_path=camera_path,\r\n            output_dir=output_path,\r\n            capture_settings=capture_settings\r\n        )\n'})}),"\n",(0,t.jsx)(n.h3,{id:"1122-material-definition-language-mdl",children:"11.2.2 Material Definition Language (MDL)"}),"\n",(0,t.jsx)(n.p,{children:"Create photorealistic materials using MDL:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MDLMaterialLibrary:\r\n    def __init__(self):\r\n        self.materials = {}\r\n\r\n    def create_metal_material(self, name, base_color, roughness, metallic):\r\n        """Create physically-based metallic material"""\r\n        from omni.isaac.core.materials import PhysicsMaterial\r\n\r\n        material = PhysicsMaterial(\r\n            prim_path=f"/World/Materials/{name}",\r\n            dynamic_friction=0.7,\r\n            static_friction=0.7,\r\n            restitution=0.1\r\n        )\r\n\r\n        # Apply visual properties using MDL\r\n        from pxr import UsdShade\r\n\r\n        stage = omni.usd.get_context().get_stage()\r\n        shader = UsdShade.Shader.Get(stage, f"/World/Materials/{name}/Shader")\r\n\r\n        if not shader:\r\n            shader = UsdShade.Shader.Define(\r\n                stage,\r\n                f"/World/Materials/{name}/Shader"\r\n            )\r\n            shader.CreateIdAttr("MDL")\r\n            shader.CreateInput("mdl", Sdf.ValueTypeNames.Asset).Set("OmniPBR.mdl")\r\n\r\n        # Set material parameters\r\n        shader.CreateInput("base_color", Sdf.ValueTypeNames.Float3).Set(base_color)\r\n        shader.CreateInput("roughness", Sdf.ValueTypeNames.Float).Set(roughness)\r\n        shader.CreateInput("metallic", Sdf.ValueTypeNames.Float).Set(metallic)\r\n        shader.CreateInput("specular_level", Sdf.ValueTypeNames.Float).Set(0.5)\r\n        shader.CreateInput("normal_map", Sdf.ValueTypeNames.Asset).Set("")\r\n\r\n        self.materials[name] = material\r\n        return material\r\n\r\n    def create_complex_material(self, name, properties):\r\n        """Create complex material with multiple layers"""\r\n        from pxr import Usd, UsdGeom\r\n\r\n        stage = omni.usd.get_context().get_stage()\r\n\r\n        # Create material prim\r\n        material_path = f"/World/Materials/{name}"\r\n        material = UsdShade.Material.Define(stage, material_path)\r\n\r\n        # Surface shader\r\n        surface_shader = UsdShade.Shader.Define(\r\n            stage,\r\n            f"{material_path}/SurfaceShader"\r\n        )\r\n        surface_shader.CreateIdAttr("MDL")\r\n        surface_shader.CreateInput("mdl", Sdf.ValueTypeNames.Asset).Set(\r\n            "OmniPBR.mdl"\r\n        )\r\n\r\n        # Apply complex properties\r\n        for param_name, param_value in properties.items():\r\n            if param_name == "base_color":\r\n                surface_shader.CreateInput(\r\n                    param_name,\r\n                    Sdf.ValueTypeNames.Float3\r\n                ).Set(param_value)\r\n            elif param_name in ["roughness", "metallic", "specular", "opacity"]:\r\n                surface_shader.CreateInput(\r\n                    param_name,\r\n                    Sdf.ValueTypeNames.Float\r\n                ).Set(param_value)\r\n            elif param_name == "normal_map":\r\n                surface_shader.CreateInput(\r\n                    param_name,\r\n                    Sdf.ValueTypeNames.Asset\r\n                ).Set(param_value)\r\n\r\n        # Connect surface shader to material\r\n        material.CreateSurfaceOutput().ConnectToSource(\r\n            surface_shader.ConnectableAPI()\r\n        )\r\n\r\n        return material\n'})}),"\n",(0,t.jsx)(n.h2,{id:"113-synthetic-data-generation",children:"11.3 Synthetic Data Generation"}),"\n",(0,t.jsx)(n.h3,{id:"1131-domain-randomization",children:"11.3.1 Domain Randomization"}),"\n",(0,t.jsx)(n.p,{children:"Domain randomization creates varied training data to improve model robustness:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DomainRandomization:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.randomization_params = {\r\n            "lighting": {\r\n                "intensity_range": (500, 5000),\r\n                "color_range": [(0.9, 0.9, 0.9), (1.0, 1.0, 1.0)],\r\n                "position_range": [(-10, -10, 5), (10, 10, 20)]\r\n            },\r\n            "materials": {\r\n                "roughness_range": (0.1, 0.9),\r\n                "metallic_range": (0.0, 1.0),\r\n                "color_variations": 0.3\r\n            },\r\n            "objects": {\r\n                "position_range": (-5, 5),\r\n                "rotation_range": (0, 360),\r\n                "scale_range": (0.8, 1.2)\r\n            },\r\n            "camera": {\r\n                "position_range": [(-8, -8, 2), (8, 8, 8)],\r\n                "rotation_range": (-30, 30),\r\n                "focal_length_range": (20, 35)\r\n            }\r\n        }\r\n\r\n    async def randomize_scene(self):\r\n        """Apply domain randomization to entire scene"""\r\n        # Randomize lighting\r\n        await self._randomize_lighting()\r\n\r\n        # Randomize materials\r\n        await self._randomize_materials()\r\n\r\n        # Randomize object poses\r\n        await self._randomize_objects()\r\n\r\n        # Randomize camera positions\r\n        await self._randomize_cameras()\r\n\r\n    async def _randomize_lighting(self):\r\n        """Randomize lighting conditions"""\r\n        import random\r\n        from omni.isaac.core import Light\r\n\r\n        # Randomize dome light\r\n        dome_light = self.world.scene.get_object("/World/DomeLight")\r\n        if dome_light:\r\n            intensity = random.uniform(*self.randomization_params["lighting"]["intensity_range"])\r\n            color = [\r\n                random.uniform(*self.randomization_params["lighting"]["color_range"][0]),\r\n                random.uniform(*self.randomization_params["lighting"]["color_range"][1]),\r\n                random.uniform(*self.randomization_params["lighting"]["color_range"][2])\r\n            ]\r\n\r\n            dome_light.set_intensity(intensity)\r\n            dome_light.set_color(color + [1.0])\r\n\r\n        # Add random point lights\r\n        num_lights = random.randint(1, 3)\r\n        for i in range(num_lights):\r\n            pos = [\r\n                random.uniform(*self.randomization_params["lighting"]["position_range"][0]),\r\n                random.uniform(*self.randomization_params["lighting"]["position_range"][1]),\r\n                random.uniform(*self.randomization_params["lighting"]["position_range"][2])\r\n            ]\r\n\r\n            point_light = Light(\r\n                prim_path=f"/World/RandomLight_{i}",\r\n                light_type="sphere",\r\n                intensity=random.uniform(100, 1000),\r\n                position=pos,\r\n                radius=0.1\r\n            )\r\n            self.world.scene.add(point_light)\r\n\r\n    async def _randomize_materials(self):\r\n        """Randomize material properties"""\r\n        import random\r\n\r\n        for material_name, material in self.world.materials.items():\r\n            if hasattr(material, \'set_roughness\'):\r\n                # Randomize PBR properties\r\n                roughness = random.uniform(*self.randomization_params["materials"]["roughness_range"])\r\n                metallic = random.uniform(*self.randomization_params["materials"]["metallic_range"])\r\n\r\n                material.set_roughness(roughness)\r\n                material.set_metallic(metallic)\r\n\r\n                # Randomize color if applicable\r\n                if hasattr(material, \'get_base_color\'):\r\n                    base_color = list(material.get_base_color())\r\n                    color_var = self.randomization_params["materials"]["color_variations"]\r\n\r\n                    for i in range(3):\r\n                        base_color[i] += random.uniform(-color_var, color_var)\r\n                        base_color[i] = max(0, min(1, base_color[i]))\r\n\r\n                    material.set_base_color(base_color)\r\n\r\n    async def _randomize_objects(self):\r\n        """Randomize object positions and orientations"""\r\n        import random\r\n\r\n        for obj_name, obj in self.world.objects.items():\r\n            # Random position\r\n            pos = [\r\n                random.uniform(*self.randomization_params["objects"]["position_range"]),\r\n                random.uniform(*self.randomization_params["objects"]["position_range"]),\r\n                random.uniform(0, 3)\r\n            ]\r\n\r\n            # Random orientation\r\n            rotation = [\r\n                0,\r\n                random.uniform(*self.randomization_params["objects"]["rotation_range"]),\r\n                0\r\n            ]\r\n\r\n            # Random scale\r\n            scale = random.uniform(*self.randomization_params["objects"]["scale_range"])\r\n\r\n            obj.set_world_pose(pos, rotation)\r\n            obj.scale = [scale, scale, scale]\r\n\r\n    async def _randomize_cameras(self):\r\n        """Randomize camera positions and settings"""\r\n        import random\r\n\r\n        cameras = ["/World/Camera_1", "/World/Camera_2", "/World/Camera_3"]\r\n\r\n        for camera_path in cameras:\r\n            camera = self.world.scene.get_object(camera_path)\r\n            if camera:\r\n                # Random position\r\n                pos = [\r\n                    random.uniform(*self.randomization_params["camera"]["position_range"][0]),\r\n                    random.uniform(*self.randomization_params["camera"]["position_range"][1]),\r\n                    random.uniform(*self.randomization_params["camera"]["position_range"][2])\r\n                ]\r\n\r\n                # Look at origin\r\n                target = [0, 0, 1]\r\n                camera.set_world_pose(pos, target)\r\n\r\n                # Random focal length\r\n                focal_length = random.uniform(*self.randomization_params["camera"]["focal_length_range"])\r\n                camera.set_focal_length(focal_length)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"1132-automated-data-capture-pipeline",children:"11.3.2 Automated Data Capture Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Create automated pipeline for large-scale dataset generation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SyntheticDataPipeline:\r\n    def __init__(self, output_dir="synthetic_data"):\r\n        self.output_dir = output_dir\r\n        self.scenarios = []\r\n        self.capture_settings = {\r\n            "resolution": (1920, 1080),\r\n            "formats": ["jpg", "png", "exr"],\r\n            "annotations": {\r\n                "segmentation": True,\r\n                "depth": True,\r\n                "normals": True,\r\n                "motion_vectors": True,\r\n                "bounding_boxes": True\r\n            }\r\n        }\r\n\r\n    def add_scenario(self, scenario_config):\r\n        """Add a scenario to the generation pipeline"""\r\n        self.scenarios.append(scenario_config)\r\n\r\n    async def generate_dataset(self, num_samples_per_scenario=100):\r\n        """Generate complete synthetic dataset"""\r\n        import os\r\n        from datetime import datetime\r\n\r\n        # Create output directory structure\r\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\r\n        base_dir = f"{self.output_dir}/dataset_{timestamp}"\r\n\r\n        os.makedirs(f"{base_dir}/images", exist_ok=True)\r\n        os.makedirs(f"{base_dir}/annotations", exist_ok=True)\r\n        os.makedirs(f"{base_dir}/metadata", exist_ok=True)\r\n\r\n        # Generate data for each scenario\r\n        for scenario_idx, scenario in enumerate(self.scenarios):\r\n            print(f"Generating scenario {scenario_idx + 1}/{len(self.scenarios)}: {scenario[\'name\']}")\r\n\r\n            for sample_idx in range(num_samples_per_scenario):\r\n                # Setup scenario\r\n                await self._setup_scenario(scenario)\r\n\r\n                # Apply domain randomization\r\n                await self._apply_randomization(scenario.get(\'randomization\', {}))\r\n\r\n                # Capture data\r\n                sample_data = await self._capture_sample(scenario_idx, sample_idx)\r\n\r\n                # Save data\r\n                await self._save_sample(sample_data, base_dir, scenario_idx, sample_idx)\r\n\r\n                # Progress indicator\r\n                if (sample_idx + 1) % 10 == 0:\r\n                    print(f"  Generated {sample_idx + 1}/{num_samples_per_scenario} samples")\r\n\r\n    async def _setup_scenario(self, scenario):\r\n        """Setup a specific scenario"""\r\n        # Clear previous scene\r\n        await self._clear_scene()\r\n\r\n        # Load environment\r\n        if \'environment\' in scenario:\r\n            await self._load_environment(scenario[\'environment\'])\r\n\r\n        # Add objects\r\n        for obj_config in scenario.get(\'objects\', []):\r\n            await self._add_object(obj_config)\r\n\r\n        # Setup cameras\r\n        for cam_config in scenario.get(\'cameras\', []):\r\n            await self._setup_camera(cam_config)\r\n\r\n        # Configure lighting\r\n        if \'lighting\' in scenario:\r\n            await self._setup_lighting(scenario[\'lighting\'])\r\n\r\n    async def _capture_sample(self, scenario_idx, sample_idx):\r\n        """Capture comprehensive sample data"""\r\n        sample_data = {\r\n            \'scenario_id\': scenario_idx,\r\n            \'sample_id\': sample_idx,\r\n            \'timestamp\': time.time(),\r\n            \'data\': {}\r\n        }\r\n\r\n        # Capture from each camera\r\n        cameras = self.world.scene.get_cameras()\r\n        for cam_idx, camera in enumerate(cameras):\r\n            cam_data = {}\r\n\r\n            # Color images\r\n            for fmt in self.capture_settings["formats"]:\r\n                image = await self._capture_color_image(camera, fmt)\r\n                cam_data[f"color_{fmt}"] = image\r\n\r\n            # Annotations\r\n            if self.capture_settings["annotations"]["depth"]:\r\n                cam_data["depth"] = await self._capture_depth(camera)\r\n\r\n            if self.capture_settings["annotations"]["segmentation"]:\r\n                cam_data["segmentation"] = await self._capture_segmentation(camera)\r\n\r\n            if self.capture_settings["annotations"]["normals"]:\r\n                cam_data["normals"] = await self._capture_normals(camera)\r\n\r\n            if self.capture_settings["annotations"]["bounding_boxes"]:\r\n                cam_data["bounding_boxes"] = await self._capture_bounding_boxes(camera)\r\n\r\n            sample_data[\'data\'][f\'camera_{cam_idx}\'] = cam_data\r\n\r\n        # Scene metadata\r\n        sample_data[\'metadata\'] = await self._get_scene_metadata()\r\n\r\n        return sample_data\r\n\r\n    async def _save_sample(self, sample_data, base_dir, scenario_idx, sample_idx):\r\n        """Save sample data to disk"""\r\n        import json\r\n        import numpy as np\r\n        from PIL import Image\r\n\r\n        # Save images and annotations\r\n        for cam_name, cam_data in sample_data[\'data\'].items():\r\n            cam_dir = f"{base_dir}/images/scenario_{scenario_idx:03d}/{cam_name}"\r\n            os.makedirs(cam_dir, exist_ok=True)\r\n\r\n            # Save color images\r\n            for key, data in cam_data.items():\r\n                if key.startswith("color_"):\r\n                    fmt = key.split("_")[1]\r\n                    filename = f"sample_{sample_idx:06d}.{fmt}"\r\n\r\n                    if isinstance(data, np.ndarray):\r\n                        Image.fromarray(data).save(f"{cam_dir}/{filename}")\r\n                    else:\r\n                        with open(f"{cam_dir}/{filename}", \'wb\') as f:\r\n                            f.write(data)\r\n\r\n                # Save annotations\r\n                elif key in ["depth", "segmentation", "normals"]:\r\n                    filename = f"sample_{sample_idx:06d}_{key}.npy"\r\n                    np.save(f"{cam_dir}/{filename}", data)\r\n\r\n                elif key == "bounding_boxes":\r\n                    filename = f"sample_{sample_idx:06d}_boxes.json"\r\n                    with open(f"{cam_dir}/{filename}", \'w\') as f:\r\n                        json.dump(data, f, indent=2)\r\n\r\n        # Save metadata\r\n        metadata_dir = f"{base_dir}/metadata/scenario_{scenario_idx:03d}"\r\n        os.makedirs(metadata_dir, exist_ok=True)\r\n\r\n        with open(f"{metadata_dir}/sample_{sample_idx:06d}.json", \'w\') as f:\r\n            json.dump({\r\n                \'scenario_id\': sample_data[\'scenario_id\'],\r\n                \'sample_id\': sample_data[\'sample_id\'],\r\n                \'timestamp\': sample_data[\'timestamp\'],\r\n                \'metadata\': sample_data[\'metadata\']\r\n            }, f, indent=2)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"114-ai-training-integration",children:"11.4 AI Training Integration"}),"\n",(0,t.jsx)(n.h3,{id:"1141-ground-truth-data-generation",children:"11.4.1 Ground Truth Data Generation"}),"\n",(0,t.jsx)(n.p,{children:"Generate perfect ground truth for supervised learning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class GroundTruthGenerator:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.gt_data = {}\r\n\r\n    def generate_perfect_annotations(self):\r\n        \"\"\"Generate perfect ground truth annotations\"\"\"\r\n        return {\r\n            'semantic_segmentation': self._get_semantic_gt(),\r\n            'instance_segmentation': self._get_instance_gt(),\r\n            'depth': self._get_depth_gt(),\r\n            'normals': self._get_normal_gt(),\r\n            'optical_flow': self._get_optical_flow_gt(),\r\n            'bounding_boxes': self._get_bbox_gt(),\r\n            'pose': self._get_pose_gt(),\r\n            'keypoints': self._get_keypoint_gt()\r\n        }\r\n\r\n    def _get_semantic_gt(self):\r\n        \"\"\"Generate perfect semantic segmentation\"\"\"\r\n        from omni.isaac.synthetic_utils import get_semantic_segmentation\r\n\r\n        # Get semantic data from Isaac Sim\r\n        semantic_data = get_semantic_segmentation()\r\n\r\n        # Convert to class labels\r\n        semantic_map = np.zeros(semantic_data.shape[:2], dtype=np.uint8)\r\n\r\n        # Define semantic classes\r\n        class_mapping = {\r\n            0: 'background',\r\n            1: 'floor',\r\n            2: 'wall',\r\n            3: 'robot',\r\n            4: 'obstacle',\r\n            5: 'tool',\r\n            6: 'target_object'\r\n        }\r\n\r\n        for class_id, class_name in class_mapping.items():\r\n            mask = semantic_data == class_name\r\n            semantic_map[mask] = class_id\r\n\r\n        return {\r\n            'semantic_map': semantic_map,\r\n            'class_mapping': class_mapping,\r\n            'confidence_map': np.ones_like(semantic_map, dtype=np.float32)\r\n        }\r\n\r\n    def _get_instance_gt(self):\r\n        \"\"\"Generate perfect instance segmentation\"\"\"\r\n        from omni.isaac.synthetic_utils import get_instance_segmentation\r\n\r\n        instance_data = get_instance_segmentation()\r\n\r\n        # Process instance data\r\n        unique_instances = np.unique(instance_data)\r\n        instance_masks = []\r\n\r\n        for instance_id in unique_instances:\r\n            if instance_id == 0:  # Background\r\n                continue\r\n\r\n            mask = instance_data == instance_id\r\n            instance_masks.append({\r\n                'id': int(instance_id),\r\n                'mask': mask,\r\n                'centroid': self._calculate_centroid(mask),\r\n                'area': np.sum(mask),\r\n                'bounding_box': self._calculate_bbox(mask)\r\n            })\r\n\r\n        return instance_masks\r\n\r\n    def _get_depth_gt(self):\r\n        \"\"\"Generate perfect depth ground truth\"\"\"\r\n        from omni.isaac.synthetic_utils import get_depth\r\n\r\n        depth_data = get_depth()\r\n\r\n        # Convert to meters if needed\r\n        if depth_data.max() < 100:  # Already in meters\r\n            depth_meters = depth_data\r\n        else:  # Convert from millimeters\r\n            depth_meters = depth_data / 1000.0\r\n\r\n        # Calculate depth statistics\r\n        return {\r\n            'depth_map': depth_meters,\r\n            'min_depth': np.min(depth_meters),\r\n            'max_depth': np.max(depth_meters),\r\n            'mean_depth': np.mean(depth_meters),\r\n            'valid_pixels': np.sum(depth_meters > 0)\r\n        }\r\n\r\n    def _get_bbox_gt(self):\r\n        \"\"\"Generate perfect bounding box ground truth\"\"\"\r\n        bboxes = []\r\n\r\n        for obj_name, obj in self.world.objects.items():\r\n            # Get object's axis-aligned bounding box\r\n            aabb = obj.get_aabb()\r\n\r\n            # Convert to image coordinates\r\n            cam_poses = self._get_all_camera_poses()\r\n\r\n            for cam_name, (camera, pose) in cam_poses.items():\r\n                # Project 3D bbox to 2D\r\n                bbox_2d = self._project_aabb_to_2d(aabb, camera, pose)\r\n\r\n                if bbox_2d is not None:\r\n                    bboxes.append({\r\n                        'object_name': obj_name,\r\n                        'camera': cam_name,\r\n                        'bbox_2d': bbox_2d,\r\n                        'bbox_3d': {\r\n                            'min': aabb[0].tolist(),\r\n                            'max': aabb[1].tolist()\r\n                        },\r\n                        'confidence': 1.0  # Perfect ground truth\r\n                    })\r\n\r\n        return bboxes\r\n\r\n    def _get_pose_gt(self):\r\n        \"\"\"Generate perfect pose ground truth\"\"\"\r\n        poses = {}\r\n\r\n        for obj_name, obj in self.world.objects.items():\r\n            pose = obj.get_world_pose()\r\n\r\n            # Convert pose to different formats\r\n            poses[obj_name] = {\r\n                'position': pose[0].tolist(),\r\n                'orientation_quat': pose[1].tolist(),\r\n                'orientation_euler': self._quat_to_euler(pose[1]).tolist(),\r\n                'transformation_matrix': self._pose_to_matrix(pose).tolist()\r\n            }\r\n\r\n        return poses\n"})}),"\n",(0,t.jsx)(n.h3,{id:"1142-training-pipeline-integration",children:"11.4.2 Training Pipeline Integration"}),"\n",(0,t.jsx)(n.p,{children:"Integrate Isaac Sim with machine learning training pipelines:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class IsaacTrainingIntegration:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.training_pipeline = None\r\n        self.active = False\r\n\r\n    def connect_to_ml_framework(self, framework_type="pytorch"):\r\n        """Connect to machine learning framework"""\r\n        if framework_type == "pytorch":\r\n            self._connect_pytorch()\r\n        elif framework_type == "tensorflow":\r\n            self._connect_tensorflow()\r\n        elif framework_type == "mlx":\r\n            self._connect_mlx()\r\n\r\n    def _connect_pytorch(self):\r\n        """Connect to PyTorch training pipeline"""\r\n        import torch\r\n        from torch.utils.data import DataLoader\r\n\r\n        class IsaacDataset(torch.utils.data.Dataset):\r\n            def __init__(self, isaac_integration):\r\n                self.isaac = isaac_integration\r\n                self.samples = []\r\n\r\n            async def generate_samples(self, num_samples):\r\n                """Generate samples from Isaac Sim"""\r\n                for i in range(num_samples):\r\n                    # Randomize scene\r\n                    await self.isaac.randomize_scene()\r\n\r\n                    # Capture data\r\n                    sample = await self.isaac.capture_training_sample()\r\n                    self.samples.append(sample)\r\n\r\n            def __len__(self):\r\n                return len(self.samples)\r\n\r\n            def __getitem__(self, idx):\r\n                sample = self.samples[idx]\r\n\r\n                # Convert to tensors\r\n                image = torch.from_numpy(sample[\'image\']).float() / 255.0\r\n                image = image.permute(2, 0, 1)  # HWC to CHW\r\n\r\n                target = torch.from_numpy(sample[\'target\']).long()\r\n\r\n                return image, target\r\n\r\n        # Create dataset and dataloader\r\n        self.dataset = IsaacDataset(self)\r\n        self.dataloader = DataLoader(\r\n            self.dataset,\r\n            batch_size=32,\r\n            shuffle=True,\r\n            num_workers=4\r\n        )\r\n\r\n    async def train_policy(self, num_epochs=100):\r\n        """Train policy using synthetic data"""\r\n        import torch\r\n        import torch.nn as nn\r\n        import torch.optim as optim\r\n\r\n        # Define model\r\n        class PolicyNetwork(nn.Module):\r\n            def __init__(self, input_dim, hidden_dim, output_dim):\r\n                super().__init__()\r\n                self.network = nn.Sequential(\r\n                    nn.Linear(input_dim, hidden_dim),\r\n                    nn.ReLU(),\r\n                    nn.Linear(hidden_dim, hidden_dim),\r\n                    nn.ReLU(),\r\n                    nn.Linear(hidden_dim, output_dim),\r\n                    nn.Tanh()  # Scale to [-1, 1]\r\n                )\r\n\r\n            def forward(self, x):\r\n                return self.network(x)\r\n\r\n        # Initialize model\r\n        model = PolicyNetwork(input_dim=128, hidden_dim=256, output_dim=12)\r\n        criterion = nn.MSELoss()\r\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n\r\n        # Training loop\r\n        for epoch in range(num_epochs):\r\n            total_loss = 0\r\n\r\n            for batch_idx, (images, targets) in enumerate(self.dataloader):\r\n                # Forward pass\r\n                outputs = model(images)\r\n                loss = criterion(outputs, targets)\r\n\r\n                # Backward pass\r\n                optimizer.zero_grad()\r\n                loss.backward()\r\n                optimizer.step()\r\n\r\n                total_loss += loss.item()\r\n\r\n            # Generate new training data\r\n            if epoch % 10 == 0:\r\n                await self.dataset.generate_samples(100)\r\n                print(f"Epoch {epoch}, Loss: {total_loss/len(self.dataloader):.4f}")\r\n\r\n        return model\r\n\r\n    async def reinforcement_learning_loop(self, policy, num_episodes=1000):\r\n        """Reinforcement learning loop with Isaac Sim"""\r\n        import torch\r\n\r\n        for episode in range(num_episodes):\r\n            # Reset environment\r\n            await self.reset_environment()\r\n\r\n            episode_reward = 0\r\n            done = False\r\n            step = 0\r\n\r\n            while not done and step < 1000:  # Max steps per episode\r\n                # Get current state\r\n                state = await self.get_state()\r\n\r\n                # Select action using policy\r\n                with torch.no_grad():\r\n                    action = policy(torch.from_numpy(state).float().unsqueeze(0))\r\n                    action = action.squeeze().numpy()\r\n\r\n                # Execute action\r\n                next_state, reward, done = await self.step_environment(action)\r\n\r\n                episode_reward += reward\r\n                step += 1\r\n\r\n            print(f"Episode {episode}, Reward: {episode_reward:.2f}, Steps: {step}")\r\n\r\n    async def reset_environment(self):\r\n        """Reset simulation environment"""\r\n        # Reset object positions\r\n        for obj_name, obj in self.world.objects.items():\r\n            obj.reset()\r\n\r\n        # Randomize initial conditions\r\n        await self._randomize_initial_conditions()\r\n\r\n    async def get_state(self):\r\n        """Get current environment state"""\r\n        state = []\r\n\r\n        # Get robot pose\r\n        robot_pose = self.world.robot.get_world_pose()\r\n        state.extend(robot_pose[0])  # Position\r\n        state.extend(robot_pose[1][:3])  # Quaternion (x,y,z only)\r\n\r\n        # Get sensor readings\r\n        sensor_data = await self._get_sensor_readings()\r\n        state.extend(sensor_data)\r\n\r\n        # Get object positions\r\n        for obj_name, obj in self.world.objects.items():\r\n            if obj_name != \'robot\':\r\n                obj_pose = obj.get_world_pose()\r\n                state.extend(obj_pose[0])  # Position only\r\n\r\n        return np.array(state, dtype=np.float32)\r\n\r\n    async def step_environment(self, action):\r\n        """Execute action and return new state, reward, done"""\r\n        # Apply action to robot\r\n        await self._apply_robot_action(action)\r\n\r\n        # Step simulation\r\n        await self.world.step_async()\r\n\r\n        # Get new state\r\n        new_state = await self.get_state()\r\n\r\n        # Calculate reward\r\n        reward = await self._calculate_reward()\r\n\r\n        # Check if done\r\n        done = await self._check_termination()\r\n\r\n        return new_state, reward, done\n'})}),"\n",(0,t.jsx)(n.h2,{id:"115-advanced-isaac-sim-features",children:"11.5 Advanced Isaac Sim Features"}),"\n",(0,t.jsx)(n.h3,{id:"1151-distributed-simulation",children:"11.5.1 Distributed Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Scale simulation across multiple GPUs and nodes:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DistributedSimulation:\r\n    def __init__(self, num_workers=4):\r\n        self.num_workers = num_workers\r\n        self.workers = []\r\n        self.job_queue = asyncio.Queue()\r\n        self.result_queue = asyncio.Queue()\r\n\r\n    async def initialize_workers(self):\r\n        """Initialize distributed workers"""\r\n        for worker_id in range(self.num_workers):\r\n            worker = IsaacWorker(worker_id)\r\n            await worker.initialize()\r\n            self.workers.append(worker)\r\n\r\n            # Start worker task\r\n            asyncio.create_task(self._worker_loop(worker))\r\n\r\n    async def _worker_loop(self, worker):\r\n        """Worker processing loop"""\r\n        while True:\r\n            try:\r\n                # Get job from queue\r\n                job = await self.job_queue.get()\r\n\r\n                # Process job\r\n                result = await worker.process_job(job)\r\n\r\n                # Put result in queue\r\n                await self.result_queue.put(result)\r\n\r\n                # Mark job as done\r\n                self.job_queue.task_done()\r\n\r\n            except Exception as e:\r\n                print(f"Worker {worker.id} error: {e}")\r\n\r\n    async def generate_dataset_distributed(self, num_samples):\r\n        """Generate dataset using distributed workers"""\r\n        # Create jobs\r\n        for i in range(num_samples):\r\n            job = {\r\n                \'sample_id\': i,\r\n                \'scenario\': self._select_scenario(),\r\n                \'randomization_seed\': i\r\n            }\r\n            await self.job_queue.put(job)\r\n\r\n        # Collect results\r\n        results = []\r\n        completed = 0\r\n\r\n        while completed < num_samples:\r\n            result = await self.result_queue.get()\r\n            results.append(result)\r\n            completed += 1\r\n\r\n            if completed % 100 == 0:\r\n                print(f"Generated {completed}/{num_samples} samples")\r\n\r\n        return results\r\n\r\nclass IsaacWorker:\r\n    def __init__(self, worker_id):\r\n        self.id = worker_id\r\n        self.simulation_app = None\r\n        self.world = None\r\n\r\n    async def initialize(self):\r\n        """Initialize worker instance"""\r\n        # Create separate simulation app for each worker\r\n        self.simulation_app = SimulationApp({\r\n            "headless": True,  # Workers run headless\r\n            "width": 640,\r\n            "height": 480,\r\n            "renderer": "RayTracedLighting"\r\n        })\r\n\r\n        self.world = World()\r\n        await self.world.initialize_simulation_async()\r\n\r\n    async def process_job(self, job):\r\n        """Process individual generation job"""\r\n        # Set random seed\r\n        np.random.seed(job[\'randomization_seed\'])\r\n\r\n        # Load scenario\r\n        await self._load_scenario(job[\'scenario\'])\r\n\r\n        # Apply randomization\r\n        await self._randomize_scene()\r\n\r\n        # Capture data\r\n        data = await self._capture_data()\r\n\r\n        return {\r\n            \'sample_id\': job[\'sample_id\'],\r\n            \'worker_id\': self.id,\r\n            \'data\': data\r\n        }\r\n\r\n    async def cleanup(self):\r\n        """Clean up worker resources"""\r\n        if self.world:\r\n            self.world.clear()\r\n\r\n        if self.simulation_app:\r\n            self.simulation_app.close()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"1152-cloud-integration",children:"11.5.2 Cloud Integration"}),"\n",(0,t.jsx)(n.p,{children:"Deploy Isaac Sim in cloud environments:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class CloudIsaacDeployment:\r\n    def __init__(self, cloud_config):\r\n        self.cloud_config = cloud_config\r\n        self.compute_instances = []\r\n        self.storage_bucket = None\r\n\r\n    async def deploy_to_cloud(self):\r\n        \"\"\"Deploy simulation to cloud\"\"\"\r\n        # Initialize cloud provider\r\n        if self.cloud_config['provider'] == 'aws':\r\n            await self._deploy_to_aws()\r\n        elif self.cloud_config['provider'] == 'gcp':\r\n            await self._deploy_to_gcp()\r\n        elif self.cloud_config['provider'] == 'azure':\r\n            await self._deploy_to_azure()\r\n\r\n    async def _deploy_to_aws(self):\r\n        \"\"\"Deploy to AWS EC2\"\"\"\r\n        import boto3\r\n\r\n        ec2 = boto3.client('ec2')\r\n\r\n        # Launch EC2 instances with NVIDIA GPUs\r\n        response = ec2.run_instances(\r\n            ImageId='ami-0c02fb55956c7d3165',  # NVIDIA Deep Learning AMI\r\n            InstanceType='p3.2xlarge',  # NVIDIA V100 GPU\r\n            MinCount=1,\r\n            MaxCount=self.cloud_config['num_instances'],\r\n            KeyName=self.cloud_config['key_pair'],\r\n            SecurityGroupIds=[self.cloud_config['security_group']],\r\n            SubnetId=self.cloud_config['subnet_id'],\r\n            UserData=self._generate_cloud_init_script(),\r\n            TagSpecifications=[\r\n                {\r\n                    'ResourceType': 'instance',\r\n                    'Tags': [\r\n                        {'Key': 'Name', 'Value': 'isaac-sim-worker'},\r\n                        {'Key': 'Project', 'Value': 'robotics-training'}\r\n                    ]\r\n                }\r\n            ]\r\n        )\r\n\r\n        # Store instance IDs\r\n        for instance in response['Instances']:\r\n            self.compute_instances.append(instance['InstanceId'])\r\n\r\n        # Setup S3 for data storage\r\n        s3 = boto3.client('s3')\r\n        self.storage_bucket = f\"isaac-sim-{self.cloud_config['project_name']}-{int(time.time())}\"\r\n\r\n        s3.create_bucket(\r\n            Bucket=self.storage_bucket,\r\n            CreateBucketConfiguration={'LocationConstraint': 'us-west-2'}\r\n        )\r\n\r\n    def _generate_cloud_init_script(self):\r\n        \"\"\"Generate cloud initialization script\"\"\"\r\n        return f'''#!/bin/bash\r\n# Update system\r\napt-get update -y\r\n\r\n# Install Docker\r\ncurl -fsSL https://get.docker.com -o get-docker.sh\r\nsh get-docker.sh\r\n\r\n# Install NVIDIA Container Toolkit\r\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list\r\n\r\napt-get update -y\r\napt-get install -y nvidia-docker2\r\n\r\n# Restart Docker\r\nsystemctl restart docker\r\n\r\n# Pull Isaac Sim container\r\ndocker pull nvcr.io/isaac/sim/2023.1.1:isaac-sim\r\n\r\n# Create working directory\r\nmkdir -p /workspace\r\ncd /workspace\r\n\r\n# Clone project repository\r\ngit clone {self.cloud_config['project_repo']} .\r\n\r\n# Setup environment variables\r\necho \"export ISAAC_SIM_PATH=/opt/isaac-sim\" >> ~/.bashrc\r\necho \"export PROJECT_DIR=/workspace\" >> ~/.bashrc\r\necho \"export AWS_BUCKET={self.storage_bucket}\" >> ~/.bashrc\r\n'''\r\n\r\n    async def run_distributed_training(self):\r\n        \"\"\"Run distributed training across cloud instances\"\"\"\r\n        # Setup SSH connections to instances\r\n        connections = []\r\n\r\n        for instance_id in self.compute_instances:\r\n            connection = await self._connect_to_instance(instance_id)\r\n            connections.append(connection)\r\n\r\n        # Distribute training jobs\r\n        jobs = self._create_training_jobs()\r\n\r\n        # Execute jobs across instances\r\n        tasks = []\r\n        for i, job in enumerate(jobs):\r\n            connection = connections[i % len(connections)]\r\n            task = asyncio.create_task(\r\n                self._run_remote_job(connection, job)\r\n            )\r\n            tasks.append(task)\r\n\r\n        # Wait for all jobs to complete\r\n        results = await asyncio.gather(*tasks)\r\n\r\n        # Collect results from S3\r\n        return await self._collect_results()\r\n\r\n    async def scale_cluster(self, num_instances):\r\n        \"\"\"Scale cluster up or down\"\"\"\r\n        current_count = len(self.compute_instances)\r\n\r\n        if num_instances > current_count:\r\n            # Scale up\r\n            await self._add_instances(num_instances - current_count)\r\n        elif num_instances < current_count:\r\n            # Scale down\r\n            await self._remove_instances(current_count - num_instances)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covered NVIDIA Isaac Sim's advanced capabilities for photorealistic robotics simulation:"}),"\n",(0,t.jsx)(n.h3,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac Sim Architecture"}),": Omniverse platform integration with USD and MDL"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RTX Rendering"}),": Real-time ray tracing for photorealistic visuals"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": Creating varied training data for robust AI models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic Data Pipeline"}),": Automated large-scale dataset generation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AI Integration"}),": Ground truth generation and training pipeline integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Distributed Simulation"}),": Multi-GPU and cloud deployment strategies"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"practical-implementations",children:"Practical Implementations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Complete Isaac Sim environment setup with RTX rendering"}),"\n",(0,t.jsx)(n.li,{children:"Domain randomization for varied training conditions"}),"\n",(0,t.jsx)(n.li,{children:"Automated synthetic data generation pipeline"}),"\n",(0,t.jsx)(n.li,{children:"Integration with PyTorch for model training"}),"\n",(0,t.jsx)(n.li,{children:"Cloud deployment on AWS with GPU instances"}),"\n",(0,t.jsx)(n.li,{children:"Distributed simulation across multiple workers"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"With Isaac Sim mastery, you're prepared for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Chapter 12: Digital Twin Development"}),"\n",(0,t.jsx)(n.li,{children:"Creating production-grade simulation pipelines"}),"\n",(0,t.jsx)(n.li,{children:"Scaling to large fleet training scenarios"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"glossary-terms",children:"Glossary Terms"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Term"}),": ",(0,t.jsx)(n.strong,{children:"Universal Scene Description (USD)"}),"\r\n",(0,t.jsx)(n.strong,{children:"Definition"}),": Pixar's open-source 3D scene description format that enables interchange between 3D applications, serving as the foundation of Omniverse\r\n",(0,t.jsx)(n.strong,{children:"Related"}),": ",(0,t.jsx)(n.strong,{children:"MDL"}),", ",(0,t.jsx)(n.strong,{children:"Omniverse"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Term"}),": ",(0,t.jsx)(n.strong,{children:"Material Definition Language (MDL)"}),"\r\n",(0,t.jsx)(n.strong,{children:"Definition"}),": NVIDIA's material definition language for describing physically-based materials that can be shared across applications and renderers\r\n",(0,t.jsx)(n.strong,{children:"Related"}),": ",(0,t.jsx)(n.strong,{children:"Physically-Based Rendering"}),", ",(0,t.jsx)(n.strong,{children:"RTX"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Term"}),": ",(0,t.jsx)(n.strong,{children:"Domain Randomization"}),"\r\n",(0,t.jsx)(n.strong,{children:"Definition"}),": Technique of randomly varying simulation parameters (lighting, textures, physics) to generate diverse training data for robust AI model development\r\n",(0,t.jsx)(n.strong,{children:"Related"}),": ",(0,t.jsx)(n.strong,{children:"Data Augmentation"}),", ",(0,t.jsx)(n.strong,{children:"Transfer Learning"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Term"}),": ",(0,t.jsx)(n.strong,{children:"Ground Truth"}),"\r\n",(0,t.jsx)(n.strong,{children:"Definition"}),": Perfect, oracle-correct annotations generated from simulation that serve as targets for supervised learning\r\n",(0,t.jsx)(n.strong,{children:"Related"}),": ",(0,t.jsx)(n.strong,{children:"Synthetic Data"}),", ",(0,t.jsx)(n.strong,{children:"Supervised Learning"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Term"}),": ",(0,t.jsx)(n.strong,{children:"Digital Twin"}),"\r\n",(0,t.jsx)(n.strong,{children:"Definition"}),": High-fidelity virtual representation of a physical system that maintains bi-directional data flow and synchronization with the real world\r\n",(0,t.jsx)(n.strong,{children:"Related"}),": ",(0,t.jsx)(n.strong,{children:"Simulation"}),", ",(0,t.jsx)(n.strong,{children:"IoT Integration"})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-111-rtx-scene-setup",children:"Exercise 11.1: RTX Scene Setup"}),"\n",(0,t.jsx)(n.p,{children:"Create a photorealistic scene in Isaac Sim:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Set up RTX path tracing with proper lighting"}),"\n",(0,t.jsx)(n.li,{children:"Create realistic materials using MDL"}),"\n",(0,t.jsx)(n.li,{children:"Configure multiple cameras for data capture"}),"\n",(0,t.jsx)(n.li,{children:"Validate visual quality with reference images"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-112-domain-randomization-pipeline",children:"Exercise 11.2: Domain Randomization Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Implement comprehensive domain randomization:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Randomize lighting, materials, and object positions"}),"\n",(0,t.jsx)(n.li,{children:"Generate dataset with varied conditions"}),"\n",(0,t.jsx)(n.li,{children:"Train object detection model on synthetic data"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate robustness on real-world images"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-113-synthetic-data-generation",children:"Exercise 11.3: Synthetic Data Generation"}),"\n",(0,t.jsx)(n.p,{children:"Build automated data generation pipeline:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Define multiple scenarios with object configurations"}),"\n",(0,t.jsx)(n.li,{children:"Capture multi-modal data (RGB, depth, segmentation)"}),"\n",(0,t.jsx)(n.li,{children:"Generate large-scale dataset (10,000+ samples)"}),"\n",(0,t.jsx)(n.li,{children:"Implement data validation and quality checks"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-114-cloud-deployment",children:"Exercise 11.4: Cloud Deployment"}),"\n",(0,t.jsx)(n.p,{children:"Deploy Isaac Sim to cloud platform:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Set up AWS EC2 instances with NVIDIA GPUs"}),"\n",(0,t.jsx)(n.li,{children:"Configure Docker containers for Isaac Sim"}),"\n",(0,t.jsx)(n.li,{children:"Implement distributed training across multiple instances"}),"\n",(0,t.jsx)(n.li,{children:"Monitor and manage cloud resources"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-115-digital-twin-integration",children:"Exercise 11.5: Digital Twin Integration"}),"\n",(0,t.jsx)(n.p,{children:"Create bidirectional digital twin:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Connect physical robot sensors to simulation"}),"\n",(0,t.jsx)(n.li,{children:"Implement real-time state synchronization"}),"\n",(0,t.jsx)(n.li,{children:"Validate fidelity between physical and virtual"}),"\n",(0,t.jsx)(n.li,{children:"Deploy predictive maintenance scenarios"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var a=r(6540);const t={},i=a.createContext(t);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);