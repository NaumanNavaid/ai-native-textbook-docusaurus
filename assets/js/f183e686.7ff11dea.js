"use strict";(globalThis.webpackChunkai_native_textbook_docusaurus=globalThis.webpackChunkai_native_textbook_docusaurus||[]).push([[8588],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},9056:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"part-4-perception/chapter-14-sensor-fusion-state-estimation","title":"Sensor Fusion and State Estimation","description":"14.1 State Estimation Fundamentals","source":"@site/docs/part-4-perception/chapter-14-sensor-fusion-state-estimation.mdx","sourceDirName":"part-4-perception","slug":"/part-4-perception/chapter-14-sensor-fusion-state-estimation","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-14-sensor-fusion-state-estimation","draft":false,"unlisted":false,"editUrl":"https://github.com/NaumanNavaid/ai-native-textbook-docusaurus/tree/main/docs/part-4-perception/chapter-14-sensor-fusion-state-estimation.mdx","tags":[],"version":"current","frontMatter":{"title":"Sensor Fusion and State Estimation","part":4,"chapter":14,"difficulty":"advanced","prerequisites":["chapter-13-computer-vision-robots","chapter-3-sensors-actuators-physical-limits"],"estimatedTime":55,"objectives":["Master sensor fusion techniques for robotics","Implement Kalman filters and variants","Apply particle filters for non-linear estimation","Develop robust state estimation systems"]},"sidebar":"chaptersSidebar","previous":{"title":"Computer Vision for Robots","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-13-computer-vision-robots"},"next":{"title":"SLAM, VSLAM, and Navigation","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-15-slam-vslam-navigation"}}');var i=t(4848),s=t(8453);const r={title:"Sensor Fusion and State Estimation",part:4,chapter:14,difficulty:"advanced",prerequisites:["chapter-13-computer-vision-robots","chapter-3-sensors-actuators-physical-limits"],estimatedTime:55,objectives:["Master sensor fusion techniques for robotics","Implement Kalman filters and variants","Apply particle filters for non-linear estimation","Develop robust state estimation systems"]},o="Chapter 14: Sensor Fusion and State Estimation",l={},m=[{value:"14.1 State Estimation Fundamentals",id:"141-state-estimation-fundamentals",level:2},{value:"14.1.1 Introduction to Sensor Fusion",id:"1411-introduction-to-sensor-fusion",level:3},{value:"14.1.2 Bayesian Filtering Framework",id:"1412-bayesian-filtering-framework",level:3}];function c(e){const n={admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",span:"span",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-14-sensor-fusion-and-state-estimation",children:"Chapter 14: Sensor Fusion and State Estimation"})}),"\n",(0,i.jsx)(n.h2,{id:"141-state-estimation-fundamentals",children:"14.1 State Estimation Fundamentals"}),"\n",(0,i.jsx)(n.h3,{id:"1411-introduction-to-sensor-fusion",children:"14.1.1 Introduction to Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to produce more accurate, reliable, and comprehensive information than any single sensor could provide. In robotics, sensor fusion is essential for robust perception and control."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Sensor fusion addresses fundamental challenges in robotics: sensor noise, incomplete coverage, different update rates, and complementary sensor modalities. By intelligently combining measurements, robots can achieve environmental awareness that exceeds the capabilities of individual sensors."})}),"\n",(0,i.jsx)(n.h3,{id:"1412-bayesian-filtering-framework",children:"14.1.2 Bayesian Filtering Framework"}),"\n",(0,i.jsx)(n.p,{children:"Bayesian filtering provides the theoretical foundation for state estimation:"}),"\n",(0,i.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Can't use function '$' in math mode at position 87: \u20261:t-1) dx_{t-1}$\u0332$\n\nWhere:\n- \u2026",style:{color:"#cc0000"},children:'p(x_t | z_1:t) = \u03b7 * p(z_t | x_t) * \u222b p(x_t | x_{t-1}) * p(x_{t-1} | z_1:t-1) dx_{t-1}$$\n\nWhere:\n- x_t = State at time t\n- z_t = Measurement at time t\n- \u03b7 = Normalization constant\n- p(x_t | x_{t-1}) = Motion model\n- p(z_t | x_t) = Measurement model\n\n```python\nclass BayesianFilter:\n    def __init__(self, initial_state, initial_covariance):\n        self.state = initial_state\n        self.covariance = initial_covariance\n        self.state_history = [initial_state.copy()]\n        self.covariance_history = [initial_covariance.copy()]\n\n    def predict(self, motion_model, control_input, dt):\n        """Predict step using motion model"""\n        # Predict mean\n        predicted_state = motion_model.predict_state(self.state, control_input, dt)\n\n        # Predict covariance\n        predicted_covariance = motion_model.predict_covariance(\n            self.covariance, control_input, dt\n        )\n\n        self.state = predicted_state\n        self.covariance = predicted_covariance\n\n        return self.state, self.covariance\n\n    def update(self, measurement_model, measurement):\n        """Update step using measurement model"""\n        # Calculate Kalman gain\n        H = measurement_model.jacobian(self.state)\n        R = measurement_model.noise_covariance\n\n        innovation_covariance = H @ self.covariance @ H.T + R\n        kalman_gain = self.covariance @ H.T @ np.linalg.inv(innovation_covariance)\n\n        # Update state and covariance\n        predicted_measurement = measurement_model.predict_measurement(self.state)\n        innovation = measurement - predicted_measurement\n\n        self.state = self.state + kalman_gain @ innovation\n        self.covariance = (np.eye(len(self.state)) - kalman_gain @ H) @ self.covariance\n\n        return self.state, self.covariance\n\n    def get_state_estimate(self):\n        """Get current state estimate with uncertainty"""\n        return {\n            \'state\': self.state.copy(),\n            \'covariance\': self.covariance.copy(),\n            \'uncertainty\': np.diag(self.covariance)\n        }\n\n    def reset(self, new_state=None, new_covariance=None):\n        """Reset filter to new state"""\n        if new_state is not None:\n            self.state = new_state\n        if new_covariance is not None:\n            self.covariance = new_covariance\n\n        self.state_history = [self.state.copy()]\n        self.covariance_history = [self.covariance.copy()]\n```\n\n## 14.2 Kalman Filter and Variants\n\n### 14.2.1 Extended Kalman Filter (EKF)\n\nFor non-linear systems, the Extended Kalman Filter linearizes around the current estimate:\n\n```python\nclass ExtendedKalmanFilter(BayesianFilter):\n    def __init__(self, initial_state, initial_covariance):\n        super().__init__(initial_state, initial_covariance)\n\n    def predict_nonlinear(self, motion_function, control_input, dt):\n        """Predict step for non-linear motion model"""\n        # Predict state using non-linear function\n        self.state = motion_function(self.state, control_input, dt)\n\n        # Linearize around current state\n        F = self._compute_jacobian(motion_function, self.state, control_input, dt)\n        Q = motion_function.process_noise_covariance\n\n        # Predict covariance\n        self.covariance = F @ self.covariance @ F.T + Q\n\n        return self.state, self.covariance\n\n    def update_nonlinear(self, measurement_function, measurement):\n        """Update step for non-linear measurement model"""\n        # Predict measurement\n        predicted_measurement = measurement_function(self.state)\n\n        # Linearize measurement function\n        H = self._compute_jacobian(measurement_function, self.state)\n        R = measurement_function.measurement_noise_covariance\n\n        # Kalman gain\n        S = H @ self.covariance @ H.T + R\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        innovation = measurement - predicted_measurement\n        self.state = self.state + K @ innovation\n        self.covariance = (np.eye(len(self.state)) - K @ H) @ self.covariance\n\n        return self.state, self.covariance\n\n    def _compute_jacobian(self, function, state, *args):\n        """Compute Jacobian matrix using numerical differentiation"""\n        epsilon = 1e-6\n        n = len(state)\n        m = len(function(state, *args))\n\n        J = np.zeros((m, n))\n\n        for i in range(n):\n            # Perturb state\n            state_plus = state.copy()\n            state_plus[i] += epsilon\n\n            state_minus = state.copy()\n            state_minus[i] -= epsilon\n\n            # Compute function values\n            f_plus = function(state_plus, *args)\n            f_minus = function(state_minus, *args)\n\n            # Compute partial derivative\n            J[:, i] = (f_plus - f_minus) / (2 * epsilon)\n\n        return J\n\nclass RobotMotionModel:\n    def __init__(self, process_noise):\n        self.process_noise = process_noise\n\n    def predict_state(self, state, control, dt):\n        """Non-linear motion model for differential drive robot"""\n        x, y, theta, v, omega = state\n        v_cmd, omega_cmd = control\n\n        # Add control noise\n        v_actual = v_cmd + np.random.normal(0, self.process_noise[0])\n        omega_actual = omega_cmd + np.random.normal(0, self.process_noise[1])\n\n        # Update velocity (with dynamics)\n        alpha = 0.1  # Smoothing factor\n        v = alpha * v_actual + (1 - alpha) * v\n        omega = alpha * omega_actual + (1 - alpha) * omega\n\n        # Update pose\n        if abs(omega) < 1e-6:\n            # Straight line motion\n            x += v * np.cos(theta) * dt\n            y += v * np.sin(theta) * dt\n        else:\n            # Circular motion\n            R = v / omega\n            x += R * (np.sin(theta + omega * dt) - np.sin(theta))\n            y += R * (-np.cos(theta + omega * dt) + np.cos(theta))\n            theta += omega * dt\n\n        return np.array([x, y, theta, v, omega])\n\n    def predict_covariance(self, covariance, control, dt):\n        """Predict covariance using linearized motion model"""\n        # State transition matrix\n        x, y, theta, v, omega = self.current_state\n\n        F = np.eye(5)\n        F[0, 2] = -v * np.sin(theta) * dt\n        F[0, 3] = np.cos(theta) * dt\n        F[1, 2] = v * np.cos(theta) * dt\n        F[1, 3] = np.sin(theta) * dt\n        F[2, 4] = dt\n\n        # Process noise covariance\n        Q = np.diag([\n            self.process_noise[2] * dt**2,  # Position noise\n            self.process_noise[2] * dt**2,\n            self.process_noise[3] * dt**2,  # Orientation noise\n            self.process_noise[0] * dt**2,  # Velocity noise\n            self.process_noise[1] * dt**2   # Angular velocity noise\n        ])\n\n        return F @ covariance @ F.T + Q\n```\n\n### 14.2.2 Unscented Kalman Filter (UKF)\n\nThe Unscented Kalman Filter handles non-linearities without linearization:\n\n```python\nclass UnscentedKalmanFilter(BayesianFilter):\n    def __init__(self, initial_state, initial_covariance, alpha=1e-3, beta=2, kappa=0):\n        super().__init__(initial_state, initial_covariance)\n        self.alpha = alpha\n        self.beta = beta\n        self.kappa = kappa\n        self.n = len(initial_state)\n\n        # Calculate weights\n        self.lambda_ = alpha**2 * (self.n + kappa) - self.n\n        self.weights_m, self.weights_c = self._calculate_weights()\n\n    def _calculate_weights(self):\n        """Calculate weights for sigma points"""\n        n = self.n\n        lambda_ = self.lambda_\n\n        # Mean weights\n        weights_m = np.zeros(2 * n + 1)\n        weights_m[0] = lambda_ / (n + lambda_)\n        weights_m[1:] = 0.5 / (n + lambda_)\n\n        # Covariance weights\n        weights_c = np.zeros(2 * n + 1)\n        weights_c[0] = lambda_ / (n + lambda_) + (1 - alpha**2 + beta)\n        weights_c[1:] = 0.5 / (n + lambda_)\n\n        return weights_m, weights_c\n\n    def generate_sigma_points(self, state, covariance):\n        """Generate sigma points"""\n        n = self.n\n        sigma_points = np.zeros((2 * n + 1, n))\n\n        # Calculate square root of covariance\n        sqrt_cov = np.linalg.cholesky((n + self.lambda_) * covariance).T\n\n        # First sigma point is the mean\n        sigma_points[0] = state\n\n        # Generate remaining sigma points\n        for i in range(n):\n            sigma_points[i + 1] = state + sqrt_cov[i]\n            sigma_points[i + n + 1] = state - sqrt_cov[i]\n\n        return sigma_points\n\n    def predict(self, motion_function, control, dt):\n        """UKF prediction step"""\n        # Generate sigma points\n        sigma_points = self.generate_sigma_points(self.state, self.covariance)\n\n        # Propagate sigma points through motion model\n        propagated_points = np.zeros_like(sigma_points)\n        for i, point in enumerate(sigma_points):\n            propagated_points[i] = motion_function(point, control, dt)\n\n        # Calculate predicted mean\n        self.state = np.sum(self.weights_m[:, np.newaxis] * propagated_points, axis=0)\n\n        # Calculate predicted covariance\n        centered_points = propagated_points - self.state\n        self.covariance = np.sum(\n            self.weights_c[:, np.newaxis, np.newaxis] *\n            np.einsum(\'ij,ik->ijk\', centered_points, centered_points),\n            axis=0\n        )\n\n        # Add process noise\n        self.covariance += motion_function.process_noise_covariance\n\n        return self.state, self.covariance\n\n    def update(self, measurement_function, measurement):\n        """UKF update step"""\n        # Generate sigma points\n        sigma_points = self.generate_sigma_points(self.state, self.covariance)\n\n        # Propagate sigma points through measurement model\n        measurement_sigma_points = np.zeros((len(sigma_points), len(measurement)))\n        for i, point in enumerate(sigma_points):\n            measurement_sigma_points[i] = measurement_function(point)\n\n        # Predict measurement mean\n        predicted_measurement = np.sum(self.weights_m[:, np.newaxis] * measurement_sigma_points, axis=0)\n\n        # Calculate innovation covariance\n        centered_measurements = measurement_sigma_points - predicted_measurement\n        P_yy = np.sum(\n            self.weights_c[:, np.newaxis, np.newaxis] *\n            np.einsum(\'ij,ik->ijk\', centered_measurements, centered_measurements),\n            axis=0\n        )\n        P_yy += measurement_function.measurement_noise_covariance\n\n        # Calculate cross-covariance\n        centered_states = sigma_points - self.state\n        P_xy = np.sum(\n            self.weights_c[:, np.newaxis, np.newaxis] *\n            np.einsum(\'ij,ik->ijk\', centered_states, centered_measurements),\n            axis=0\n        )\n\n        # Kalman gain\n        K = P_xy @ np.linalg.inv(P_yy)\n\n        # Update state\n        innovation = measurement - predicted_measurement\n        self.state = self.state + K @ innovation\n\n        # Update covariance\n        self.covariance = self.covariance - K @ P_yy @ K.T\n\n        return self.state, self.covariance\n```\n\n## 14.3 Multi-Sensor Fusion\n\n### 14.3.1 Complementary Filter\n\nFor simple orientation estimation using accelerometer and gyroscope:\n\n```python\nclass ComplementaryFilter:\n    def __init__(self, alpha=0.98):\n        self.alpha = alpha  # Complementary filter gain\n        self.angle = 0.0\n        self.bias = 0.0\n        self.initialized = False\n\n    def update(self, accel_data, gyro_data, dt):\n        """Update orientation estimate"""\n        ax, ay, az = accel_data\n        gx, gy, gz = gyro_data\n\n        if not self.initialized:\n            # Initialize with accelerometer\n            self.angle = math.atan2(ay, az)\n            self.bias = 0.0\n            self.initialized = True\n            return self.angle\n\n        # Integrate gyroscope\n        angle_gyro = self.angle + (gz - self.bias) * dt\n\n        # Calculate angle from accelerometer\n        angle_accel = math.atan2(ay, az)\n\n        # Complementary filter\n        self.angle = self.alpha * angle_gyro + (1 - self.alpha) * angle_accel\n\n        # Estimate bias (assuming angle_accel is low-frequency)\n        self.bias = self.bias + 0.01 * (angle_gyro - angle_accel)\n\n        return self.angle\n\nclass MadgwickFilter:\n    def __init__(self, beta=0.1):\n        self.beta = beta  # Filter gain parameter\n        self.q = np.array([1.0, 0.0, 0.0, 0.0])  # Quaternion [w, x, y, z]\n\n    def update(self, accel, gyro, mag=None, dt=0.01):\n        """Update orientation using Madgwick algorithm"""\n        ax, ay, az = accel\n        gx, gy, gz = gyro\n\n        # Normalize accelerometer\n        norm = np.sqrt(ax**2 + ay**2 + az**2)\n        if norm > 0:\n            ax, ay, az = ax/norm, ay/norm, az/norm\n\n        # Rate of change of quaternion from gyroscope\n        q_dot = 0.5 * self._quaternion_multiply(\n            self.q,\n            np.array([0, gx, gy, gz])\n        )\n\n        if mag is not None:\n            mx, my, mz = mag\n\n            # Normalize magnetometer\n            norm = np.sqrt(mx**2 + my**2 + mz**2)\n            if norm > 0:\n                mx, my, mz = mx/norm, my/norm, mz/norm\n\n            # Compute objective function and Jacobian\n            f = self._compute_objective_marg(accel, mag, self.q)\n            J = self._compute_jacobian_marg(accel, mag, self.q)\n\n            # Gradient descent step\n            step = J.T @ f\n            norm = np.linalg.norm(step)\n            if norm > 0:\n                step = step / norm\n                step *= self.beta * dt\n                q_dot -= step\n\n        # Integrate quaternion\n        self.q += q_dot * dt\n\n        # Normalize quaternion\n        self.q = self.q / np.linalg.norm(self.q)\n\n        return self.q\n\n    def _quaternion_multiply(self, q1, q2):\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n\n        return np.array([\n            w1*w2 - x1*x2 - y1*y2 - z1*z2,\n            w1*x2 + x1*w2 + y1*z2 - z1*y2,\n            w1*y2 - x1*z2 + y1*w2 + z1*x2,\n            w1*z2 + x1*y2 - y1*x2 + z1*w2\n        ])\n\n    def _compute_objective_marg(self, accel, mag, q):\n        """Compute objective function for Madgwick filter"""\n        ax, ay, az = accel\n        mx, my, mz = mag\n\n        # Quaternion components\n        q0, q1, q2, q3 = q\n\n        # Rotate accelerometer into earth frame\n        ax_earth = 2 * (q1*q3 - q0*q2)\n        ay_earth = 2 * (q0*q1 + q2*q3)\n        az_earth = q0**2 - q1**2 - q2**2 + q3**2\n\n        # Rotate magnetometer into earth frame\n        hx_earth = mx * (q0**2 + q1**2 - q2**2 - q3**2) + 2 * my * (q1*q2 - q0*q3) + 2 * mz * (q1*q3 + q0*q2)\n        hy_earth = 2 * mx * (q1*q2 + q0*q3) + my * (q0**2 - q1**2 + q2**2 - q3**2) + 2 * mz * (q2*q3 - q0*q1)\n        hz_earth = 2 * mx * (q1*q3 - q0*q2) + 2 * my * (q2*q3 + q0*q1) + mz * (q0**2 - q1**2 - q2**2 + q3**2)\n\n        # Objective function (measured - expected)\n        f = np.array([\n            ax_earth - ax,  # Should be 0 for gravity\n            ay_earth - ay,\n            az_earth - az,\n            hx_earth,       # Should align with north\n            hy_earth,\n            hz_earth - np.sqrt(hx_earth**2 + hy_earth**2)\n        ])\n\n        return f\n\n    def _compute_jacobian_marg(self, accel, mag, q):\n        """Compute Jacobian matrix for Madgwick filter"""\n        # Numerical differentiation\n        epsilon = 1e-6\n        J = np.zeros((6, 4))\n\n        f0 = self._compute_objective_marg(accel, mag, q)\n\n        for i in range(4):\n            q_plus = q.copy()\n            q_plus[i] += epsilon\n\n            f_plus = self._compute_objective_marg(accel, mag, q_plus)\n            J[:, i] = (f_plus - f0) / epsilon\n\n        return J\n```\n\n### 14.3.2 Particle Filter\n\nFor highly non-linear, non-Gaussian estimation problems:\n\n```python\nclass ParticleFilter:\n    def __init__(self, num_particles, state_dim, motion_model, measurement_model):\n        self.num_particles = num_particles\n        self.state_dim = state_dim\n        self.motion_model = motion_model\n        self.measurement_model = measurement_model\n\n        # Initialize particles\n        self.particles = np.zeros((num_particles, state_dim))\n        self.weights = np.ones(num_particles) / num_particles\n        self.effective_sample_size_threshold = num_particles / 2\n\n    def initialize_particles(self, mean, covariance):\n        """Initialize particles from Gaussian distribution"""\n        self.particles = np.random.multivariate_normal(\n            mean, covariance, self.num_particles\n        )\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def predict(self, control, dt):\n        """Predict step - propagate particles"""\n        for i in range(self.num_particles):\n            # Add process noise\n            noise = np.random.multivariate_normal(\n                np.zeros(self.state_dim),\n                self.motion_model.process_noise_covariance\n            )\n\n            # Propagate particle through motion model\n            self.particles[i] = self.motion_model.predict_state(\n                self.particles[i], control, dt\n            ) + noise\n\n    def update(self, measurement):\n        """Update step - weight particles based on measurement likelihood"""\n        for i in range(self.num_particles):\n            # Calculate measurement likelihood\n            predicted_measurement = self.measurement_model.predict_measurement(\n                self.particles[i]\n            )\n\n            likelihood = self.measurement_model.calculate_likelihood(\n                predicted_measurement, measurement\n            )\n\n            # Update weight\n            self.weights[i] *= likelihood\n\n        # Normalize weights\n        total_weight = np.sum(self.weights)\n        if total_weight > 0:\n            self.weights /= total_weight\n        else:\n            self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def resample(self):\n        """Resample particles if effective sample size is low"""\n        effective_sample_size = 1.0 / np.sum(self.weights**2)\n\n        if effective_sample_size < self.effective_sample_size_threshold:\n            # Systematic resampling\n            cumsum_weights = np.cumsum(self.weights)\n            positions = (np.arange(self.num_particles) + np.random.uniform(0, 1)) / self.num_particles\n\n            indices = np.searchsorted(cumsum_weights, positions)\n            indices = np.clip(indices, 0, self.num_particles - 1)\n\n            self.particles = self.particles[indices]\n            self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def estimate_state(self):\n        """Estimate state from particles"""\n        # Weighted mean\n        mean_state = np.average(self.particles, weights=self.weights, axis=0)\n\n        # Weighted covariance\n        centered_particles = self.particles - mean_state\n        covariance = np.average(\n            np.einsum(\'ij,ik->ijk\', centered_particles, centered_particles),\n            weights=self.weights,\n            axis=0\n        )\n\n        return mean_state, covariance\n\n    def get_particle_cloud(self):\n        """Get all particles for visualization"""\n        return self.particles.copy(), self.weights.copy()\n\nclass RobotParticleFilter(ParticleFilter):\n    def __init__(self, num_particles=1000):\n        # Robot state: [x, y, theta, v, omega]\n        motion_model = DifferentialDriveMotionModel()\n        measurement_model = RobotMeasurementModel()\n\n        super().__init__(num_particles, 5, motion_model, measurement_model)\n\n    def update_from_sensors(self, control, lidar_data, camera_data):\n        """Update filter from sensor measurements"""\n        # Predict step\n        self.predict(control, 0.1)  # 10Hz update rate\n\n        # Combine measurements\n        measurement = self._combine_measurements(lidar_data, camera_data)\n\n        # Update step\n        self.update(measurement)\n\n        # Resample if necessary\n        self.resample()\n\n        # Return estimated state\n        return self.estimate_state()\n\n    def _combine_measurements(self, lidar_data, camera_data):\n        """Combine multiple sensor measurements"""\n        # Simple combination - in practice, use more sophisticated fusion\n        combined_measurement = {\n            \'landmark_observations\': lidar_data.get(\'landmarks\', []),\n            \'object_detections\': camera_data.get(\'objects\', []),\n            \'timestamp\': time.time()\n        }\n\n        return combined_measurement\n\nclass DifferentialDriveMotionModel:\n    def __init__(self):\n        # Process noise parameters\n        self.alpha1 = 0.1  # Rotation error\n        self.alpha2 = 0.1  # Translation error\n        self.alpha3 = 0.1  # Translation error\n        self.alpha4 = 0.1  # Rotation error\n\n        # Process noise covariance\n        self.process_noise_covariance = np.diag([\n            0.1**2,  # x position\n            0.1**2,  # y position\n            0.05**2, # theta\n            0.05**2, # velocity\n            0.05**2  # angular velocity\n        ])\n\n    def predict_state(self, state, control, dt):\n        """Differential drive motion model with noise"""\n        x, y, theta, v, omega = state\n        v_cmd, omega_cmd = control\n\n        # Add control noise\n        v_actual = v_cmd + np.random.normal(0, self.alpha1 * abs(v_cmd) + self.alpha2 * abs(omega_cmd))\n        omega_actual = omega_cmd + np.random.normal(0, self.alpha3 * abs(v_cmd) + self.alpha4 * abs(omega_cmd))\n\n        # Motion model\n        if abs(omega_actual) < 1e-6:\n            # Straight line motion\n            x_new = x + v_actual * np.cos(theta) * dt\n            y_new = y + v_actual * np.sin(theta) * dt\n            theta_new = theta\n        else:\n            # Circular motion\n            R = v_actual / omega_actual\n            x_new = x + R * (np.sin(theta + omega_actual * dt) - np.sin(theta))\n            y_new = y + R * (-np.cos(theta + omega_actual * dt) + np.cos(theta))\n            theta_new = theta + omega_actual * dt\n\n        # Velocity with dynamics\n        alpha = 0.1  # Smoothing factor\n        v_new = alpha * v_actual + (1 - alpha) * v\n        omega_new = alpha * omega_actual + (1 - alpha) * omega\n\n        return np.array([x_new, y_new, theta_new, v_new, omega_new])\n```\n\n## 14.4 Visual-Inertial Odometry\n\n### 14.4.1 Tightly-Coupled VIO\n\nCombining visual and inertial measurements at the feature level:\n\n```python\nclass VisualInertialOdometry:\n    def __init__(self, camera_parameters, imu_parameters):\n        self.camera = CameraModel(camera_parameters)\n        self.imu = IMUModel(imu_parameters)\n\n        # State: [position, orientation (quaternion), velocity, biases]\n        self.state = np.zeros(16)  # [p, q, v, ba, bg]\n        self.covariance = np.eye(16) * 0.1\n\n        # Visual features\n        self.features = []\n        self.max_features = 100\n\n        # EKF for VIO\n        self.ekf = ExtendedKalmanFilter(self.state, self.covariance)\n\n        # Sliding window optimization\n        self.window_size = 10\n        self.keyframes = []\n        self.imu_measurements = []\n\n    def add_imu_measurement(self, measurement, timestamp):\n        """Add IMU measurement"""\n        # Pre-integrate IMU measurements\n        if len(self.imu_measurements) > 0:\n            self._preintegrate_imu(measurement, timestamp)\n        else:\n            self.imu_measurements.append({\n                \'measurement\': measurement,\n                \'timestamp\': timestamp,\n                \'delta_state\': np.zeros(9),  # [\u0394p, \u0394v, \u0394\u03b8]\n                \'jacobian\': np.zeros((9, 12)),\n                \'covariance\': np.zeros((9, 9))\n            })\n\n    def process_image(self, image, timestamp):\n        """Process visual measurement"""\n        # Extract features\n        keypoints, descriptors = self._extract_features(image)\n\n        # Match with previous features\n        if len(self.features) > 0:\n            matches = self._match_features(descriptors)\n            self._update_with_visual_matches(matches, image, timestamp)\n\n        # Add new features\n        new_features = self._add_new_features(keypoints, descriptors, image, timestamp)\n\n        # Check for new keyframe\n        if self._should_create_keyframe(image, timestamp):\n            self._create_keyframe(image, timestamp)\n\n    def _preintegrate_imu(self, measurement, timestamp):\n        """Pre-integrate IMU measurements between frames"""\n        if len(self.imu_measurements) == 0:\n            return\n\n        last_imu = self.imu_measurements[-1]\n        dt = timestamp - last_imu[\'timestamp\']\n\n        if dt <= 0:\n            return\n\n        # Get current bias estimates\n        ba = self.state[13:16]  # Accelerometer bias\n        bg = self.state[16:19]  # Gyroscope bias\n\n        # IMU measurement\n        acc = measurement[\'acceleration\'] - ba\n        gyro = measurement[\'gyroscope\'] - bg\n\n        # Current orientation estimate\n        q = self.state[3:7]  # Quaternion [w, x, y, z]\n\n        # Pre-integration\n        delta_state = last_imu[\'delta_state\']\n\n        # Position update\n        delta_state[0:3] += last_imu[\'delta_state\'][3:6] * dt + 0.5 * self._rotate_vector(q, acc) * dt**2\n\n        # Velocity update\n        delta_state[3:6] += self._rotate_vector(q, acc) * dt\n\n        # Orientation update (small angle approximation)\n        theta_increment = gyro * dt\n        delta_state[6:9] += theta_increment\n\n        # Update pre-integrated state\n        self.imu_measurements.append({\n            \'measurement\': measurement,\n            \'timestamp\': timestamp,\n            \'delta_state\': delta_state,\n            \'jacobian\': last_imu[\'jacobian\'],\n            \'covariance\': last_imu[\'covariance\']\n        })\n\n    def _rotate_vector(self, q, v):\n        """Rotate vector by quaternion"""\n        w, x, y, z = q\n        rotation_matrix = np.array([\n            [1-2*(y**2+z**2), 2*(x*y-w*z), 2*(x*z+w*y)],\n            [2*(x*y+w*z), 1-2*(x**2+z**2), 2*(y*z-w*x)],\n            [2*(x*z-w*y), 2*(y*z+w*x), 1-2*(x**2+y**2)]\n        ])\n\n        return rotation_matrix @ v\n\n    def _update_with_visual_matches(self, matches, image, timestamp):\n        """Update state with visual measurements"""\n        if len(matches) < 5:\n            return  # Not enough matches\n\n        # Get current state\n        position = self.state[0:3]\n        orientation = self.state[3:7]\n        velocity = self.state[7:10]\n\n        # Visual measurement function\n        def visual_measurement_function(state):\n            pos = state[0:3]\n            ori = state[3:7]\n\n            measurements = []\n            for match in matches:\n                feature_pos = self.features[match.queryIdx][\'position\']\n\n                # Project feature to image plane\n                pixel = self.camera.project_3d_to_2d(feature_pos, pos, ori)\n                measurements.append(pixel)\n\n            return np.array(measurements)\n\n        # Measurement noise\n        visual_noise = np.eye(len(matches) * 2) * 1.0  # 1 pixel standard deviation\n\n        # Predicted measurement\n        predicted = visual_measurement_function(self.state)\n\n        # Actual measurement\n        actual = np.array([\n            self.features[match.trainIdx][\'pixel\'] for match in matches\n        ]).flatten()\n\n        # Update using EKF\n        self.state, self.covariance = self.ekf.update_nonlinear(\n            visual_measurement_function, actual\n        )\n\n    def estimate_trajectory(self):\n        """Get current trajectory estimate"""\n        return {\n            \'position\': self.state[0:3].copy(),\n            \'orientation\': self.state[3:7].copy(),\n            \'velocity\': self.state[7:10].copy(),\n            \'uncertainty\': np.diag(self.covariance[:9, :9])\n        }\n\nclass IMUModel:\n    def __init__(self, parameters):\n        self.gravity = parameters.get(\'gravity\', 9.81)\n        self.noise_accelerometer = parameters.get(\'noise_accelerometer\', 0.1)\n        self.noise_gyroscope = parameters.get(\'noise_gyroscope\', 0.01)\n        self.bias_stability_accel = parameters.get(\'bias_stability_accel\', 0.001)\n        self.bias_stability_gyro = parameters.get(\'bias_stability_gyro\', 0.0001)\n\n    def propagate_state(self, state, measurement, dt):\n        """Propagate IMU state"""\n        x, y, z, vx, vy, vz, qw, qx, qy, qz = state\n        ax, ay, az, gx, gy, gz = measurement\n\n        # Remove gravity\n        ax -= 0  # Assuming IMU is gravity-aligned in world frame\n        ay -= 0\n        az -= self.gravity\n\n        # Update velocity\n        vx += ax * dt\n        vy += ay * dt\n        vz += az * dt\n\n        # Update position\n        x += vx * dt\n        y += vy * dt\n        z += vz * dt\n\n        # Update orientation (simplified)\n        # In practice, use quaternion integration\n        dw = 0.5 * (-qx * gx - qy * gy - qz * gz)\n        dx = 0.5 * (qw * gx + qy * gz - qz * gy)\n        dy = 0.5 * (qw * gy - qx * gz + qz * gx)\n        dz = 0.5 * (qw * gz + qx * gy - qy * gx)\n\n        qw += dw * dt\n        qx += dx * dt\n        qy += dy * dt\n        qz += dz * dt\n\n        # Normalize quaternion\n        norm = np.sqrt(qw**2 + qx**2 + qy**2 + qz**2)\n        if norm > 0:\n            qw, qx, qy, qz = qw/norm, qx/norm, qy/norm, qz/norm\n\n        return np.array([x, y, z, vx, vy, vz, qw, qx, qy, qz])\n```\n\n## Chapter Summary\n\nThis chapter covered advanced sensor fusion and state estimation techniques for robotics:\n\n### Key Concepts Covered\n1. **Bayesian Filtering**: Mathematical foundation for recursive state estimation\n2. **Kalman Filter Family**: Linear, Extended, and Unscented Kalman Filters\n3. **Multi-Sensor Fusion**: Complementary filters and adaptive fusion techniques\n4. **Particle Filters**: Non-parametric estimation for highly non-linear systems\n5. **Visual-Inertial Odometry**: Tightly-coupled fusion of visual and inertial data\n6. **State Estimation**: Robust estimation under uncertainty and noise\n\n### Practical Implementations\n- Complete Extended Kalman Filter with Jacobian computation\n- Unscented Kalman Filter with sigma point generation\n- Complementary and Madgwick filters for orientation estimation\n- Particle filter for differential drive robot localization\n- Visual-Inertial Odometry with IMU pre-integration\n- Multi-modal sensor fusion architectures\n\n### Next Steps\nWith sensor fusion expertise, you\'re ready for:\n- Chapter 15: SLAM, VSLAM, and Navigation\n- Chapter 16: Path Planning Algorithms\n- Part V: Embodied Intelligence & VLA\n\n---\n\n## Glossary Terms\n\n**Term**: **Kalman Gain**\n**Definition**: Matrix that determines how much to trust new measurements versus predicted state in Kalman filtering\n**Related**: **State Estimation**, **Bayesian Filtering**\n\n**Term**: **Sigma Points**\n**Definition**: Sample points used in Unscented Kalman Filter to capture mean and covariance of non-linear transformations\n**Related**: **Unscented Transform**, **UKF**\n\n**Term**: **Pre-integration**\n**Definition**: Technique to integrate high-frequency IMU measurements between visual updates in VIO\n**Related**: **Visual-Inertial Odometry**, **IMU Processing**\n\n**Term**: **Effective Sample Size**\n**Definition**: Metric indicating the diversity of particles in particle filter, used to determine when resampling is needed\n**Related**: **Particle Filter**, **Resampling**\n\n**Term**: **Complementary Filter**\n**Definition**: Filter that combines high-frequency and low-frequency sensor measurements using complementary filter gains\n**Related**: **Sensor Fusion**, **Orientation Estimation**\n\n---\n\n## Exercises\n\n### Exercise 14.1: Extended Kalman Filter\nImplement EKF for mobile robot:\n- Define non-linear motion model\n- Compute Jacobian matrices numerically\n- Handle GPS and odometry measurements\n- Visualize uncertainty ellipses\n\n### Exercise 14.2: Unscented Kalman Filter\nCreate UKF for quadrotor attitude estimation:\n- Implement sigma point generation\n- Handle quaternion representation\n- Process accelerometer, gyroscope, magnetometer\n- Compare with EKF performance\n\n### Exercise 14.3: Particle Filter SLAM\nBuild particle filter-based SLAM:\n- Implement FastSLAM algorithm\n- Handle data association uncertainty\n- Create efficient resampling strategy\n- Test in simulation environment\n\n### Exercise 14.4: Visual-Inertial Odometry\nImplement tightly-coupled VIO:\n- Pre-integrate IMU measurements\n- Track visual features with uncertainty\n- Optimize sliding window poses\n- Evaluate trajectory accuracy\n\n### Exercise 14.5: Multi-Sensor Fusion\nCreate adaptive sensor fusion system:\n- Dynamically weight sensor measurements\n- Detect and handle sensor failures\n- Implement fault-tolerant estimation\n- Benchmark against ground truth'})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);