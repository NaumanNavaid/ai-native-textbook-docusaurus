"use strict";(globalThis.webpackChunkai_native_textbook_docusaurus=globalThis.webpackChunkai_native_textbook_docusaurus||[]).push([[1609],{8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var s=r(6540);const a={},t=s.createContext(a);function i(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(t.Provider,{value:n},e.children)}},9810:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"part-4-perception/chapter-15-slam-vslam-navigation","title":"SLAM, VSLAM, and Navigation","description":"15.1 SLAM Fundamentals","source":"@site/docs/part-4-perception/chapter-15-slam-vslam-navigation.mdx","sourceDirName":"part-4-perception","slug":"/part-4-perception/chapter-15-slam-vslam-navigation","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-15-slam-vslam-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/NaumanNavaid/ai-native-textbook-docusaurus/tree/main/docs/part-4-perception/chapter-15-slam-vslam-navigation.mdx","tags":[],"version":"current","frontMatter":{"title":"SLAM, VSLAM, and Navigation","part":4,"chapter":15,"difficulty":"advanced","prerequisites":["chapter-14-sensor-fusion-state-estimation","chapter-13-computer-vision-robots"],"estimatedTime":65,"objectives":["Master SLAM algorithms and architectures","Implement visual SLAM with loop closure detection","Develop robust navigation systems","Apply SLAM to real-world robotic applications"]},"sidebar":"chaptersSidebar","previous":{"title":"Sensor Fusion and State Estimation","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-14-sensor-fusion-state-estimation"},"next":{"title":"Path Planning Algorithms","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-16-path-planning-algorithms"}}');var a=r(4848),t=r(8453);const i={title:"SLAM, VSLAM, and Navigation",part:4,chapter:15,difficulty:"advanced",prerequisites:["chapter-14-sensor-fusion-state-estimation","chapter-13-computer-vision-robots"],estimatedTime:65,objectives:["Master SLAM algorithms and architectures","Implement visual SLAM with loop closure detection","Develop robust navigation systems","Apply SLAM to real-world robotic applications"]},l="Chapter 15: SLAM, VSLAM, and Navigation",o={},c=[{value:"15.1 SLAM Fundamentals",id:"151-slam-fundamentals",level:2},{value:"15.1.1 The SLAM Problem",id:"1511-the-slam-problem",level:3},{value:"15.1.2 SLAM Mathematical Framework",id:"1512-slam-mathematical-framework",level:3},{value:"15.2 Extended Kalman Filter SLAM",id:"152-extended-kalman-filter-slam",level:2},{value:"15.2.1 EKF-SLAM Implementation",id:"1521-ekf-slam-implementation",level:3},{value:"15.3 Visual SLAM (VSLAM)",id:"153-visual-slam-vslam",level:2},{value:"15.3.1 Feature-Based VSLAM",id:"1531-feature-based-vslam",level:3},{value:"15.3.2 Dense SLAM with RGB-D",id:"1532-dense-slam-with-rgb-d",level:3},{value:"15.4 Navigation with SLAM",id:"154-navigation-with-slam",level:2},{value:"15.4.1 Path Planning in SLAM Maps",id:"1541-path-planning-in-slam-maps",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:3},{value:"Practical Implementations",id:"practical-implementations",level:3},{value:"Next Steps",id:"next-steps",level:3},{value:"Glossary Terms",id:"glossary-terms",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 15.1: EKF-SLAM Implementation",id:"exercise-151-ekf-slam-implementation",level:3},{value:"Exercise 15.2: Feature-Based VSLAM",id:"exercise-152-feature-based-vslam",level:3},{value:"Exercise 15.3: Dense RGB-D SLAM",id:"exercise-153-dense-rgb-d-slam",level:3},{value:"Exercise 15.4: SLAM Navigation",id:"exercise-154-slam-navigation",level:3},{value:"Exercise 15.5: Multi-Robot SLAM",id:"exercise-155-multi-robot-slam",level:3}];function m(e){const n={admonition:"admonition",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-15-slam-vslam-and-navigation",children:"Chapter 15: SLAM, VSLAM, and Navigation"})}),"\n",(0,a.jsx)(n.h2,{id:"151-slam-fundamentals",children:"15.1 SLAM Fundamentals"}),"\n",(0,a.jsx)(n.h3,{id:"1511-the-slam-problem",children:"15.1.1 The SLAM Problem"}),"\n",(0,a.jsx)(n.p,{children:"Simultaneous Localization and Mapping (SLAM) is one of the fundamental challenges in robotics. The robot must build a map of an unknown environment while simultaneously estimating its position within that map."}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsx)(n.p,{children:"SLAM addresses the chicken-and-egg problem: to build a map, you need to know your position; to know your position, you need a map. The solution lies in the probabilistic framework that estimates both pose and map simultaneously while maintaining uncertainty estimates."})}),"\n",(0,a.jsx)(n.h3,{id:"1512-slam-mathematical-framework",children:"15.1.2 SLAM Mathematical Framework"}),"\n",(0,a.jsx)(n.p,{children:"The SLAM problem can be formulated as a Bayesian estimation problem:"}),"\n",(0,a.jsx)(n.span,{className:"katex-display",children:(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"p"}),(0,a.jsx)(n.mo,{stretchy:"false",children:"("}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mn,{children:"1"}),(0,a.jsx)(n.mo,{children:":"}),(0,a.jsx)(n.mi,{children:"t"})]})]}),(0,a.jsx)(n.mo,{separator:"true",children:","}),(0,a.jsx)(n.mi,{children:"m"}),(0,a.jsx)(n.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"z"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mn,{children:"1"}),(0,a.jsx)(n.mo,{children:":"}),(0,a.jsx)(n.mi,{children:"t"})]})]}),(0,a.jsx)(n.mo,{separator:"true",children:","}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"u"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mn,{children:"1"}),(0,a.jsx)(n.mo,{children:":"}),(0,a.jsx)(n.mi,{children:"t"})]})]}),(0,a.jsx)(n.mo,{stretchy:"false",children:")"}),(0,a.jsx)(n.mo,{children:"="}),(0,a.jsx)(n.mi,{children:"\u03b7"}),(0,a.jsx)(n.mo,{children:"\u22c5"}),(0,a.jsx)(n.mi,{children:"p"}),(0,a.jsx)(n.mo,{stretchy:"false",children:"("}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"z"}),(0,a.jsx)(n.mi,{children:"t"})]}),(0,a.jsx)(n.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mi,{children:"t"})]}),(0,a.jsx)(n.mo,{separator:"true",children:","}),(0,a.jsx)(n.mi,{children:"m"}),(0,a.jsx)(n.mo,{stretchy:"false",children:")"}),(0,a.jsx)(n.mo,{children:"\u22c5"}),(0,a.jsx)(n.mo,{children:"\u222b"}),(0,a.jsx)(n.mi,{children:"p"}),(0,a.jsx)(n.mo,{stretchy:"false",children:"("}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsx)(n.mi,{children:"t"})]}),(0,a.jsx)(n.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"t"}),(0,a.jsx)(n.mo,{children:"\u2212"}),(0,a.jsx)(n.mn,{children:"1"})]})]}),(0,a.jsx)(n.mo,{separator:"true",children:","}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"u"}),(0,a.jsx)(n.mi,{children:"t"})]}),(0,a.jsx)(n.mo,{stretchy:"false",children:")"}),(0,a.jsx)(n.mo,{children:"\u22c5"}),(0,a.jsx)(n.mi,{children:"p"}),(0,a.jsx)(n.mo,{stretchy:"false",children:"("}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mn,{children:"1"}),(0,a.jsx)(n.mo,{children:":"}),(0,a.jsx)(n.mi,{children:"t"}),(0,a.jsx)(n.mo,{children:"\u2212"}),(0,a.jsx)(n.mn,{children:"1"})]})]}),(0,a.jsx)(n.mo,{separator:"true",children:","}),(0,a.jsx)(n.mi,{children:"m"}),(0,a.jsx)(n.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"z"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mn,{children:"1"}),(0,a.jsx)(n.mo,{children:":"}),(0,a.jsx)(n.mi,{children:"t"}),(0,a.jsx)(n.mo,{children:"\u2212"}),(0,a.jsx)(n.mn,{children:"1"})]})]}),(0,a.jsx)(n.mo,{separator:"true",children:","}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"u"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mn,{children:"1"}),(0,a.jsx)(n.mo,{children:":"}),(0,a.jsx)(n.mi,{children:"t"}),(0,a.jsx)(n.mo,{children:"\u2212"}),(0,a.jsx)(n.mn,{children:"1"})]})]}),(0,a.jsx)(n.mo,{stretchy:"false",children:")"}),(0,a.jsx)(n.mi,{children:"d"}),(0,a.jsxs)(n.msub,{children:[(0,a.jsx)(n.mi,{children:"x"}),(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"t"}),(0,a.jsx)(n.mo,{children:"\u2212"}),(0,a.jsx)(n.mn,{children:"1"})]})]})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"p(x_{1:t}, m | z_{1:t}, u_{1:t}) = \\eta \\cdot p(z_t | x_t, m) \\cdot \\int p(x_t | x_{t-1}, u_t) \\cdot p(x_{1:t-1}, m | z_{1:t-1}, u_{1:t-1}) dx_{t-1}"})]})})}),(0,a.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(n.span,{className:"mopen",children:"("}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mtight",children:"1"}),(0,a.jsx)(n.span,{className:"mrel mtight",children:":"}),(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mpunct",children:","}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,a.jsx)(n.span,{className:"mord",children:"\u2223"}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.04398em"},children:"z"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"-0.044em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mtight",children:"1"}),(0,a.jsx)(n.span,{className:"mrel mtight",children:":"}),(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mpunct",children:","}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"u"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mtight",children:"1"}),(0,a.jsx)(n.span,{className:"mrel mtight",children:":"}),(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mclose",children:")"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(n.span,{className:"mrel",children:"="}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6389em",verticalAlign:"-0.1944em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03b7"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(n.span,{className:"mbin",children:"\u22c5"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(n.span,{className:"mopen",children:"("}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.04398em"},children:"z"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2806em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"-0.044em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mord",children:"\u2223"}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2806em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mpunct",children:","}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,a.jsx)(n.span,{className:"mclose",children:")"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(n.span,{className:"mbin",children:"\u22c5"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"2.2222em",verticalAlign:"-0.8622em"}}),(0,a.jsx)(n.span,{className:"mop op-symbol large-op",style:{marginRight:"0.44445em",position:"relative",top:"-0.0011em"},children:"\u222b"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(n.span,{className:"mopen",children:"("}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2806em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mord",children:"\u2223"}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,a.jsx)(n.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(n.span,{className:"mord mtight",children:"1"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2083em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mpunct",children:","}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"u"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2806em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mclose",children:")"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(n.span,{className:"mbin",children:"\u22c5"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(n.span,{className:"mopen",children:"("}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mtight",children:"1"}),(0,a.jsx)(n.span,{className:"mrel mtight",children:":"}),(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,a.jsx)(n.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(n.span,{className:"mord mtight",children:"1"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2083em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mpunct",children:","}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,a.jsx)(n.span,{className:"mord",children:"\u2223"}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.04398em"},children:"z"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"-0.044em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mtight",children:"1"}),(0,a.jsx)(n.span,{className:"mrel mtight",children:":"}),(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,a.jsx)(n.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(n.span,{className:"mord mtight",children:"1"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2083em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mpunct",children:","}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"u"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mtight",children:"1"}),(0,a.jsx)(n.span,{className:"mrel mtight",children:":"}),(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,a.jsx)(n.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(n.span,{className:"mord mtight",children:"1"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2083em"},children:(0,a.jsx)(n.span,{})})})]})})]}),(0,a.jsx)(n.span,{className:"mclose",children:")"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"d"}),(0,a.jsxs)(n.span,{className:"mord",children:[(0,a.jsx)(n.span,{className:"mord mathnormal",children:"x"}),(0,a.jsx)(n.span,{className:"msupsub",children:(0,a.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(n.span,{className:"vlist-r",children:[(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(n.span,{className:"mord mtight",children:[(0,a.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"}),(0,a.jsx)(n.span,{className:"mbin mtight",children:"\u2212"}),(0,a.jsx)(n.span,{className:"mord mtight",children:"1"})]})})]})}),(0,a.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(n.span,{className:"vlist-r",children:(0,a.jsx)(n.span,{className:"vlist",style:{height:"0.2083em"},children:(0,a.jsx)(n.span,{})})})]})})]})]})]})]})}),"\n",(0,a.jsx)(n.p,{children:"Where:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"x_{1:t}"})," = Robot poses over time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"m"})," = Map (landmarks or occupancy grid)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"z_{1:t}"})," = Measurements over time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"u_{1:t}"})," = Controls over time"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class SLAMFramework:\r\n    def __init__(self, initial_pose, map_parameters):\r\n        self.robot_pose = initial_pose\r\n        self.map = Map(map_parameters)\r\n        self.landmarks = []\r\n        self.trajectory = [initial_pose.copy()]\r\n        self.uncertainty = np.eye(3) * 0.1  # Initial pose uncertainty\r\n\r\n    def predict(self, control, dt):\r\n        """Predict robot pose using motion model"""\r\n        # Update pose using control input\r\n        self.robot_pose = self.motion_model(self.robot_pose, control, dt)\r\n\r\n        # Propagate uncertainty\r\n        self.uncertainty = self.propagate_uncertainty(control, dt)\r\n\r\n        return self.robot_pose.copy(), self.uncertainty.copy()\r\n\r\n    def update(self, measurements):\r\n        """Update pose and map with new measurements"""\r\n        # Data association - match measurements to landmarks\r\n        associations = self.data_association(measurements)\r\n\r\n        for measurement, landmark_id in associations:\r\n            if landmark_id == \'new\':\r\n                # Initialize new landmark\r\n                self.initialize_landmark(measurement)\r\n            else:\r\n                # Update existing landmark and robot pose\r\n                self.update_with_known_landmark(measurement, landmark_id)\r\n\r\n        # Store trajectory\r\n        self.trajectory.append(self.robot_pose.copy())\r\n\r\n    def motion_model(self, pose, control, dt):\r\n        """Differential drive motion model"""\r\n        x, y, theta = pose\r\n        v, omega = control\r\n\r\n        # Add control noise\r\n        v_noisy = v + np.random.normal(0, 0.1)\r\n        omega_noisy = omega + np.random.normal(0, 0.1)\r\n\r\n        # Update pose\r\n        if abs(omega_noisy) < 1e-6:\r\n            # Straight line motion\r\n            x_new = x + v_noisy * np.cos(theta) * dt\r\n            y_new = y + v_noisy * np.sin(theta) * dt\r\n            theta_new = theta\r\n        else:\r\n            # Circular motion\r\n            R = v_noisy / omega_noisy\r\n            x_new = x + R * (np.sin(theta + omega_noisy * dt) - np.sin(theta))\r\n            y_new = y + R * (-np.cos(theta + omega_noisy * dt) + np.cos(theta))\r\n            theta_new = theta + omega_noisy * dt\r\n\r\n        return np.array([x_new, y_new, theta_new])\r\n\r\n    def data_association(self, measurements):\r\n        """Associate measurements with known landmarks"""\r\n        associations = []\r\n\r\n        for measurement in measurements:\r\n            best_landmark = None\r\n            best_distance = float(\'inf\')\r\n\r\n            # Find nearest landmark\r\n            for i, landmark in enumerate(self.landmarks):\r\n                predicted_measurement = self.predict_measurement(landmark)\r\n                distance = np.linalg.norm(measurement - predicted_measurement)\r\n\r\n                if distance < best_distance and distance < 2.0:  # Association threshold\r\n                    best_distance = distance\r\n                    best_landmark = i\r\n\r\n            if best_landmark is not None:\r\n                associations.append((measurement, best_landmark))\r\n            else:\r\n                associations.append((measurement, \'new\'))\r\n\r\n        return associations\r\n\r\n    def predict_measurement(self, landmark):\r\n        """Predict measurement from landmark position"""\r\n        dx = landmark[\'position\'][0] - self.robot_pose[0]\r\n        dy = landmark[\'position\'][1] - self.robot_pose[1]\r\n        distance = np.sqrt(dx**2 + dy**2)\r\n\r\n        # Bearing to landmark\r\n        bearing = np.arctan2(dy, dx) - self.robot_pose[2]\r\n\r\n        return np.array([distance, bearing])\n'})}),"\n",(0,a.jsx)(n.h2,{id:"152-extended-kalman-filter-slam",children:"15.2 Extended Kalman Filter SLAM"}),"\n",(0,a.jsx)(n.h3,{id:"1521-ekf-slam-implementation",children:"15.2.1 EKF-SLAM Implementation"}),"\n",(0,a.jsx)(n.p,{children:"Extended Kalman Filter SLAM maintains joint state of robot pose and landmark positions:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class EKFSLAM:\r\n    def __init__(self, initial_pose, initial_covariance):\r\n        # State: [x, y, theta, l1_x, l1_y, l2_x, l2_y, ...]\r\n        self.state = np.zeros(3)\r\n        self.state[:3] = initial_pose\r\n\r\n        # Covariance matrix\r\n        self.covariance = np.eye(3) * 0.1\r\n        self.initial_covariance = initial_covariance\r\n\r\n        # Process and measurement noise\r\n        self.Q = np.diag([0.1, 0.1, 0.01])  # Process noise\r\n        self.R = np.diag([0.5, 0.1])  # Measurement noise (range, bearing)\r\n\r\n        # Landmarks\r\n        self.landmarks = []\r\n        self.next_landmark_id = 0\r\n\r\n        # Data association parameters\r\n        self.gating_threshold = 3.0  # Mahalanobis distance threshold\r\n\r\n    def predict(self, control, dt):\r\n        """Predict robot pose and covariance"""\r\n        # Extract robot pose\r\n        pose = self.state[:3]\r\n\r\n        # Predict new pose\r\n        new_pose = self.motion_model(pose, control, dt)\r\n        self.state[:3] = new_pose\r\n\r\n        # Jacobian of motion model\r\n        F = self.motion_jacobian(pose, control, dt)\r\n\r\n        # Augment Jacobian for landmarks (identity)\r\n        n_landmarks = len(self.landmarks)\r\n        F_full = np.eye(3 + 2*n_landmarks)\r\n        F_full[:3, :3] = F\r\n\r\n        # Update covariance\r\n        Q_full = np.eye(3 + 2*n_landmarks)\r\n        Q_full[:3, :3] = self.Q\r\n\r\n        self.covariance = F_full @ self.covariance @ F_full.T + Q_full\r\n\r\n    def update(self, measurements):\r\n        """Update with new measurements"""\r\n        # Data association\r\n        associations = self.associate_observations(measurements)\r\n\r\n        for meas, landmark_id in associations:\r\n            if landmark_id == \'new\':\r\n                # Initialize new landmark\r\n                self.initialize_landmark(meas)\r\n            else:\r\n                # Update with known landmark\r\n                self.update_landmark(meas, landmark_id)\r\n\r\n    def associate_observations(self, measurements):\r\n        """Associate measurements with landmarks using gating"""\r\n        associations = []\r\n        used_landmarks = set()\r\n\r\n        for meas in measurements:\r\n            best_match = None\r\n            best_nis = float(\'inf\')\r\n\r\n            # Check against all landmarks\r\n            for i, landmark in enumerate(self.landmarks):\r\n                if i in used_landmarks:\r\n                    continue\r\n\r\n                # Get landmark state index\r\n                landmark_state_idx = 3 + 2*i\r\n\r\n                # Predict measurement\r\n                z_pred = self.predict_landmark_measurement(landmark_state_idx)\r\n\r\n                # Innovation covariance\r\n                H = self.measurement_jacobian(landmark_state_idx)\r\n                S = H @ self.covariance[landmark_state_idx:landmark_state_idx+2,\r\n                                          landmark_state_idx:landmark_state_idx+2] @ H.T + self.R\r\n\r\n                # Normalized innovation squared\r\n                innovation = meas - z_pred\r\n                nis = innovation.T @ np.linalg.inv(S) @ innovation\r\n\r\n                # Gating\r\n                if nis < self.gating_threshold and nis < best_nis:\r\n                    best_nis = nis\r\n                    best_match = i\r\n\r\n            if best_match is not None:\r\n                associations.append((meas, best_match))\r\n                used_landmarks.add(best_match)\r\n            else:\r\n                associations.append((meas, \'new\'))\r\n\r\n        return associations\r\n\r\n    def predict_landmark_measurement(self, landmark_idx):\r\n        """Predict measurement for landmark"""\r\n        # Landmark position\r\n        lx = self.state[landmark_idx]\r\n        ly = self.state[landmark_idx + 1]\r\n\r\n        # Robot pose\r\n        x, y, theta = self.state[:3]\r\n\r\n        # Relative position\r\n        dx = lx - x\r\n        dy = ly - y\r\n\r\n        # Range and bearing\r\n        range_pred = np.sqrt(dx**2 + dy**2)\r\n        bearing_pred = np.arctan2(dy, dx) - theta\r\n\r\n        return np.array([range_pred, bearing_pred])\r\n\r\n    def measurement_jacobian(self, landmark_idx):\r\n        """Jacobian of measurement function"""\r\n        # Landmark position\r\n        lx = self.state[landmark_idx]\r\n        ly = self.state[landmark_idx + 1]\r\n\r\n        # Robot pose\r\n        x, y, theta = self.state[:3]\r\n\r\n        # Relative position\r\n        dx = lx - x\r\n        dy = ly - y\r\n        dist = np.sqrt(dx**2 + dy**2)\r\n\r\n        # Jacobian w.r.t robot pose\r\n        H_robot = np.zeros((2, 3))\r\n        H_robot[0, 0] = -dx / dist  # \u2202range/\u2202x\r\n        H_robot[0, 1] = -dy / dist  # \u2202range/\u2202y\r\n        H_robot[1, 0] = -dy / (dist**2)  # \u2202bearing/\u2202x\r\n        H_robot[1, 1] =  dx / (dist**2)   # \u2202bearing/\u2202y\r\n        H_robot[1, 2] = -1              # \u2202bearing/\u2202\u03b8\r\n\r\n        # Jacobian w.r.t landmark position\r\n        H_landmark = np.zeros((2, 2))\r\n        H_landmark[0, 0] = dx / dist   # \u2202range/\u2202lx\r\n        H_landmark[0, 1] = dy / dist   # \u2202range/\u2202ly\r\n        H_landmark[1, 0] = -dy / (dist**2)  # \u2202bearing/\u2202lx\r\n        H_landmark[1, 1] = dx / (dist**2)   # \u2202bearing/\u2202ly\r\n\r\n        # Full Jacobian\r\n        state_size = len(self.state)\r\n        H_full = np.zeros((2, state_size))\r\n        H_full[:, :3] = H_robot\r\n        H_full[:, landmark_idx:landmark_idx+2] = H_landmark\r\n\r\n        return H_full\r\n\r\n    def update_landmark(self, measurement, landmark_id):\r\n        """Update landmark and robot pose using EKF"""\r\n        landmark_idx = 3 + 2 * landmark_id\r\n\r\n        # Predict measurement\r\n        z_pred = self.predict_landmark_measurement(landmark_idx)\r\n\r\n        # Jacobian\r\n        H = self.measurement_jacobian(landmark_idx)\r\n\r\n        # Innovation and covariance\r\n        innovation = measurement - z_pred\r\n        S = H @ self.covariance @ H.T + self.R\r\n\r\n        # Kalman gain\r\n        K = self.covariance @ H.T @ np.linalg.inv(S)\r\n\r\n        # State update\r\n        self.state += K @ innovation\r\n\r\n        # Covariance update\r\n        I_KH = np.eye(len(self.state)) - K @ H\r\n        self.covariance = I_KH @ self.covariance @ I_KH.T + K @ self.R @ K.T\r\n\r\n    def initialize_landmark(self, measurement):\r\n        """Initialize new landmark from measurement"""\r\n        # Robot pose\r\n        x, y, theta = self.state[:3]\r\n\r\n        # Landmark position in world frame\r\n        range_obs, bearing_obs = measurement\r\n\r\n        lx = x + range_obs * np.cos(bearing_obs + theta)\r\n        ly = y + range_obs * np.sin(bearing_obs + theta)\r\n\r\n        # Add to state\r\n        self.state = np.append(self.state, [lx, ly])\r\n\r\n        # Augment covariance\r\n        n = len(self.state) - 2\r\n        new_covariance = np.eye(n + 2)\r\n        new_covariance[:n, :n] = self.covariance\r\n\r\n        # Initial landmark uncertainty\r\n        new_covariance[n:n+2, n:n+2] = self.initial_covariance\r\n\r\n        # Cross-covariances\r\n        G = self.landmark_initialization_jacobian(range_obs, bearing_obs, theta)\r\n        new_covariance[:3, n:n+2] = G.T\r\n        new_covariance[n:n+2, :3] = G\r\n\r\n        self.covariance = new_covariance\r\n        self.landmarks.append({\r\n            \'id\': self.next_landmark_id,\r\n            \'position\': np.array([lx, ly]),\r\n            \'observations\': 1\r\n        })\r\n\r\n        self.next_landmark_id += 1\n'})}),"\n",(0,a.jsx)(n.h2,{id:"153-visual-slam-vslam",children:"15.3 Visual SLAM (VSLAM)"}),"\n",(0,a.jsx)(n.h3,{id:"1531-feature-based-vslam",children:"15.3.1 Feature-Based VSLAM"}),"\n",(0,a.jsx)(n.p,{children:"Visual SLAM uses visual features from camera images:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class VisualSLAM:\r\n    def __init__(self, camera_parameters):\r\n        self.camera = PinholeCamera(camera_parameters)\r\n        self.feature_detector = ORBDetector()\r\n        self.feature_matcher = BFMatcher()\r\n\r\n        # Keyframes and map points\r\n        self.keyframes = []\r\n        self.map_points = []\r\n        self.current_pose = np.eye(4)\r\n\r\n        # Tracking state\r\n        self.tracking_state = \'NOT_INITIALIZED\'\r\n        self.min_matches_for_tracking = 15\r\n        self.min_matches_for_keyframe = 50\r\n\r\n        # Loop closure detection\r\n        self.loop_closure_detector = LoopClosureDetector()\r\n        self.loop_closure_threshold = 0.75\r\n\r\n    def process_frame(self, image, timestamp):\r\n        """Process new camera frame"""\r\n        # Extract features\r\n        keypoints, descriptors = self.feature_detector.detect_and_compute(image)\r\n\r\n        if self.tracking_state == \'NOT_INITIALIZED\':\r\n            # Initialize map with first frame\r\n            self.initialize_map(image, keypoints, descriptors, timestamp)\r\n            self.tracking_state = \'OK\'\r\n            return self.current_pose\r\n\r\n        # Track current frame\r\n        tracked = self.track_frame(keypoints, descriptors, image, timestamp)\r\n\r\n        if not tracked:\r\n            # Tracking lost - try to relocalize\r\n            self.relocalize(image, keypoints, descriptors)\r\n            self.tracking_state = \'LOST\'\r\n\r\n        # Check for loop closure\r\n        if self.tracking_state == \'OK\':\r\n            loop_closure = self.detect_loop_closure(image, keypoints, descriptors)\r\n            if loop_closure:\r\n                self.perform_loop_closure(loop_closure)\r\n\r\n        return self.current_pose, self.map_points\r\n\r\n    def initialize_map(self, image, keypoints, descriptors, timestamp):\r\n        """Initialize map with first frame"""\r\n        # Create initial keyframe\r\n        initial_keyframe = KeyFrame(\r\n            image=image.copy(),\r\n            keypoints=keypoints,\r\n            descriptors=descriptors,\r\n            pose=np.eye(4),\r\n            timestamp=timestamp\r\n        )\r\n        self.keyframes.append(initial_keyframe)\r\n\r\n        # Initialize map points from keypoints\r\n        for i, (kp, desc) in enumerate(zip(keypoints, descriptors)):\r\n            map_point = MapPoint(\r\n                position=self._triangulate_initial_point(kp, np.eye(4)),\r\n                descriptor=desc,\r\n                observations=[{\r\n                    \'keyframe_id\': 0,\r\n                    \'keypoint_idx\': i,\r\n                    \'pixel_coord\': kp.pt,\r\n                    \'depth\': None\r\n                }]\r\n            )\r\n            self.map_points.append(map_point)\r\n\r\n    def track_frame(self, keypoints, descriptors, image, timestamp):\r\n        """Track current frame against map"""\r\n        # Match features with last keyframe\r\n        last_keyframe = self.keyframes[-1]\r\n        matches = self.feature_matcher.match(last_keyframe.descriptors, descriptors)\r\n\r\n        if len(matches) < self.min_matches_for_tracking:\r\n            return False\r\n\r\n        # Separate tracked and new features\r\n        tracked_points = []\r\n        new_keypoints = []\r\n        new_descriptors = []\r\n\r\n        tracked_map_points = []\r\n        for match in matches:\r\n            map_point_idx = match.queryIdx\r\n            if map_point_idx < len(self.map_points):\r\n                tracked_points.append({\r\n                    \'map_point_idx\': map_point_idx,\r\n                    \'keypoint_idx\': match.trainIdx,\r\n                    \'pixel_coord\': keypoints[match.trainIdx].pt\r\n                })\r\n                tracked_map_points.append(map_point_idx)\r\n\r\n        # Find unmatched keypoints\r\n        matched_indices = {m.trainIdx for m in matches}\r\n        for i, (kp, desc) in enumerate(zip(keypoints, descriptors)):\r\n            if i not in matched_indices:\r\n                new_keypoints.append(kp)\r\n                new_descriptors.append(desc)\r\n\r\n        # Estimate motion (PnP)\r\n        if len(tracked_points) >= 4:\r\n            pose_estimated = self.estimate_motion_pnp(tracked_points, image.shape)\r\n            if pose_estimated is not None:\r\n                self.current_pose = pose_estimated\r\n\r\n                # Triangulate new points\r\n                new_map_points = self.triangulate_new_points(\r\n                    new_keypoints, new_descriptors, image.shape\r\n                )\r\n                self.map_points.extend(new_map_points)\r\n\r\n                # Check for keyframe creation\r\n                if self.should_create_keyframe(image, len(tracked_points)):\r\n                    self.create_keyframe(image, keypoints, descriptors, timestamp)\r\n\r\n                return True\r\n\r\n        return False\r\n\r\n    def estimate_motion_pnp(self, correspondences, image_shape):\r\n        """Estimate camera pose using PnP"""\r\n        if len(correspondences) < 4:\r\n            return None\r\n\r\n        # Prepare data for PnP\r\n        object_points = []\r\n        image_points = []\r\n\r\n        for corr in correspondences:\r\n            map_point = self.map_points[corr[\'map_point_idx\']]\r\n            object_points.append(map_point.position)\r\n            image_points.append(corr[\'pixel_coord\'])\r\n\r\n        object_points = np.array(object_points, dtype=np.float32)\r\n        image_points = np.array(image_points, dtype=np.float32)\r\n\r\n        # Solve PnP\r\n        success, rvec, tvec, inliers = cv2.solvePnPRansac(\r\n            object_points,\r\n            image_points,\r\n            self.camera.intrinsic_matrix,\r\n            None,\r\n            iterationsCount=1000,\r\n            reprojectionError=2.0,\r\n            confidence=0.99\r\n        )\r\n\r\n        if success:\r\n            # Convert rotation vector to matrix\r\n            R, _ = cv2.Rodrigues(rvec)\r\n\r\n            # Create pose matrix\r\n            pose = np.eye(4)\r\n            pose[:3, :3] = R\r\n            pose[:3, 3] = tvec.flatten()\r\n\r\n            return pose\r\n\r\n        return None\r\n\r\n    def detect_loop_closure(self, image, keypoints, descriptors):\r\n        """Detect loop closure using visual place recognition"""\r\n        # Extract current bag-of-words descriptor\r\n        current_bow = self._extract_bag_of_words(descriptors)\r\n\r\n        # Compare with previous keyframes\r\n        best_match = None\r\n        best_similarity = 0\r\n\r\n        for i, keyframe in enumerate(self.keyframes[:-10]):  # Don\'t check recent frames\r\n            similarity = self._compare_bow_descriptors(current_bow, keyframe.bow_descriptor)\r\n\r\n            if similarity > best_similarity and similarity > self.loop_closure_threshold:\r\n                best_similarity = similarity\r\n                best_match = i\r\n\r\n        if best_match is not None:\r\n            return {\r\n                \'current_frame_idx\': len(self.keyframes),\r\n                \'loop_frame_idx\': best_match,\r\n                \'similarity\': best_similarity,\r\n                \'current_keypoints\': keypoints,\r\n                \'current_descriptors\': descriptors\r\n            }\r\n\r\n        return None\r\n\r\n    def perform_loop_closure(self, loop_closure):\r\n        """Perform loop closure correction"""\r\n        # Match features between current and loop frame\r\n        loop_frame = self.keyframes[loop_closure[\'loop_frame_idx\']]\r\n        current_keypoints = loop_closure[\'current_keypoints\']\r\n        current_descriptors = loop_closure[\'current_descriptors\']\r\n\r\n        matches = self.feature_matcher.match(loop_frame.descriptors, current_descriptors)\r\n\r\n        if len(matches) < 20:  # Minimum matches for loop closure\r\n            return\r\n\r\n        # Estimate relative pose\r\n        relative_pose = self._estimate_relative_pose(loop_frame, current_keypoints, matches)\r\n\r\n        if relative_pose is not None:\r\n            # Correct poses using pose graph optimization\r\n            self.correct_poses_with_loop_closure(\r\n                loop_closure[\'loop_frame_idx\'],\r\n                len(self.keyframes),\r\n                relative_pose\r\n            )\r\n\r\n    def _extract_bag_of_words(self, descriptors):\r\n        """Extract bag-of-words descriptor for place recognition"""\r\n        # In practice, use VLAD or Fisher Vector\r\n        # Here, use simple histogram of visual words\r\n        visual_words = self._quantize_to_visual_words(descriptors)\r\n        histogram, _ = np.histogram(visual_words, bins=1000)\r\n        return histogram / np.linalg.norm(histogram)\r\n\r\n    def _quantize_to_visual_words(self, descriptors):\r\n        """Quantize descriptors to visual words"""\r\n        # In practice, use pre-trained visual vocabulary\r\n        # Here, use simple quantization\r\n        visual_words = []\r\n        for desc in descriptors:\r\n            word = int(np.sum(desc) * 100) % 1000\r\n            visual_words.append(word)\r\n        return visual_words\n'})}),"\n",(0,a.jsx)(n.h3,{id:"1532-dense-slam-with-rgb-d",children:"15.3.2 Dense SLAM with RGB-D"}),"\n",(0,a.jsx)(n.p,{children:"Dense SLAM using depth cameras provides complete 3D reconstruction:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class DenseSLAM:\r\n    def __init__(self, camera_parameters):\r\n        self.color_camera = PinholeCamera(camera_parameters[\'color\'])\r\n        self.depth_camera = PinholeCamera(camera_parameters[\'depth\'])\r\n\r\n        # Keyframe management\r\n        self.keyframes = []\r\n        self.max_keyframes = 100\r\n\r\n        # Map representation\r\n        self.volume = TSDFVolume(resolution=0.01, size=10.0)\r\n        self.raycasting = RayCasting(self.depth_camera)\r\n\r\n        # Pose tracking\r\n        self.current_pose = np.eye(4)\r\n        self.pose_graph = PoseGraph()\r\n\r\n        # Dense tracking\r\n        self.feature_tracker = DenseFeatureTracker()\r\n        self.direct_method = DirectMethod()\r\n\r\n    def process_rgbd_frame(self, color_image, depth_image, timestamp):\r\n        """Process RGB-D frame"""\r\n        # Track frame using direct method\r\n        pose_delta, tracking_quality = self.direct_method.track_frame(\r\n            color_image, depth_image, self.current_pose\r\n        )\r\n\r\n        if tracking_quality < 0.3:\r\n            # Lost tracking - try global registration\r\n            pose_estimate = self.global_registration(color_image, depth_image)\r\n            if pose_estimate is not None:\r\n                self.current_pose = pose_estimate\r\n                tracking_quality = 0.8\r\n\r\n        else:\r\n            # Update pose\r\n            self.current_pose = pose_delta @ self.current_pose\r\n\r\n        # Update volume if keyframe\r\n        if self.should_create_keyframe(tracking_quality, timestamp):\r\n            self.create_dense_keyframe(color_image, depth_image, timestamp)\r\n\r\n        # Raycast for depth prediction\r\n        predicted_depth = self.raycasting.render_depth(\r\n            color_image.shape[:2], self.current_pose\r\n        )\r\n\r\n        return self.current_pose, predicted_depth\r\n\r\n    def create_dense_keyframe(self, color_image, depth_image, timestamp):\r\n        """Create dense keyframe and update volume"""\r\n        keyframe = DenseKeyFrame(\r\n            color_image=color_image.copy(),\r\n            depth_image=depth_image.copy(),\r\n            pose=self.current_pose.copy(),\r\n            timestamp=timestamp,\r\n            pyramid=self._create_image_pyramid(color_image)\r\n        )\r\n\r\n        # Add to keyframe list\r\n        self.keyframes.append(keyframe)\r\n\r\n        # Update TSDF volume\r\n        self.volume.integrate_frame(\r\n            color_image, depth_image,\r\n            self.current_pose,\r\n            self.color_camera.intrinsic_matrix\r\n        )\r\n\r\n        # Add to pose graph\r\n        if len(self.keyframes) > 1:\r\n            last_keyframe = self.keyframes[-2]\r\n            relative_pose = np.linalg.inv(last_keyframe.pose) @ self.current_pose\r\n            self.pose_graph.add_edge(\r\n                len(self.keyframes) - 2,\r\n                len(self.keyframes) - 1,\r\n                relative_pose\r\n            )\r\n\r\n        # Limit number of keyframes\r\n        if len(self.keyframes) > self.max_keyframes:\r\n            self.remove_oldest_keyframe()\r\n\r\n    def global_registration(self, color_image, depth_image):\r\n        """Global registration when tracking is lost"""\r\n        best_match = None\r\n        best_inliers = 0\r\n\r\n        for i, keyframe in enumerate(self.keyframes):\r\n            # Perform feature matching\r\n            matches = self.feature_tracker.match_frames(\r\n                keyframe.color_image, color_image\r\n            )\r\n\r\n            if len(matches) > 30:\r\n                # Estimate pose using PnP with depth\r\n                pose, inliers = self._estimate_pose_with_depth(\r\n                    matches, keyframe, depth_image\r\n                )\r\n\r\n                if inliers > best_inliers:\r\n                    best_inliers = inliers\r\n                    best_match = (i, pose)\r\n\r\n        if best_match:\r\n            keyframe_idx, pose = best_match\r\n\r\n            # Add loop closure edge to pose graph\r\n            self.pose_graph.add_edge(\r\n                keyframe_idx,\r\n                len(self.keyframes),\r\n                np.linalg.inv(self.keyframes[keyframe_idx].pose) @ pose,\r\n                loop_closure=True\r\n            )\r\n\r\n            # Optimize pose graph\r\n            optimized_poses = self.pose_graph.optimize()\r\n\r\n            if len(optimized_poses) > len(self.keyframes):\r\n                self.current_pose = optimized_poses[-1]\r\n\r\n            return pose\r\n\r\n        return None\r\n\r\n    def extract_mesh(self):\r\n        """Extract triangle mesh from TSDF volume"""\r\n        vertices, faces = self.volume.extract_mesh()\r\n\r\n        return {\r\n            \'vertices\': vertices,\r\n            \'faces\': faces,\r\n            \'colors\': self._extract_vertex_colors(vertices)\r\n        }\r\n\r\nclass TSDFVolume:\r\n    def __init__(self, resolution, size):\r\n        self.resolution = resolution\r\n        self.size = size\r\n        self.voxels_per_dim = int(size / resolution)\r\n\r\n        # TSDF volume\r\n        self.tsdf = np.ones((self.voxels_per_dim, self.voxels_per_dim, self.voxels_per_dim))\r\n        self.weight = np.zeros((self.voxels_per_dim, self.voxels_per_dim, self.voxels_per_dim))\r\n\r\n        # Color volume\r\n        self.color = np.zeros((self.voxels_per_dim, self.voxels_per_dim, self.voxels_per_dim, 3))\r\n\r\n        # Truncation distance\r\n        self.truncation = 4 * resolution\r\n\r\n    def integrate_frame(self, color_image, depth_image, pose, intrinsic_matrix):\r\n        """Integrate new RGB-D frame into TSDF volume"""\r\n        height, width = depth_image.shape\r\n\r\n        # Create coordinate grids\r\n        u, v = np.meshgrid(np.arange(width), np.arange(height))\r\n\r\n        # Backproject depth to 3D points\r\n        fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]\r\n        cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]\r\n\r\n        # Convert depth to meters if needed\r\n        depth_meters = depth_image / 1000.0\r\n\r\n        # Backproject to camera coordinates\r\n        x_cam = (u - cx) * depth_meters / fx\r\n        y_cam = (v - cy) * depth_meters / fy\r\n        z_cam = depth_meters\r\n\r\n        # Transform to world coordinates\r\n        points_cam = np.stack([x_cam.flatten(), y_cam.flatten(), z_cam.flatten()], axis=1)\r\n        ones = np.ones((points_cam.shape[0], 1))\r\n        points_cam_homo = np.hstack([points_cam, ones])\r\n\r\n        points_world = (pose @ points_cam_homo.T).T\r\n        points_world = points_world[:, :3]\r\n\r\n        # Filter valid points\r\n        valid = (depth_meters.flatten() > 0.1) & (depth_meters.flatten() < 10.0)\r\n        points_world = points_world[valid]\r\n        colors = color_image.reshape(-1, 3)[valid]\r\n\r\n        # Update TSDF for each point\r\n        for i, (point, color) in enumerate(zip(points_world, colors)):\r\n            voxel = self._world_to_voxel(point)\r\n            if self._is_valid_voxel(voxel):\r\n                # Calculate signed distance\r\n                distance = np.linalg.norm(point - (pose @ np.array([0, 0, 0, 1]).T)[:3])\r\n\r\n                # Get current TSDF value\r\n                current_tsdf = self.tsdf[voxel[0], voxel[1], voxel[2]]\r\n                current_weight = self.weight[voxel[0], voxel[1], voxel[2]]\r\n\r\n                # Running average update\r\n                new_tsdf = min(distance, self.truncation)\r\n                new_weight = 1.0\r\n\r\n                if current_weight + new_weight > 0:\r\n                    updated_tsdf = (current_tsdf * current_weight + new_tsdf * new_weight) / (current_weight + new_weight)\r\n                    self.tsdf[voxel[0], voxel[1], voxel[2]] = updated_tsdf\r\n                    self.weight[voxel[0], voxel[1], voxel[2]] = current_weight + new_weight\r\n\r\n                    # Update color\r\n                    self.color[voxel[0], voxel[1], voxel[2]] = color\r\n\r\n    def extract_mesh(self):\r\n        """Extract mesh using marching cubes"""\r\n        from skimage.measure import marching_cubes\r\n\r\n        # Find zero-crossing surface\r\n        vertices, faces, normals, values = marching_cubes(\r\n            self.tsdf, level=0\r\n        )\r\n\r\n        # Scale voxel coordinates to world coordinates\r\n        scale = self.size / self.voxels_per_dim\r\n        vertices = vertices * scale - self.size / 2\r\n\r\n        # Filter small triangles\r\n        face_areas = self._calculate_face_areas(vertices, faces)\r\n        valid_faces = face_areas > (scale ** 2) * 0.01  # Minimum triangle area\r\n\r\n        vertices = vertices[valid_faces]\r\n        faces = faces[valid_faces]\r\n\r\n        return vertices, faces\n'})}),"\n",(0,a.jsx)(n.h2,{id:"154-navigation-with-slam",children:"15.4 Navigation with SLAM"}),"\n",(0,a.jsx)(n.h3,{id:"1541-path-planning-in-slam-maps",children:"15.4.1 Path Planning in SLAM Maps"}),"\n",(0,a.jsx)(n.p,{children:"Plan paths through SLAM-constructed environments:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class SLAMNavigation:\r\n    def __init__(self, slam_system):\r\n        self.slam = slam_system\r\n        self.occupancy_grid = OccupancyGrid(resolution=0.05, size=100.0)\r\n\r\n        # Path planning\r\n        self.path_planner = AStarPlanner(self.occupancy_grid)\r\n        self.trajectory_planner = TrajectoryPlanner()\r\n\r\n        # Navigation state\r\n        self.current_goal = None\r\n        self.current_path = None\r\n        self.navigation_state = \'IDLE\'\r\n\r\n    def update_navigation_map(self):\r\n        """Update occupancy grid from SLAM map"""\r\n        # Convert SLAM landmarks to occupancy grid\r\n        self.occupancy_grid.clear()\r\n\r\n        # Add landmarks as obstacles\r\n        for landmark in self.slam.landmarks:\r\n            x, y = landmark[\'position\']\r\n            self.occupancy_grid.set_occupancy(x, y, 1.0)\r\n\r\n        # Add trajectory as free space\r\n        for pose in self.slam.trajectory:\r\n            x, y = pose[:2]\r\n            self.occupancy_grid.set_occupancy(x, y, 0.0)\r\n\r\n        # Inflate obstacles for robot footprint\r\n        self.occupancy_grid.inflate_obstacles(robot_radius=0.3)\r\n\r\n    def navigate_to_goal(self, goal_position):\r\n        """Navigate to specified goal position"""\r\n        self.current_goal = goal_position\r\n        self.navigation_state = \'PLANNING\'\r\n\r\n        # Update navigation map\r\n        self.update_navigation_map()\r\n\r\n        # Get current position\r\n        current_position = self.slam.robot_pose[:2]\r\n\r\n        # Plan path\r\n        path = self.path_planner.plan_path(current_position, goal_position)\r\n\r\n        if path is not None:\r\n            self.current_path = path\r\n            self.navigation_state = \'EXECUTING\'\r\n            return True\r\n        else:\r\n            self.navigation_state = \'NO_PATH\'\r\n            return False\r\n\r\n    def execute_path(self):\r\n        """Execute planned path"""\r\n        if self.navigation_state != \'EXECUTING\' or self.current_path is None:\r\n            return None\r\n\r\n        # Get current position\r\n        current_position = self.slam.robot_pose[:2]\r\n\r\n        # Find target point on path\r\n        target_point = self._get_target_point(current_position)\r\n\r\n        if target_point is None:\r\n            self.navigation_state = \'REACHED_GOAL\'\r\n            return None\r\n\r\n        # Generate trajectory to target point\r\n        trajectory = self.trajectory_planner.generate_trajectory(\r\n            current_position,\r\n            self.slam.robot_pose[2],  # Current heading\r\n            target_point\r\n        )\r\n\r\n        # Calculate control command\r\n        control = self._trajectory_to_control(trajectory)\r\n\r\n        return control\r\n\r\n    def _get_target_point(self, current_position):\r\n        """Get target point along path"""\r\n        if self.current_path is None or len(self.current_path) == 0:\r\n            return None\r\n\r\n        # Find closest point on path\r\n        min_dist = float(\'inf\')\r\n        closest_idx = 0\r\n\r\n        for i, point in enumerate(self.current_path):\r\n            dist = np.linalg.norm(current_position - point)\r\n            if dist < min_dist:\r\n                min_dist = dist\r\n                closest_idx = i\r\n\r\n        # Look ahead on path\r\n        look_ahead_distance = 2.0\r\n        target_point = None\r\n\r\n        for j in range(closest_idx, min(closest_idx + 20, len(self.current_path))):\r\n            point = self.current_path[j]\r\n            dist = np.linalg.norm(current_position - point)\r\n\r\n            if dist >= look_ahead_distance:\r\n                target_point = point\r\n                break\r\n\r\n        # If no point found ahead, use last point\r\n        if target_point is None:\r\n            target_point = self.current_path[-1]\r\n\r\n        return target_point\r\n\r\nclass AStarPlanner:\r\n    def __init__(self, occupancy_grid):\r\n        self.grid = occupancy_grid\r\n        self.heuristic_weight = 1.0\r\n\r\n    def plan_path(self, start, goal):\r\n        """Plan path using A* algorithm"""\r\n        start_cell = self.grid.world_to_grid(start)\r\n        goal_cell = self.grid.world_to_grid(goal)\r\n\r\n        if not self.grid.is_valid_cell(start_cell) or not self.grid.is_valid_cell(goal_cell):\r\n            return None\r\n\r\n        # A* algorithm\r\n        open_set = PriorityQueue()\r\n        closed_set = set()\r\n\r\n        # Start node\r\n        start_node = Node(start_cell, None, 0, self._heuristic(start_cell, goal_cell))\r\n        open_set.put(start_node)\r\n\r\n        while not open_set.empty():\r\n            current = open_set.get()\r\n\r\n            if current.cell == goal_cell:\r\n                # Reconstruct path\r\n                path = []\r\n                node = current\r\n                while node is not None:\r\n                    path.append(self.grid.grid_to_world(node.cell))\r\n                    node = node.parent\r\n                return path[::-1]\r\n\r\n            closed_set.add(current.cell)\r\n\r\n            # Explore neighbors\r\n            for neighbor in self._get_neighbors(current.cell):\r\n                if not self.grid.is_valid_cell(neighbor) or neighbor in closed_set:\r\n                    continue\r\n\r\n                g_cost = current.g_cost + self._distance(current.cell, neighbor)\r\n                h_cost = self._heuristic(neighbor, goal_cell)\r\n                f_cost = g_cost + self.heuristic_weight * h_cost\r\n\r\n                neighbor_node = Node(neighbor, current, g_cost, h_cost)\r\n                open_set.put(neighbor_node)\r\n\r\n        return None  # No path found\r\n\r\n    def _heuristic(self, cell1, cell2):\r\n        """A* heuristic (Euclidean distance)"""\r\n        return self._distance(cell1, cell2)\r\n\r\n    def _distance(self, cell1, cell2):\r\n        """Distance between grid cells"""\r\n        return np.linalg.norm(np.array(cell1) - np.array(cell2))\r\n\r\n    def _get_neighbors(self, cell):\r\n        """Get valid neighbor cells"""\r\n        neighbors = []\r\n        directions = [(0, 1), (1, 0), (0, -1), (-1, 0),  # 4-connected\r\n                       (1, 1), (1, -1), (-1, 1), (-1, -1)]  # 8-connected\r\n\r\n        for dx, dy in directions:\r\n            neighbor = (cell[0] + dx, cell[1] + dy)\r\n            if self.grid.is_valid_cell(neighbor):\r\n                neighbors.append(neighbor)\r\n\r\n        return neighbors\r\n\r\nclass Node:\r\n    def __init__(self, cell, parent, g_cost, h_cost):\r\n        self.cell = cell\r\n        self.parent = parent\r\n        self.g_cost = g_cost  # Cost from start\r\n        self.h_cost = h_cost  # Heuristic cost to goal\r\n        self.f_cost = g_cost + h_cost  # Total cost\r\n\r\n    def __lt__(self, other):\r\n        return self.f_cost < other.f_cost\r\n\r\nclass TrajectoryPlanner:\r\n    def __init__(self):\r\n        self.max_velocity = 1.0\r\n        self.max_angular_velocity = 1.0\r\n        self.dt = 0.1\r\n\r\n    def generate_trajectory(self, current_pos, current_heading, target_pos):\r\n        """Generate smooth trajectory to target"""\r\n        # Calculate relative position and angle\r\n        dx = target_pos[0] - current_pos[0]\r\n        dy = target_pos[1] - current_pos[1]\r\n        target_heading = np.arctan2(dy, dx)\r\n\r\n        # Calculate heading error\r\n        heading_error = self._normalize_angle(target_heading - current_heading)\r\n\r\n        # Simple P-controller for velocities\r\n        kp_linear = 1.0\r\n        kp_angular = 2.0\r\n\r\n        # Distance to target\r\n        distance = np.sqrt(dx**2 + dy**2)\r\n\r\n        # Control velocities\r\n        linear_velocity = kp_linear * distance\r\n        angular_velocity = kp_angular * heading_error\r\n\r\n        # Saturate velocities\r\n        linear_velocity = np.clip(linear_velocity, -self.max_velocity, self.max_velocity)\r\n        angular_velocity = np.clip(angular_velocity, -self.max_angular_velocity, self.max_angular_velocity)\r\n\r\n        # Stop if very close\r\n        if distance < 0.1:\r\n            linear_velocity = 0\r\n            angular_velocity = 0\r\n\r\n        return {\r\n            \'linear_velocity\': linear_velocity,\r\n            \'angular_velocity\': angular_velocity,\r\n            \'trajectory\': self._predict_trajectory(\r\n                current_pos, current_heading, linear_velocity, angular_velocity, self.dt\r\n            )\r\n        }\r\n\r\n    def _normalize_angle(self, angle):\r\n        """Normalize angle to [-pi, pi]"""\r\n        while angle > np.pi:\r\n            angle -= 2 * np.pi\r\n        while angle < -np.pi:\r\n            angle += 2 * np.pi\r\n        return angle\r\n\r\n    def _predict_trajectory(self, pos, heading, v, omega, dt, horizon=1.0):\r\n        """Predict future trajectory"""\r\n        trajectory = [pos.copy()]\r\n        current_pos = pos.copy()\r\n        current_heading = heading\r\n\r\n        steps = int(horizon / dt)\r\n        for _ in range(steps):\r\n            # Update position\r\n            current_pos[0] += v * np.cos(current_heading) * dt\r\n            current_pos[1] += v * np.sin(current_heading) * dt\r\n            current_heading += omega * dt\r\n            current_heading = self._normalize_angle(current_heading)\r\n\r\n            trajectory.append(current_pos.copy())\r\n\r\n        return trajectory\n'})}),"\n",(0,a.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered comprehensive SLAM and navigation systems for robotics:"}),"\n",(0,a.jsx)(n.h3,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SLAM Fundamentals"}),": Mathematical framework and uncertainty representation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"EKF-SLAM"}),": Extended Kalman Filter for landmark-based SLAM"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual SLAM"}),": Feature-based and dense RGB-D SLAM systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Loop Closure"}),": Place recognition and pose graph optimization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Navigation"}),": Path planning and execution in SLAM maps"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Applications"}),": Practical implementations for autonomous robots"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"practical-implementations",children:"Practical Implementations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Complete EKF-SLAM with data association and landmark management"}),"\n",(0,a.jsx)(n.li,{children:"Feature-based VSLAM with ORB features and loop closure detection"}),"\n",(0,a.jsx)(n.li,{children:"Dense SLAM with TSDF volume integration and mesh extraction"}),"\n",(0,a.jsx)(n.li,{children:"A* path planning with SLAM map integration"}),"\n",(0,a.jsx)(n.li,{children:"Trajectory planning with smooth control laws"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"With SLAM and navigation expertise, you're ready for:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Chapter 16: Path Planning Algorithms"}),"\n",(0,a.jsx)(n.li,{children:"Part V: Embodied Intelligence & VLA (Chapters 17-20)"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"glossary-terms",children:"Glossary Terms"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Term"}),": ",(0,a.jsx)(n.strong,{children:"Data Association"}),"\r\n",(0,a.jsx)(n.strong,{children:"Definition"}),": Process of matching sensor measurements to map landmarks, a critical challenge in SLAM\r\n",(0,a.jsx)(n.strong,{children:"Related"}),": ",(0,a.jsx)(n.strong,{children:"Nearest Neighbor"}),", ",(0,a.jsx)(n.strong,{children:"Joint Compatibility Test"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Term"}),": ",(0,a.jsx)(n.strong,{children:"Loop Closure"}),"\r\n",(0,a.jsx)(n.strong,{children:"Definition:"})," Detection when robot returns to previously visited location, enabling map correction and global consistency\r\n",(0,a.jsx)(n.strong,{children:"Related:"})," ",(0,a.jsx)(n.strong,{children:"Place Recognition"}),", ",(0,a.jsx)(n.strong,{children:"Pose Graph Optimization"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Term"}),": ",(0,a.jsx)(n.strong,{children:"Pose Graph"}),"\r\n",(0,a.jsx)(n.strong,{children:"Definition:"})," Graph representation of robot poses and constraints between them, optimized for global consistency\r\n",(0,a.jsx)(n.strong,{children:"Related:"})," ",(0,a.jsx)(n.strong,{children:"Bundle Adjustment"}),", ",(0,a.jsx)(n.strong,{children:"Graph SLAM"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Term"}),": ",(0,a.jsx)(n.strong,{children:"TSDF (Truncated Signed Distance Function)"}),"\r\n",(0,a.jsx)(n.strong,{children:"Definition:"})," Implicit surface representation storing distance to nearest surface with truncation for bandlimited updates\r\n",(0,a.jsx)(n.strong,{children:"Related:"})," ",(0,a.jsx)(n.strong,{children:"Volumetric Mapping"}),", ",(0,a.jsx)(n.strong,{children:"Marching Cubes"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Term"}),": ",(0,a.jsx)(n.strong,{children:"Visual Odometry"}),"\r\n",(0,a.jsx)(n.strong,{children:"Definition:"})," Estimation of camera motion by tracking visual features between consecutive frames\r\n",(0,a.jsx)(n.strong,{children:"Related:"})," ",(0,a.jsx)(n.strong,{children:"Feature Tracking"}),", ",(0,a.jsx)(n.strong,{children:"Essential Matrix"})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-151-ekf-slam-implementation",children:"Exercise 15.1: EKF-SLAM Implementation"}),"\n",(0,a.jsx)(n.p,{children:"Implement complete EKF-SLAM system:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Add landmark initialization and management"}),"\n",(0,a.jsx)(n.li,{children:"Implement robust data association with gating"}),"\n",(0,a.jsx)(n.li,{children:"Handle map management and pruning"}),"\n",(0,a.jsx)(n.li,{children:"Visualize uncertainty ellipses"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-152-feature-based-vslam",children:"Exercise 15.2: Feature-Based VSLAM"}),"\n",(0,a.jsx)(n.p,{children:"Build visual SLAM with loop closure:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement ORB feature extraction and matching"}),"\n",(0,a.jsx)(n.li,{children:"Create keyframe management system"}),"\n",(0,a.jsx)(n.li,{children:"Add loop closure detection with place recognition"}),"\n",(0,a.jsx)(n.li,{children:"Optimize pose graph for global consistency"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-153-dense-rgb-d-slam",children:"Exercise 15.3: Dense RGB-D SLAM"}),"\n",(0,a.jsx)(n.p,{children:"Develop dense SLAM with TSDF volume:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement direct method tracking"}),"\n",(0,a.jsx)(n.li,{children:"Create TSDF volume integration"}),"\n",(0,a.jsx)(n.li,{children:"Extract and visualize mesh"}),"\n",(0,a.jsx)(n.li,{children:"Handle missing depth data"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-154-slam-navigation",children:"Exercise 15.4: SLAM Navigation"}),"\n",(0,a.jsx)(n.p,{children:"Create navigation system using SLAM map:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Build occupancy grid from SLAM landmarks"}),"\n",(0,a.jsx)(n.li,{children:"Implement A* path planning algorithm"}),"\n",(0,a.jsx)(n.li,{children:"Add trajectory planning with smooth control"}),"\n",(0,a.jsx)(n.li,{children:"Handle dynamic obstacles"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-155-multi-robot-slam",children:"Exercise 15.5: Multi-Robot SLAM"}),"\n",(0,a.jsx)(n.p,{children:"Design collaborative SLAM system:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement multi-robot pose graph optimization"}),"\n",(0,a.jsx)(n.li,{children:"Create inter-robot loop closure detection"}),"\n",(0,a.jsx)(n.li,{children:"Design distributed map merging"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate system scalability"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}}}]);