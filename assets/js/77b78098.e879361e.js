"use strict";(globalThis.webpackChunkai_native_textbook_docusaurus=globalThis.webpackChunkai_native_textbook_docusaurus||[]).push([[4791],{1105:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"part-3-simulation/chapter-9-nvidia-isaac-synthetic-data","title":"NVIDIA Isaac Sim & Synthetic Data","description":"Introduction","source":"@site/docs/part-3-simulation/chapter-9-nvidia-isaac-synthetic-data.mdx","sourceDirName":"part-3-simulation","slug":"/part-3-simulation/chapter-9-nvidia-isaac-synthetic-data","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-9-nvidia-isaac-synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/NaumanNavaid/ai-native-textbook-docusaurus/tree/main/docs/part-3-simulation/chapter-9-nvidia-isaac-synthetic-data.mdx","tags":[],"version":"current","frontMatter":{"title":"NVIDIA Isaac Sim & Synthetic Data","part":3,"chapter":9,"difficulty":"advanced","estimatedTime":65,"prerequisites":["chapter-7-gazebo-physics-simulation","chapter-8-unity-robotics-visualization"],"objectives":["Master NVIDIA Isaac Sim for advanced robotics simulation","Generate synthetic data for AI training","Understand photorealistic simulation environments","Create domain randomization and variation techniques"]},"sidebar":"chaptersSidebar","previous":{"title":"Unity for Robotics Visualization","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-8-unity-robotics-visualization"},"next":{"title":"Physics Simulations for Robotics","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-10-physics-simulations"}}');var t=r(4848),a=r(8453);const o={title:"NVIDIA Isaac Sim & Synthetic Data",part:3,chapter:9,difficulty:"advanced",estimatedTime:65,prerequisites:["chapter-7-gazebo-physics-simulation","chapter-8-unity-robotics-visualization"],objectives:["Master NVIDIA Isaac Sim for advanced robotics simulation","Generate synthetic data for AI training","Understand photorealistic simulation environments","Create domain randomization and variation techniques"]},s="Chapter 9: NVIDIA Isaac Synthetic Data",l={},m=[{value:"Introduction",id:"introduction",level:2},{value:"9.1 Isaac Sim Architecture",id:"91-isaac-sim-architecture",level:2},{value:"9.1.1 Omniverse Platform Foundation",id:"911-omniverse-platform-foundation",level:3},{value:"9.1.2 Key Components",id:"912-key-components",level:3},{value:"9.2 Synthetic Data Generation",id:"92-synthetic-data-generation",level:2},{value:"9.2.1 Domain Randomization",id:"921-domain-randomization",level:3},{value:"9.2.2 Ground Truth Generation",id:"922-ground-truth-generation",level:3},{value:"9.3 AI Training Integration",id:"93-ai-training-integration",level:2},{value:"9.3.1 Synthetic Data Pipeline",id:"931-synthetic-data-pipeline",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 9.1: Isaac Sim Setup",id:"exercise-91-isaac-sim-setup",level:3},{value:"Exercise 9.2: Domain Randomization",id:"exercise-92-domain-randomization",level:3},{value:"Exercise 9.3: Ground Truth Generation",id:"exercise-93-ground-truth-generation",level:3},{value:"Exercise 9.4: AI Training Pipeline",id:"exercise-94-ai-training-pipeline",level:3},{value:"Exercise 9.5: Advanced Isaac Sim Features",id:"exercise-95-advanced-isaac-sim-features",level:3},{value:"Glossary Terms",id:"glossary-terms",level:2}];function d(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-9-nvidia-isaac-synthetic-data",children:"Chapter 9: NVIDIA Isaac Synthetic Data"})}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA Isaac Sim represents the cutting edge of robotics simulation platforms, combining photorealistic rendering, advanced physics simulation, and AI-driven synthetic data generation. Built on NVIDIA's Omniverse platform, Isaac Sim provides a comprehensive environment for developing, testing, and training autonomous robots. This chapter explores Isaac Sim's capabilities, synthetic data generation techniques, and integration with modern AI training pipelines."}),"\n",(0,t.jsx)(e.admonition,{type:"info",children:(0,t.jsx)(e.p,{children:"Isaac Sim bridges the gap between simulation and reality by creating photorealistic environments that can generate unlimited training data for AI models, dramatically accelerating robotic learning and development."})}),"\n",(0,t.jsx)(e.h2,{id:"91-isaac-sim-architecture",children:"9.1 Isaac Sim Architecture"}),"\n",(0,t.jsx)(e.h3,{id:"911-omniverse-platform-foundation",children:"9.1.1 Omniverse Platform Foundation"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim is built on NVIDIA's Omniverse, a collaborative 3D simulation platform:"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Diagram: Isaac Sim Architecture"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Omniverse Platform\r\n\u251c\u2500\u2500 Nucleus (Core Services)\r\n\u2502   \u251c\u2500\u2500 Scene Management\r\n\u2502   \u251c\u2500\u2500 Asset Loading\r\n\u2502   \u251c\u2500\u2500 Collaboration\r\n\u2502   \u2514\u2500\u2500 Version Control\r\n\u251c\u2500\u2500 USD (Universal Scene Description)\r\n\u2502   \u251c\u2500\u2500 Scene Graph\r\n\u2502   \u251c\u2500\u2500 Material System\r\n\u2502   \u251c\u2500\u2500 Animation System\r\n\u2502   \u2514\u2500\u2500 Physics Integration\r\n\u251c\u2500\u2500 Simulation Engines\r\n\u2502   \u251c\u2500\u2500 NVIDIA PhysX 5\r\n\u2502   \u251c\u2500\u2500 Ray Tracing\r\n\u2502   \u251c\u2500\u2500 Fluid Dynamics\r\n\u2502   \u2514\u2500\u2500 Cloth Simulation\r\n\u251c\u2500\u2500 Isaac Sim SDK\r\n\u2502   \u251c\u2500\u2500 Python API\r\n\u2502   \u251c\u2500\u2500 C++ API\r\n\u2502   \u251c\u2500\u2500 ROS 2 Integration\r\n\u2502   \u2514\u2500\u2500 Extension System\r\n\u2514\u2500\u2500 AI Integration\r\n    \u251c\u2500\u2500 Synthesis Network\r\n    \u251c\u2500\u2500 Domain Randomization\r\n    \u251c\u2500\u2500 Data Generation\r\n    \u2514\u2500\u2500 Cloud Services\n"})}),"\n",(0,t.jsx)(e.h3,{id:"912-key-components",children:"9.1.2 Key Components"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Simulation Core"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"PhysX 5"}),": Advanced physics simulation with real-time performance"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Ray Tracing"}),": Realistic lighting, shadows, and reflections"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Material System"}),": Physically accurate material rendering"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Animation"}),": Complex character and object animation"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"AI Integration"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Synthetia"}),": NVIDIA's synthetic data generation system"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Domain Randomization"}),": Automatic scene and parameter variation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Ground Truth Generation"}),": Automated labeling and annotation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cloud Services"}),": Scalable cloud-based simulation"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example: Isaac Sim Python API Setup"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-pythonimport",metastring:"numpy as np",children:'import matplotlib.pyplot as plt\r\nfrom isaacsim import SimulationApp\r\nfrom omni.isaac.core import World, WorldSettings\r\nfrom omni.isaac.core.robots import Robot\r\nfrom omni.isaac.core.utils.nucleus import get_current_stage\r\n\r\nclass IsaacSimEnvironment:\r\n    def __init__(self):\r\n        # Initialize Isaac Sim application\r\n        self.sim_app = SimulationApp({\r\n            "headless": False,\r\n            "width": 1920,\r\n            "height": 1080\r\n        })\r\n\r\n        # Create world with specific settings\r\n        world_settings = WorldSettings(\r\n            physics_dt=1/120.0,\r\n            stage_units_in_meters=1.0,\r\n            rendering_dt=1/60.0\r\n        )\r\n\r\n        self.world = World(world_settings)\r\n        self.scene = get_current_stage()\r\n\r\n        # Initialize components\r\n        self.robots = []\r\n        self.sensors = []\r\n        self.cameras = []\r\n\r\n    def setup_scene(self):\r\n        """Setup the simulation scene"""\r\n        # Clear existing scene\r\n        self.scene.Clear()\r\n\r\n        # Add lighting\r\n        self.setup_lighting()\r\n\r\n        # Add ground plane\r\n        self.add_ground_plane()\r\n\r\n        # Add environment elements\r\n        self.add_environment()\r\n\r\n    def setup_lighting(self):\r\n        """Configure lighting for photorealistic rendering"""\r\n        # Add dome light for global illumination\r\n        from omni.isaac.core import Lighting\r\n\r\n        dome_light = self.scene.GetLighting()\r\n        dome_light.SetDomeLightIntensity(1.0)\r\n        dome_light.SetTint(1.0, 1.0, 1.0)\r\n\r\n        # Add directional lights\r\n        self.add_directional_lights()\r\n\r\n    def add_directional_lights(self):\r\n        """Add directional lights for realistic lighting"""\r\n        from omni.isaac.core import Light\r\n\r\n        # Main sun light\r\n        sun_light = Light(\r\n            prim_path="/World/light_sun",\r\n            light_type="distant",\r\n            intensity=2000.0,\r\n            color=(1.0, 0.95, 0.8)\r\n        )\r\n        sun_light.SetCameraPose(0.57735, -0.57735, 0.57735, 0, 0, 0, 1)\r\n\r\n        # Ambient fill lights\r\n        fill_light = Light(\r\n            prim_path="/World/light_fill",\r\n            light_type="distant",\r\n            intensity=500.0,\r\n            color=(0.8, 0.9, 1.0)\r\n        )\r\n        fill_light.SetCameraPose(-0.57735, 0.57735, -0.57735, 0, 0, 0, 1)\r\n\r\n    def add_ground_plane(self):\r\n        """Add photorealistic ground plane"""\r\n        from omni.isaac.core.utils.nucleus import add_ground_plane\r\n\r\n        # Add ground plane with high-quality material\r\n        add_ground_plane(\r\n            prim_path="/World/ground_plane",\r\n            size=100.0,\r\n            material="concrete_material"\r\n        )\r\n\r\n    def add_environment(self):\r\n        """Add environment elements"""\r\n        self.add_buildings()\r\n        self.add_vegetation()\r\n        self.add_obstacles()\r\n\r\n    def add_buildings(self):\r\n        """Add realistic building models"""\r\n        # Load building USD files\r\n        building_paths = [\r\n            "/path/to/office_building.usd",\r\n            "/path/to/warehouse.usd",\r\n            "/path/to/apartment_complex.usd"\r\n        ]\r\n\r\n        for i, building_path in enumerate(building_paths):\r\n            building = self.scene.ImportUSD(\r\n                building_path,\r\n                f"/World/building_{i}"\r\n            )\r\n            self.apply_photorealistic_materials(building)\r\n\r\n    def apply_photorealistic_materials(self, prim):\r\n        """Apply high-quality materials to primitives"""\r\n        from omni.isaac.core.materials import Material\r\n\r\n        # Create PBR material\r\n        material = Material(\r\n            prim_path="/Looks/photorealistic_material",\r\n            material_type="mdl"\r\n        )\r\n\r\n        # Set material properties\r\n        material.SetBaseColorRoughness(\r\n            base_color=(0.7, 0.7, 0.7, 1.0),\r\n            roughness=0.3,\r\n            metallic=0.1\r\n        )\r\n\r\n        # Apply to primitive\r\n        material.ApplyToPrim(prim)\r\n\r\n    def run_simulation(self):\r\n        """Run the simulation loop"""\r\n        self.sim_app.update()\r\n        return self.world\r\n\r\n    def cleanup(self):\r\n        """Clean up resources"""\r\n        self.world.clear()\r\n        self.sim_app.close()\r\n\r\n# Example usage\r\ndef main():\r\n    env = IsaacSimEnvironment()\r\n\r\n    try:\r\n        env.setup_scene()\r\n\r\n        # Main simulation loop\r\n        while True:\r\n            world = env.run_simulation()\r\n\r\n            # Process simulation data\r\n            pass\r\n\r\n    except KeyboardInterrupt:\r\n        print("Simulation stopped by user")\r\n    finally:\r\n        env.cleanup()\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"92-synthetic-data-generation",children:"9.2 Synthetic Data Generation"}),"\n",(0,t.jsx)(e.h3,{id:"921-domain-randomization",children:"9.2.1 Domain Randomization"}),"\n",(0,t.jsx)(e.p,{children:"Domain randomization is crucial for robust AI training:"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example: Domain Randomization System"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-pythonimport",metastring:"random",children:"import numpy as np\r\nfrom isaacsim import SimulationApp\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.nucleus import get_current_stage\r\n\r\nclass DomainRandomization:\r\n    def __init__(self, scene):\r\n        self.scene = scene\r\n        self.randomization_params = {\r\n            'lighting': {\r\n                'sun_intensity_range': (800, 3000),\r\n                'sun_color_variation': 0.2,\r\n                'ambient_intensity_range': (100, 500)\r\n            },\r\n            'weather': {\r\n                'cloud_density_range': (0.0, 0.8),\r\n                'fog_density_range': (0.0, 0.3),\r\n                'rain_intensity_range': (0.0, 1.0)\r\n            },\r\n            'materials': {\r\n                'roughness_range': (0.1, 0.9),\r\n                'metallic_range': (0.0, 0.8),\r\n                'base_color_variation': 0.3\r\n            },\r\n            'geometry': {\r\n                'scale_range': (0.8, 1.2),\r\n                'rotation_range': (-30, 30),\r\n                'position_jitter': 0.5\r\n            }\r\n        }\r\n\r\n    def randomize_all(self):\r\n        \"\"\"Apply complete domain randomization\"\"\"\r\n        self.randomize_lighting()\r\n        self.randomize_weather()\r\n        self.randomize_materials()\r\n        self.randomize_geometry()\r\n\r\n        print(f\"Domain randomization applied: {self.get_randomization_summary()}\")\r\n\r\n    def randomize_lighting(self):\r\n        \"\"\"Randomize lighting conditions\"\"\"\r\n        from omni.isaac.core import Light\r\n\r\n        # Randomize sun intensity\r\n        sun_light = self.scene.GetLighting().GetDistantLights()[0]\r\n        sun_intensity = random.uniform(*self.randomization_params['lighting']['sun_intensity_range'])\r\n        sun_light.SetIntensity(sun_intensity)\r\n\r\n        # Randomize sun color\r\n        color_variation = self.randomization_params['lighting']['sun_color_variation']\r\n        sun_color = (\r\n            1.0 + random.uniform(-color_variation, color_variation),\r\n            0.95 + random.uniform(-color_variation, color_variation),\r\n            0.8 + random.uniform(-color_variation, color_variation)\r\n        )\r\n        sun_light.SetColor(sun_color)\r\n\r\n        # Randomize ambient lighting\r\n        ambient_intensity = random.uniform(*self.randomization_params['lighting']['ambient_intensity_range'])\r\n        self.scene.GetLighting().SetAmbientLightIntensity(ambient_intensity)\r\n\r\n    def randomize_weather(self):\r\n        \"\"\"Randomize weather conditions\"\"\"\r\n        from omni.isaac.core import Weather\r\n\r\n        weather = Weather()\r\n\r\n        # Randomize cloud density\r\n        cloud_density = random.uniform(*self.randomization_params['weather']['cloud_density_range'])\r\n        weather.SetCloudDensity(cloud_density)\r\n\r\n        # Randomize fog\r\n        fog_density = random.uniform(*self.randomization_params['weather']['fog_density_range'])\r\n        weather.SetFogDensity(fog_density)\r\n\r\n        # Randomize rain (if available)\r\n        rain_intensity = random.uniform(*self.randomization_params['weather']['rain_intensity_range'])\r\n        weather.SetRainIntensity(rain_intensity)\r\n\r\n        weather.Apply()\r\n\r\n    def randomize_materials(self):\r\n        \"\"\"Randomize material properties\"\"\"\r\n        from omni.isaac.core.materials import Material\r\n\r\n        # Get all materials in scene\r\n        materials = self.scene.GetMaterials()\r\n\r\n        for material in materials:\r\n            if material.GetPrim().GetName() == \"DefaultMaterial\":\r\n                continue  # Skip default material\r\n\r\n            # Randomize roughness\r\n            roughness = random.uniform(*self.randomization_params['materials']['roughness_range'])\r\n            material.SetRoughness(roughness)\r\n\r\n            # Randomize metallic property\r\n            metallic = random.uniform(*self.randomization_params['materials']['metallic_range'])\r\n            material.SetMetallic(metallic)\r\n\r\n            # Randomize base color\r\n            color_variation = self.randomization_params['materials']['base_color_variation']\r\n            current_color = material.GetBaseColor()\r\n            new_color = tuple(\r\n                max(0, min(1, c + random.uniform(-color_variation, color_variation)))\r\n                for c in current_color\r\n            )\r\n            material.SetBaseColor(new_color)\r\n\r\n    def randomize_geometry(self):\r\n        \"\"\"Randomize object geometry\"\"\"\r\n        from omni.isaac.core.utils.nucleus import get_prims\r\n\r\n        prims = get_prims()\r\n\r\n        for prim in prims:\r\n            # Randomize scale\r\n            scale_range = self.randomization_params['geometry']['scale_range']\r\n            scale = random.uniform(*scale_range)\r\n            prim.SetScale((scale, scale, scale))\r\n\r\n            # Randomize rotation\r\n            rotation_range = self.randomization_params['geometry']['rotation_range']\r\n            rotation = random.uniform(-rotation_range, rotation_range) * np.pi / 180\r\n            prim.SetOrientation(rotation)\r\n\r\n            # Randomize position slightly\r\n            jitter = self.randomization_params['geometry']['position_jitter']\r\n            current_pos = prim.GetPosition()\r\n            jittered_pos = tuple(\r\n                pos + random.uniform(-jitter, jitter)\r\n                for pos in current_pos\r\n            )\r\n            prim.SetPosition(jittered_pos)\r\n\r\n    def get_randomization_summary(self):\r\n        \"\"\"Get summary of applied randomization\"\"\"\r\n        summary = {\r\n            'lighting': f\"Sun intensity randomized between {self.randomization_params['lighting']['sun_intensity_range']}\",\r\n            'weather': f\"Weather conditions varied including clouds, fog, and rain\",\r\n            'materials': f\"Material properties varied within specified ranges\",\r\n            'geometry': f\"Object geometry randomized with scale and rotation\"\r\n        }\r\n        return summary\r\n\r\n# Advanced domain randomization for specific training scenarios\r\nclass AdvancedDomainRandomization(DomainRandomization):\r\n    def __init__(self, scene):\r\n        super().__init__(scene)\r\n        self.training_scenarios = ['outdoor_navigation', 'indoor_manipulation', 'mixed_environment']\r\n        self.current_scenario = None\r\n\r\n    def setup_scenario_randomization(self, scenario_type):\r\n        \"\"\"Setup domain randomization for specific training scenario\"\"\"\r\n        self.current_scenario = scenario_type\r\n\r\n        if scenario_type == 'outdoor_navigation':\r\n            self.setup_outdoor_navigation_randomization()\r\n        elif scenario_type == 'indoor_manipulation':\r\n            self.setup_indoor_manipulation_randomization()\r\n        elif scenario_type == 'mixed_environment':\r\n            self.setup_mixed_environment_randomization()\r\n\r\n    def setup_outdoor_navigation_randomization(self):\r\n        \"\"\"Randomization optimized for outdoor navigation training\"\"\"\r\n        # Enhanced weather effects\r\n        self.randomization_params['weather']['rain_intensity_range'] = (0.0, 0.5)\r\n        self.randomization_params['weather']['fog_density_range'] = (0.0, 0.2)\r\n        self.randomization_params['weather']['cloud_density_range'] = (0.0, 0.6)\r\n\r\n        # Ground material variation\r\n        self.randomization_params['materials']['roughness_range'] = (0.4, 0.9)\r\n\r\n        # Time of day simulation (lighting angle)\r\n        self.randomize_sun_angle()\r\n\r\n    def setup_indoor_manipulation_randomization(self):\r\n        \"\"\"Randomization optimized for indoor manipulation training\"\"\"\r\n        # Indoor lighting conditions\r\n        self.randomization_params['lighting']['sun_intensity_range'] = (500, 1500)\r\n        self.randomization_params['lighting']['ambient_intensity_range'] = (200, 600)\r\n\r\n        # Object material variation\r\n        self.randomization_params['materials']['roughness_range'] = (0.2, 0.7)\r\n        self.randomization_params['materials']['metallic_range'] = (0.1, 0.6)\r\n\r\n        # Minimal weather effects for indoor\r\n        self.randomization_params['weather']['fog_density_range'] = (0.0, 0.05)\r\n        self.randomization_params['weather']['rain_intensity_range'] = (0.0, 0.1)\r\n\r\n    def setup_mixed_environment_randomization(self):\r\n        \"\"\"Randomization for mixed indoor/outdoor scenarios\"\"\"\r\n        # Wide range of conditions\r\n        self.randomization_params['lighting']['sun_intensity_range'] = (400, 2500)\r\n        self.randomization_params['weather']['fog_density_range'] = (0.0, 0.15)\r\n        self.randomization_params['weather']['cloud_density_range'] = (0.0, 0.7)\r\n\r\n        # Varied material properties\r\n        self.randomization_params['materials']['roughness_range'] = (0.2, 0.8)\r\n        self.randomization_params['materials']['metallic_range'] = (0.0, 0.7)\r\n\r\n    def randomize_sun_angle(self):\r\n        \"\"\"Randomize sun angle to simulate different times of day\"\"\"\r\n        from omni.isaac.core import Light\r\n\r\n        sun_light = self.scene.GetLighting().GetDistantLights()[0]\r\n\r\n        # Random sun elevation angle (20-70 degrees)\r\n        elevation = random.uniform(20, 70) * np.pi / 180\r\n        # Random azimuth angle (0-360 degrees)\r\n        azimuth = random.uniform(0, 360) * np.pi / 180\r\n\r\n        # Convert to quaternion\r\n        x = np.sin(elevation) * np.sin(azimuth)\r\n        y = np.sin(elevation) * np.cos(azimuth)\r\n        z = np.cos(elevation)\r\n        w = 0\r\n\r\n        sun_light.SetCameraPose(x, y, z, w)\n"})}),"\n",(0,t.jsx)(e.h3,{id:"922-ground-truth-generation",children:"9.2.2 Ground Truth Generation"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example: Ground Truth Data Generation"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-pythonimport",metastring:"numpy as np",children:"from isaacsim import SimulationApp\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.nucleus import get_current_stage\r\n\r\nclass GroundTruthGenerator:\r\n    def __init__(self, scene):\r\n        self.scene = scene\r\n        self.ground_truth_dir = \"synthetic_data/ground_truth\"\r\n\r\n        # Ground truth types to generate\r\n        self.gt_types = [\r\n            'semantic_segmentation',\r\n            'instance_segmentation',\r\n            'depth',\r\n            'normal',\r\n            'optical_flow',\r\n            'bounding_boxes',\r\n            'keypoints',\r\n            'robot_state'\r\n        ]\r\n\r\n    def generate_all_ground_truth(self, frame_number):\r\n        \"\"\"Generate all types of ground truth data\"\"\"\r\n        gt_data = {}\r\n\r\n        for gt_type in self.gt_types:\r\n            if gt_type == 'semantic_segmentation':\r\n                gt_data[gt_type] = self.generate_semantic_segmentation()\r\n            elif gt_type == 'instance_segmentation':\r\n                gt_data[gt_type] = self.generate_instance_segmentation()\r\n            elif gt_type == 'depth':\r\n                gt_data[gt_type] = self.generate_depth_data()\r\n            elif gt_type == 'normal':\r\n                gt_data[gt_type] = self.generate_normal_data()\r\n            elif gt_type == 'bounding_boxes':\r\n                gt_data[gt_type] = self.generate_bounding_boxes()\r\n            elif gt_type == 'robot_state':\r\n                gt_data[gt_type] = self.generate_robot_state()\r\n\r\n        # Save ground truth data\r\n        self.save_ground_truth(gt_data, frame_number)\r\n\r\n        return gt_data\r\n\r\n    def generate_semantic_segmentation(self):\r\n        \"\"\"Generate semantic segmentation ground truth\"\"\"\r\n        from omni.isaac.core.utils.nucleus import get_prims\r\n\r\n        # Get all objects in scene\r\n        prims = get_prims()\r\n\r\n        # Create semantic segmentation map\r\n        segmentation_map = np.zeros((1080, 1920), dtype=np.uint8)\r\n\r\n        # Object class mapping\r\n        class_mapping = {\r\n            'ground_plane': 0,\r\n            'building': 1,\r\n            'vehicle': 2,\r\n            'robot': 3,\r\n            'obstacle': 4,\r\n            'person': 5,\r\n            'vegetation': 6,\r\n            'sky': 7\r\n        }\r\n\r\n        for prim in prims:\r\n            class_name = self.get_object_class(prim.GetName())\r\n            if class_name in class_mapping:\r\n                # Get object's 2D projection\r\n                projection = self.get_object_projection(prim)\r\n                # Apply to segmentation map\r\n                mask = self.project_to_image(prim, projection)\r\n                segmentation_map[mask == 1] = class_mapping[class_name]\r\n\r\n        return segmentation_map\r\n\r\n    def generate_instance_segmentation(self):\r\n        \"\"\"Generate instance segmentation ground truth\"\"\"\r\n        from omni.isaac.core.utils.nucleus import get_prims\r\n\r\n        prims = get_prims()\r\n\r\n        # Create instance segmentation map\r\n        instance_map = np.zeros((1080, 1920), dtype=np.uint16)\r\n\r\n        for i, prim in enumerate(prims):\r\n            projection = self.get_object_projection(prim)\r\n            mask = self.project_to_image(prim, projection)\r\n            instance_map[mask == 1] = i + 1  # Instance IDs start from 1\r\n\r\n        return instance_map\r\n\r\n    def generate_depth_data(self):\r\n        \"\"\"Generate depth ground truth\"\"\"\r\n        from omni.isaac.core. import Camera\r\n\r\n        cameras = self.scene.GetCameras()\r\n        depth_data = {}\r\n\r\n        for camera in cameras:\r\n            # Get camera parameters\r\n            intrinsics = camera.GetIntrinsics()\r\n\r\n            # Render depth buffer\r\n            depth_buffer = camera.GetDepthBuffer()\r\n\r\n            # Convert to depth image\r\n            depth_image = self.depth_buffer_to_image(depth_buffer, intrinsics)\r\n            depth_data[camera.GetName()] = depth_image\r\n\r\n        return depth_data\r\n\r\n    def generate_normal_data(self):\r\n        \"\"\"Generate surface normal ground truth\"\"\"\r\n        from omni.isaac.core import Geometry\r\n\r\n        # Get all geometry in scene\r\n        geometries = self.scene.GetGeometries()\r\n\r\n        # Create normal map\r\n        normal_map = np.zeros((1080, 1920, 3), dtype=np.float32)\r\n\r\n        for geometry in geometries:\r\n            # Get mesh normals\r\n            mesh_normals = geometry.GetMeshNormals()\r\n\r\n            # Project normals to image space\r\n            normal_projection = self.project_normals_to_image(geometry, mesh_normals)\r\n\r\n            # Apply to normal map\r\n            mask = self.project_to_image(geometry, normal_projection['mask'])\r\n            normal_map[mask == 1] = normal_projection['normals'][mask == 1]\r\n\r\n        return normal_map\r\n\r\n    def generate_bounding_boxes(self):\r\n        \"\"\"Generate bounding box ground truth\"\"\"\r\n        from omni.isaac.core.utils.nucleus import get_prims\r\n\r\n        prims = get_prims()\r\n\r\n        bounding_boxes = []\r\n\r\n        for prim in prims:\r\n            if self.should_detect_object(prim.GetName()):\r\n                # Get 3D bounding box\r\n                bbox_3d = prim.GetLocalBoundingBox()\r\n\r\n                # Project to 2D\r\n                bbox_2d = self.project_3d_bbox_to_2d(bbox_3d)\r\n\r\n                # Create bounding box record\r\n                bbox_record = {\r\n                    'class_name': self.get_object_class(prim.GetName()),\r\n                    'bbox_2d': bbox_2d,\r\n                    'bbox_3d': bbox_3d,\r\n                    'confidence': 1.0,\r\n                    'occlusion': self.calculate_occlusion(prim)\r\n                }\r\n\r\n                bounding_boxes.append(bbox_record)\r\n\r\n        return bounding_boxes\r\n\r\n    def generate_robot_state(self):\r\n        \"\"\"Generate robot state ground truth\"\"\"\r\n        from omni.isaac.core.robots import Robot\r\n\r\n        robots = self.scene.GetRobots()\r\n        robot_states = []\r\n\r\n        for robot in robots:\r\n            # Get robot configuration\r\n            joint_positions = robot.GetJointPositions()\r\n            joint_velocities = robot.GetJointVelocities()\r\n\r\n            # Get robot pose\r\n            pose = robot.GetWorldPose()\r\n\r\n            # Create robot state record\r\n            robot_state = {\r\n                'robot_name': robot.GetName(),\r\n                'joint_positions': joint_positions,\r\n                'joint_velocities': joint_velocities,\r\n                'pose': pose,\r\n                'timestamp': self.get_current_time()\r\n            }\r\n\r\n            robot_states.append(robot_state)\r\n\r\n        return robot_states\r\n\r\n    def save_ground_truth(self, gt_data, frame_number):\r\n        \"\"\"Save ground truth data to files\"\"\"\r\n        import os\r\n        import pickle\r\n        import json\r\n\r\n        os.makedirs(self.ground_truth_dir, exist_ok=True)\r\n\r\n        for gt_type, data in gt_data.items():\r\n            filename = f\"{self.ground_truth_dir}/frame_{frame_number:06d}_{gt_type}\"\r\n\r\n            if gt_type in ['semantic_segmentation', 'instance_segmentation']:\r\n                # Save as image\r\n                self.save_image(data, f\"{filename}.png\")\r\n            elif gt_type == 'depth':\r\n                # Save depth data\r\n                self.save_depth_data(data, f\"{filename}.npy\")\r\n            elif gt_type in ['bounding_boxes', 'robot_state']:\r\n                # Save as JSON\r\n                with open(f\"{filename}.json\", 'w') as f:\r\n                    json.dump(data, f, indent=2)\r\n            else:\r\n                # Save as pickle\r\n                with open(f\"{filename}.pkl\", 'wb') as f:\r\n                    pickle.dump(data, f)\r\n\r\n    def get_object_class(self, prim_name):\r\n        \"\"\"Get object class from primitive name\"\"\"\r\n        # Extract class name from primitive name\r\n        if 'ground' in prim_name.lower():\r\n            return 'ground_plane'\r\n        elif 'building' in prim_name.lower() or 'wall' in prim_name.lower():\r\n            return 'building'\r\n        elif 'vehicle' in prim_name.lower() or 'car' in prim_name.lower():\r\n            return 'vehicle'\r\n        elif 'robot' in prim_name.lower():\r\n            return 'robot'\r\n        elif 'person' in prim_name.lower() or 'human' in prim_name.lower():\r\n            return 'person'\r\n        elif 'tree' in prim_name.lower() or 'grass' in prim_name.lower():\r\n            return 'vegetation'\r\n        else:\r\n            return 'obstacle'\r\n\r\n    def should_detect_object(self, prim_name):\r\n        \"\"\"Determine if object should be detected\"\"\"\r\n        # Objects to exclude from detection\r\n        exclude_names = ['ground_plane', 'light', 'camera', 'sky']\r\n\r\n        return not any(exclude in prim_name.lower() for exclude in exclude_names)\r\n\r\n    # Helper methods for projection and rendering\r\n    def get_object_projection(self, prim):\r\n        \"\"\"Get object's 2D projection\"\"\"\r\n        # Implementation would calculate object's screen space projection\r\n        pass\r\n\r\n    def project_to_image(self, prim, projection):\r\n        \"\"\"Project 3D object to 2D image\"\"\"\r\n        # Implementation would project 3D coordinates to 2D image space\r\n        pass\r\n\r\n    def depth_buffer_to_image(self, depth_buffer, intrinsics):\r\n        \"\"\"Convert depth buffer to depth image\"\"\"\r\n        # Implementation would convert depth buffer to actual depth values\r\n        pass\r\n\r\n    def project_normals_to_image(self, geometry, normals):\r\n        \"\"\"Project mesh normals to image space\"\"\"\r\n        # Implementation would project 3D normals to 2D image space\r\n        pass\r\n\r\n    def project_3d_bbox_to_2d(self, bbox_3d):\r\n        \"\"\"Project 3D bounding box to 2D coordinates\"\"\"\r\n        # Implementation would calculate 2D bounding box projection\r\n        pass\r\n\r\n    def calculate_occlusion(self, prim):\r\n        \"\"\"Calculate object occlusion percentage\"\"\"\r\n        # Implementation would calculate how much of object is occluded\r\n        pass\r\n\r\n    def save_image(self, image_data, filename):\r\n        \"\"\"Save image data to file\"\"\"\r\n        import cv2\r\n        cv2.imwrite(filename, image_data)\r\n\r\n    def save_depth_data(self, depth_data, filename):\r\n        \"\"\"Save depth data to file\"\"\"\r\n        np.save(filename, depth_data)\r\n\r\n    def get_current_time(self):\r\n        \"\"\"Get current simulation time\"\"\"\r\n        import time\r\n        return time.time()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"93-ai-training-integration",children:"9.3 AI Training Integration"}),"\n",(0,t.jsx)(e.h3,{id:"931-synthetic-data-pipeline",children:"9.3.1 Synthetic Data Pipeline"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example: AI Training Data Pipeline"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-pythonimport",metastring:"torch",children:'import torch.utils.data as data\r\nfrom torchvision import transforms\r\nimport numpy as np\r\n\r\nclass SyntheticDataLoader(data.Dataset):\r\n    def __init__(self, gt_data_dir, transform=None, target_transform=None):\r\n        self.gt_data_dir = gt_data_dir\r\n        self.transform = transform\r\n        self.target_transform = target_transform\r\n\r\n        # Find all frame directories\r\n        self.frame_dirs = [d for d in os.listdir(gt_data_dir)\r\n                            if os.path.isdir(os.path.join(gt_data_dir, d))]\r\n        self.frame_dirs.sort()\r\n\r\n        # Ground truth types to load\r\n        self.gt_types = [\'rgb\', \'semantic_segmentation\', \'depth\', \'bounding_boxes\']\r\n\r\n    def __len__(self):\r\n        return len(self.frame_dirs)\r\n\r\n    def __getitem__(self, idx):\r\n        frame_dir = os.path.join(self.gt_data_dir, self.frame_dirs[idx])\r\n\r\n        # Load all ground truth data for this frame\r\n        data = {}\r\n        for gt_type in self.gt_types:\r\n            data[gt_type] = self.load_ground_truth(frame_dir, gt_type)\r\n\r\n        # Apply transforms\r\n        if self.transform:\r\n            data[\'rgb\'] = self.transform(data[\'rgb\'])\r\n\r\n        if self.target_transform:\r\n            if \'semantic_segmentation\' in data:\r\n                data[\'semantic_segmentation\'] = self.target_transform(data[\'semantic_segmentation\'])\r\n            if \'depth\' in data:\r\n                data[\'depth\'] = self.target_transform(data[\'depth\'])\r\n\r\n        return data\r\n\r\n    def load_ground_truth(self, frame_dir, gt_type):\r\n        """Load specific ground truth type"""\r\n        if gt_type == \'rgb\':\r\n            return self.load_image(frame_dir, \'rgb\')\r\n        elif gt_type == \'semantic_segmentation\':\r\n            return self.load_segmentation(frame_dir, \'semantic_segmentation\')\r\n        elif gt_type == \'depth\':\r\n            return self.load_depth(frame_dir, \'depth\')\r\n        elif gt_type == \'bounding_boxes\':\r\n            return self.load_bounding_boxes(frame_dir, \'bounding_boxes\')\r\n        else:\r\n            return None\r\n\r\n    def load_image(self, frame_dir, image_type):\r\n        """Load RGB image"""\r\n        import cv2\r\n        image_path = os.path.join(frame_dir, f"frame_{self.frame_dirs.index(os.path.basename(frame_dir)):06d}_rgb.png")\r\n        image = cv2.imread(image_path)\r\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n        return image\r\n\r\n    def load_segmentation(self, frame_dir, seg_type):\r\n        """Load segmentation mask"""\r\n        import cv2\r\n        seg_path = os.path.join(frame_dir, f"frame_{self.frame_dirs.index(os.path.basename(frame_dir)):06d}_{seg_type}.png")\r\n        mask = cv2.imread(seg_path, cv2.IMREAD_GRAYSCALE)\r\n        return mask\r\n\r\n    def load_depth(self, frame_dir, depth_type):\r\n        """Load depth data"""\r\n        import numpy as np\r\n        depth_path = os.path.join(frame_dir, f"frame_{self.frame_dirs.index(os.path.basename(frame_dir)):06d}_{depth_type}.npy")\r\n        depth = np.load(depth_path)\r\n        return depth\r\n\r\n    def load_bounding_boxes(self, frame_dir, bbox_type):\r\n        """Load bounding boxes"""\r\n        import json\r\n        bbox_path = os.path.join(frame_dir, f"frame_{self.frame_dirs.index(os.path.basename(frame_dir)):06d}_{bbox_type}.json")\r\n        with open(bbox_path, \'r\') as f:\r\n            bboxes = json.load(f)\r\n        return bboxes\r\n\r\n# Training pipeline for computer vision models\r\nclass VisionTrainingPipeline:\r\n    def __init__(self, gt_data_dir):\r\n        self.gt_data_dir = gt_data_dir\r\n\r\n        # Data transforms\r\n        self.image_transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n            transforms.RandomHorizontalFlip(p=0.5),\r\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\r\n        ])\r\n\r\n        self.target_transform = transforms.Compose([\r\n            transforms.ToTensor()\r\n        ])\r\n\r\n    def create_dataloader(self, batch_size=8, shuffle=True, num_workers=4):\r\n        """Create data loader for training"""\r\n        dataset = SyntheticDataLoader(\r\n            self.gt_data_dir,\r\n            transform=self.image_transform,\r\n            target_transform=self.target_transform\r\n        )\r\n\r\n        return data.DataLoader(\r\n            dataset,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            num_workers=num_workers\r\n        )\r\n\r\n# Training loop example\r\ndef train_vision_model():\r\n    """Train computer vision model with synthetic data"""\r\n    gt_data_dir = "synthetic_data/ground_truth"\r\n\r\n    # Create data loader\r\n    pipeline = VisionTrainingPipeline(gt_data_dir)\r\n    train_loader = pipeline.create_dataloader(batch_size=4)\r\n\r\n    # Initialize model (example: segmentation model)\r\n    model = SegmentationModel(num_classes=8)\r\n\r\n    # Training loop\r\n    for epoch in range(100):\r\n        for batch in train_loader:\r\n            images = batch[\'rgb\']\r\n            targets = batch[\'semantic_segmentation\']\r\n\r\n            # Forward pass\r\n            outputs = model(images)\r\n            loss = calculate_loss(outputs, targets)\r\n\r\n            # Backward pass\r\n            loss.backward()\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")\r\n\r\ndef calculate_loss(outputs, targets):\r\n    """Calculate training loss"""\r\n    import torch.nn.functional as F\r\n\r\n    # Use CrossEntropyLoss for segmentation\r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    return criterion(outputs, targets)\r\n\r\n# Class for model training evaluation\r\nclass ModelEvaluator:\r\n    def __init__(self, model, test_loader):\r\n        self.model = model\r\n        self.test_loader = test_loader\r\n\r\n    def evaluate(self):\r\n        """Evaluate model performance"""\r\n        self.model.eval()\r\n\r\n        total_loss = 0.0\r\n        total_samples = 0\r\n        iou_scores = []\r\n\r\n        with torch.no_grad():\r\n            for batch in self.test_loader:\r\n                images = batch[\'rgb\']\r\n                targets = batch[\'semantic_segmentation\']\r\n\r\n                outputs = self.model(images)\r\n                loss = calculate_loss(outputs, targets)\r\n\r\n                total_loss += loss.item()\r\n                total_samples += images.size(0)\r\n\r\n                # Calculate IoU for segmentation\r\n                iou = self.calculate_iou(outputs, targets)\r\n                iou_scores.append(iou)\r\n\r\n        avg_loss = total_loss / len(self.test_loader)\r\n        avg_iou = np.mean(iou_scores)\r\n\r\n        print(f"Test Loss: {avg_loss:.4f}, Mean IoU: {avg_iou:.4f}")\r\n        return avg_loss, avg_iou\r\n\r\n    def calculate_iou(self, predictions, targets):\r\n        """Calculate Intersection over Union"""\r\n        # Convert predictions to class indices\r\n        pred_classes = torch.argmax(predictions, dim=1)\r\n\r\n        # Calculate IoU for each class\r\n        intersection = (pred_classes == targets).float().sum((1, 2))\r\n        union = ((pred_classes == targets) | (pred_classes != targets)).float().sum((1, 2))\r\n\r\n        iou = intersection / (union + 1e-6)\r\n        return iou.mean().item()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"This chapter explored NVIDIA Isaac Sim's capabilities for advanced robotics simulation and synthetic data generation. Isaac Sim's integration with AI training pipelines and domain randomization capabilities make it an essential tool for modern robotics development."}),"\n",(0,t.jsx)(e.p,{children:"Key takeaways:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Isaac Sim provides photorealistic simulation environments"}),"\n",(0,t.jsx)(e.li,{children:"Domain randomization is crucial for robust AI training"}),"\n",(0,t.jsx)(e.li,{children:"Synthetic data generation accelerates model training"}),"\n",(0,t.jsx)(e.li,{children:"Ground truth automation ensures accurate labeling"}),"\n",(0,t.jsx)(e.li,{children:"Integration with AI training pipelines is seamless"}),"\n",(0,t.jsx)(e.li,{children:"Cloud deployment enables scalable simulation"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(e.h3,{id:"exercise-91-isaac-sim-setup",children:"Exercise 9.1: Isaac Sim Setup"}),"\n",(0,t.jsx)(e.p,{children:"Set up Isaac Sim environment:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Install Isaac Sim and required dependencies"}),"\n",(0,t.jsx)(e.li,{children:"Configure simulation settings"}),"\n",(0,t.jsx)(e.li,{children:"Create basic scene with objects"}),"\n",(0,t.jsx)(e.li,{children:"Test physics simulation"}),"\n",(0,t.jsx)(e.li,{children:"Validate rendering quality"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-92-domain-randomization",children:"Exercise 9.2: Domain Randomization"}),"\n",(0,t.jsx)(e.p,{children:"Implement domain randomization:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Create randomization parameters"}),"\n",(0,t.jsx)(e.li,{children:"Apply lighting and material variations"}),"\n",(0,t.jsx)(e.li,{children:"Test randomization effectiveness"}),"\n",(0,t.jsx)(e.li,{children:"Measure impact on model performance"}),"\n",(0,t.jsx)(e.li,{children:"Optimize randomization ranges"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-93-ground-truth-generation",children:"Exercise 9.3: Ground Truth Generation"}),"\n",(0,t.jsx)(e.p,{children:"Develop ground truth generation:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement semantic segmentation"}),"\n",(0,t.jsx)(e.li,{children:"Create depth and normal maps"}),"\n",(0,t.jsx)(e.li,{children:"Generate bounding box annotations"}),"\n",(0,t.jsx)(e.li,{children:"Save data in standard formats"}),"\n",(0,t.jsx)(e.li,{children:"Validate annotation accuracy"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-94-ai-training-pipeline",children:"Exercise 9.4: AI Training Pipeline"}),"\n",(0,t.jsx)(e.p,{children:"Build AI training pipeline:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Create synthetic data loader"}),"\n",(0,t.jsx)(e.li,{children:"Implement data augmentation"}),"\n",(0,t.jsx)(e.li,{children:"Train vision model"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate model performance"}),"\n",(0,t.jsx)(e.li,{children:"Compare with real data training"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-95-advanced-isaac-sim-features",children:"Exercise 9.5: Advanced Isaac Sim Features"}),"\n",(0,t.jsx)(e.p,{children:"Explore advanced features:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement ray tracing simulation"}),"\n",(0,t.jsx)(e.li,{children:"Create complex material systems"}),"\n",(0,t.jsx)(e.li,{children:"Set up multi-robot scenarios"}),"\n",(0,t.jsx)(e.li,{children:"Use cloud simulation"}),"\n",(0,t.jsx)(e.li,{children:"Optimize performance"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"glossary-terms",children:"Glossary Terms"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac Sim"}),": NVIDIA's robotics simulation platform built on Omniverse"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Omniverse"}),": NVIDIA's collaborative 3D simulation platform"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"USD (Universal Scene Description)"}),": Pixar's open 3D scene format"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Domain Randomization"}),": Systematic variation of simulation parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Synthetic Data"}),": Computer-generated training data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Ground Truth"}),": Accurate labels and annotations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"PhysX 5"}),": NVIDIA's advanced physics engine"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Ray Tracing"}),": Real-time global illumination rendering"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Subsurface Scattering"}),": Light transport through translucent materials"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Semantic Segmentation"}),": Pixel-wise classification of scene elements"]}),"\n"]})]})}function c(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>s});var i=r(6540);const t={},a=i.createContext(t);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);