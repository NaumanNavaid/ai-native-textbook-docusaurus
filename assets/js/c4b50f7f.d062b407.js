"use strict";(globalThis.webpackChunkai_native_textbook_docusaurus=globalThis.webpackChunkai_native_textbook_docusaurus||[]).push([[628],{3991:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"sample-chapter","title":"Chapter 2: Neural Networks for Robotics","description":"Deep learning approaches for perception and control","source":"@site/docs/sample-chapter.mdx","sourceDirName":".","slug":"/sample-chapter","permalink":"/ai-native-textbook-docusaurus/docs/sample-chapter","draft":false,"unlisted":false,"editUrl":"https://github.com/NaumanNavaid/ai-native-textbook-docusaurus/tree/main/docs/sample-chapter.mdx","tags":[],"version":"current","frontMatter":{}}');var r=i(4848),o=i(8453);const s={},a="Chapter 2: Neural Networks for Robotics",l={},c=[{value:"Visual Perception Pipeline",id:"visual-perception-pipeline",level:3},{value:"Imitation Learning",id:"imitation-learning",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:3},{value:"Multi-Modal Sensor Fusion",id:"multi-modal-sensor-fusion",level:4},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-2-neural-networks-for-robotics",children:"Chapter 2: Neural Networks for Robotics"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Deep learning approaches for perception and control"})}),"\n",(0,r.jsx)(n.h1,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Neural networks have revolutionized robotics by enabling machines to learn from experience and adapt to new situations. This chapter explores how deep learning techniques are applied to robotic systems."}),"\n",(0,r.jsx)(n.h1,{id:"perception-systems",children:"Perception Systems"}),"\n",(0,r.jsx)(n.p,{children:"Modern robots use advanced perception systems to understand their environment. Computer vision, powered by convolutional neural networks (CNNs), allows robots to identify objects, navigate spaces, and interact with humans."}),"\n",(0,r.jsx)(n.h3,{id:"visual-perception-pipeline",children:"Visual Perception Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"The visual perception pipeline typically consists of several stages:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Image acquisition from cameras"}),"\n",(0,r.jsx)(n.li,{children:"Preprocessing and normalization"}),"\n",(0,r.jsx)(n.li,{children:"Feature extraction using CNNs"}),"\n",(0,r.jsx)(n.li,{children:"Object detection and segmentation"}),"\n",(0,r.jsx)(n.li,{children:"Scene understanding"}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{title:"Did you know?",type:"info",children:(0,r.jsx)(n.p,{children:"Modern robot vision systems can process over 60 frames per second, enabling real-time decision making in dynamic environments."})}),"\n",(0,r.jsx)(n.h1,{id:"learning-from-demonstration",children:"Learning from Demonstration"}),"\n",(0,r.jsx)(n.h3,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,r.jsx)(n.p,{children:"Imitation learning allows robots to learn tasks by observing human demonstrations. This approach is particularly useful for complex manipulation tasks that are difficult to program manually."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="Neural Network for Imitation Learning"',children:"# Imitation Learning Example\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass ImitationNetwork(nn.Module):\r\n    def __init__(self, input_dim, output_dim):\r\n        super().__init__()\r\n        self.fc1 = nn.Linear(input_dim, 256)\r\n        self.fc2 = nn.Linear(256, 128)\r\n        self.fc3 = nn.Linear(128, output_dim)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(self, x):\r\n        x = self.relu(self.fc1(x))\r\n        x = self.relu(self.fc2(x))\r\n        return torch.tanh(self.fc3(x))\n"})}),"\n",(0,r.jsx)(n.h3,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,r.jsx)(n.p,{children:"Reinforcement learning enables robots to learn optimal behaviors through interaction with their environment. The agent receives rewards for desired actions and learns to maximize cumulative reward over time."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"The combination of deep neural networks with reinforcement learning has led to breakthroughs in robotic manipulation, locomotion, and navigation."}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"advanced-topics",children:"Advanced Topics"}),"\n",(0,r.jsx)(n.h3,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Robots often need to integrate information from multiple sensors\u2014vision, touch, audio, and proprioception\u2014to make informed decisions."}),"\n",(0,r.jsx)(n.h4,{id:"multi-modal-sensor-fusion",children:"Multi-Modal Sensor Fusion"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensors"}),(0,r.jsx)(n.th,{children:"Features"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RGB Cameras"}),(0,r.jsx)(n.td,{children:"Object Detection"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Depth Sensors"}),(0,r.jsx)(n.td,{children:"3D Position"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Microphones"}),(0,r.jsx)(n.td,{children:"Sound Recognition"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Force Sensors"}),(0,r.jsx)(n.td,{children:"Force Feedback"})]})]})]}),"\n",(0,r.jsx)(n.admonition,{title:"Challenge",type:"warning",children:(0,r.jsx)(n.p,{children:"Sensor fusion requires careful synchronization and calibration to ensure accurate and reliable perception."})}),"\n",(0,r.jsx)(n.h1,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Neural networks continue to push the boundaries of what's possible in robotics. As we develop more sophisticated architectures and training methods, we move closer to creating truly intelligent robotic systems that can operate alongside humans in everyday environments."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deep learning"})," enables robots to learn complex behaviors from data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-modal fusion"})," combines information from different sensors for robust perception"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement learning"})," allows robots to learn through trial and error"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Imitation learning"})," provides an intuitive way to teach robots new skills"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, we'll explore how these neural network approaches are applied to specific robotic applications including manipulation, navigation, and human-robot interaction."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const r={},o=t.createContext(r);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);