"use strict";(globalThis.webpackChunkai_native_textbook_docusaurus=globalThis.webpackChunkai_native_textbook_docusaurus||[]).push([[5579],{7291:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"part-4-perception/chapter-13-computer-vision-robots","title":"Computer Vision for Robots","description":"13.1 Computer Vision Fundamentals","source":"@site/docs/part-4-perception/chapter-13-computer-vision-robots.mdx","sourceDirName":"part-4-perception","slug":"/part-4-perception/chapter-13-computer-vision-robots","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-13-computer-vision-robots","draft":false,"unlisted":false,"editUrl":"https://github.com/NaumanNavaid/ai-native-textbook-docusaurus/tree/main/docs/part-4-perception/chapter-13-computer-vision-robots.mdx","tags":[],"version":"current","frontMatter":{"title":"Computer Vision for Robots","part":4,"chapter":13,"difficulty":"advanced","prerequisites":["chapter-12-digital-twin-development","chapter-3-sensors-actuators-physical-limits"],"estimatedTime":60,"objectives":["Master computer vision fundamentals for robotic applications","Implement object detection and recognition systems","Develop visual SLAM capabilities for navigation","Apply deep learning for robotic perception"]},"sidebar":"chaptersSidebar","previous":{"title":"Digital Twin Development","permalink":"/ai-native-textbook-docusaurus/docs/part-3-simulation/chapter-12-digital-twin-development"},"next":{"title":"Sensor Fusion and State Estimation","permalink":"/ai-native-textbook-docusaurus/docs/part-4-perception/chapter-14-sensor-fusion-state-estimation"}}');var s=t(4848),r=t(8453);const o={title:"Computer Vision for Robots",part:4,chapter:13,difficulty:"advanced",prerequisites:["chapter-12-digital-twin-development","chapter-3-sensors-actuators-physical-limits"],estimatedTime:60,objectives:["Master computer vision fundamentals for robotic applications","Implement object detection and recognition systems","Develop visual SLAM capabilities for navigation","Apply deep learning for robotic perception"]},a="Chapter 13: Computer Vision for Robots",c={},d=[{value:"13.1 Computer Vision Fundamentals",id:"131-computer-vision-fundamentals",level:2},{value:"13.1.1 Vision in Robotics",id:"1311-vision-in-robotics",level:3},{value:"13.1.2 Image Formation and Camera Models",id:"1312-image-formation-and-camera-models",level:3},{value:"Pinhole Camera Model",id:"pinhole-camera-model",level:4}];function l(e){const n={admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",span:"span",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-13-computer-vision-for-robots",children:"Chapter 13: Computer Vision for Robots"})}),"\n",(0,s.jsx)(n.h2,{id:"131-computer-vision-fundamentals",children:"13.1 Computer Vision Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"1311-vision-in-robotics",children:"13.1.1 Vision in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Computer vision enables robots to perceive, understand, and interact with their environment through visual information. Unlike human vision, robotic vision systems must extract quantitatively precise information for decision-making and control."}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"Robotic computer vision differs from general computer vision in its emphasis on real-time performance, 3D spatial understanding, and action-oriented perception. While general CV focuses on image interpretation, robotic CV must convert visual information into actionable control signals."})}),"\n",(0,s.jsx)(n.h3,{id:"1312-image-formation-and-camera-models",children:"13.1.2 Image Formation and Camera Models"}),"\n",(0,s.jsx)(n.p,{children:"Understanding how cameras capture the physical world is fundamental:"}),"\n",(0,s.jsx)(n.h4,{id:"pinhole-camera-model",children:"Pinhole Camera Model"}),"\n",(0,s.jsx)(n.p,{children:"The pinhole camera model provides the mathematical foundation for understanding image formation:"}),"\n",(0,s.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Can't use function '$' in math mode at position 33: \u2026y = f * (Y / Z)$\u0332$\n\nWhere:\n- \u2026",style:{color:"#cc0000"},children:'x = f * (X / Z)\ny = f * (Y / Z)$$\n\nWhere:\n- (x, y) = Image coordinates\n- (X, Y, Z) = World coordinates\n- f = Focal length\n\n```python\nclass PinholeCamera:\n    def __init__(self, focal_length, image_width, image_height):\n        self.focal_length = focal_length\n        self.image_width = image_width\n        self.image_height = image_height\n\n        # Camera intrinsic matrix\n        self.intrinsic_matrix = np.array([\n            [focal_length, 0, image_width / 2],\n            [0, focal_length, image_height / 2],\n            [0, 0, 1]\n        ])\n\n        # Camera extrinsic matrix (position and orientation)\n        self.extrinsic_matrix = np.eye(4)\n\n    def world_to_pixel(self, world_point):\n        """Convert world coordinates to pixel coordinates"""\n        # Convert world point to homogeneous coordinates\n        world_homo = np.append(world_point, 1)\n\n        # Apply extrinsic transformation (world to camera)\n        camera_point = self.extrinsic_matrix @ world_homo\n        camera_point = camera_point[:3]\n\n        # Apply intrinsic projection (camera to image)\n        image_point_homo = self.intrinsic_matrix @ camera_point\n        image_point = image_point_homo / image_point_homo[2]\n\n        # Convert to pixel coordinates\n        pixel_x = int(image_point[0])\n        pixel_y = int(image_point[1])\n\n        return pixel_x, pixel_y\n\n    def pixel_to_ray(self, pixel_x, pixel_y):\n        """Convert pixel coordinates to camera ray"""\n        # Convert pixel to normalized coordinates\n        x = (pixel_x - self.intrinsic_matrix[0, 2]) / self.intrinsic_matrix[0, 0]\n        y = (pixel_y - self.intrinsic_matrix[1, 2]) / self.intrinsic_matrix[1, 1]\n\n        # Create ray in camera coordinates\n        ray_direction = np.array([x, y, 1.0])\n        ray_direction = ray_direction / np.linalg.norm(ray_direction)\n\n        # Transform to world coordinates\n        rotation = self.extrinsic_matrix[:3, :3]\n        translation = self.extrinsic_matrix[:3, 3]\n\n        world_origin = translation\n        world_direction = rotation @ ray_direction\n\n        return world_origin, world_direction\n\n    def project_3d_points(self, points_3d):\n        """Project 3D points to image plane"""\n        pixels = []\n        depths = []\n\n        for point in points_3d:\n            pixel_x, pixel_y = self.world_to_pixel(point)\n            pixels.append([pixel_x, pixel_y])\n\n            # Calculate depth\n            camera_point = self.extrinsic_matrix @ np.append(point, 1)\n            depths.append(camera_point[2])\n\n        return np.array(pixels), np.array(depths)\n```\n\n#### Lens Distortion Correction\nReal cameras exhibit lens distortion that must be corrected:\n\n```python\nclass LensDistortionModel:\n    def __init__(self, k1=0.0, k2=0.0, k3=0.0, p1=0.0, p2=0.0):\n        # Radial distortion coefficients\n        self.k1 = k1\n        self.k2 = k2\n        self.k3 = k3\n\n        # Tangential distortion coefficients\n        self.p1 = p1\n        self.p2 = p2\n\n    def distort_point(self, x, y):\n        """Apply lens distortion to normalized coordinates"""\n        r_squared = x**2 + y**2\n\n        # Radial distortion\n        radial_factor = 1 + self.k1 * r_squared + self.k2 * r_squared**2 + self.k3 * r_squared**3\n        x_distorted = x * radial_factor\n        y_distorted = y * radial_factor\n\n        # Tangential distortion\n        x_distorted += 2 * self.p1 * x * y + self.p2 * (r_squared + 2 * x**2)\n        y_distorted += self.p1 * (r_squared + 2 * y**2) + 2 * self.p2 * x * y\n\n        return x_distorted, y_distorted\n\n    def undistort_point(self, x_distorted, y_distorted, iterations=10):\n        """Remove lens distortion using iterative method"""\n        x, y = x_distorted, y_distorted\n\n        for _ in range(iterations):\n            # Calculate distortion at current estimate\n            x_d, y_d = self.distort_point(x, y)\n\n            # Update estimate\n            x = x + (x_distorted - x_d)\n            y = y + (y_distorted - y_d)\n\n        return x, y\n\n    def undistort_image(self, image):\n        """Undistort entire image"""\n        height, width = image.shape[:2]\n        undistorted = np.zeros_like(image)\n\n        # Create coordinate grids\n        y_coords, x_coords = np.mgrid[0:height, 0:width]\n\n        # Convert to normalized coordinates\n        cx, cy = width // 2, height // 2\n        fx = fy = max(width, height)  # Approximate focal length\n\n        x_norm = (x_coords - cx) / fx\n        y_norm = (y_coords - cy) / fy\n\n        # Undistort coordinates\n        for i in range(height):\n            for j in range(width):\n                x_undist, y_undist = self.undistort_point(x_norm[i, j], y_norm[i, j])\n\n                # Convert back to pixel coordinates\n                px = int(x_undist * fx + cx)\n                py = int(y_undist * fy + cy)\n\n                # Check bounds\n                if 0 <= px < width and 0 <= py < height:\n                    undistorted[i, j] = image[py, px]\n\n        return undistorted\n```\n\n## 13.2 Image Processing and Feature Extraction\n\n### 13.2.1 Preprocessing Techniques\n\nImage preprocessing prepares raw visual data for higher-level analysis:\n\n```python\nclass ImagePreprocessor:\n    def __init__(self):\n        self.gaussian_kernel = None\n        self.sobel_x = None\n        self.sobel_y = None\n\n    def preprocess_pipeline(self, image, operations=None):\n        """Apply preprocessing pipeline"""\n        if operations is None:\n            operations = [\'denoise\', \'normalize\', \'sharpen\']\n\n        processed = image.copy()\n\n        for operation in operations:\n            if operation == \'denoise\':\n                processed = self.denoise(processed)\n            elif operation == \'normalize\':\n                processed = self.normalize(processed)\n            elif operation == \'sharpen\':\n                processed = self.sharpen(processed)\n            elif operation == \'histogram_equalization\':\n                processed = self.histogram_equalization(processed)\n            elif operation == \'contrast_enhancement\':\n                processed = self.enhance_contrast(processed)\n\n        return processed\n\n    def denoise(self, image):\n        """Apply Gaussian denoising"""\n        # Create Gaussian kernel\n        if self.gaussian_kernel is None:\n            size = 5\n            sigma = 1.0\n            self.gaussian_kernel = self._create_gaussian_kernel(size, sigma)\n\n        # Apply convolution\n        return self._convolve(image, self.gaussian_kernel)\n\n    def _create_gaussian_kernel(self, size, sigma):\n        """Create Gaussian kernel"""\n        kernel = np.zeros((size, size))\n        center = size // 2\n\n        for i in range(size):\n            for j in range(size):\n                x, y = i - center, j - center\n                kernel[i, j] = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n\n        return kernel / kernel.sum()\n\n    def edge_detection(self, image, method=\'canny\', **kwargs):\n        """Detect edges in image"""\n        if method == \'sobel\':\n            return self._sobel_edge_detection(image)\n        elif method == \'canny\':\n            return self._canny_edge_detection(image, **kwargs)\n        elif method == \'laplacian\':\n            return self._laplacian_edge_detection(image)\n\n    def _canny_edge_detection(self, image, low_threshold=50, high_threshold=150):\n        """Canny edge detection implementation"""\n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        # Gaussian blur\n        blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)\n\n        # Gradient calculation using Sobel\n        grad_x = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=3)\n        grad_y = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=3)\n\n        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n        gradient_direction = np.arctan2(grad_y, grad_x)\n\n        # Non-maximum suppression\n        suppressed = self._non_maximum_suppression(gradient_magnitude, gradient_direction)\n\n        # Double thresholding\n        strong_edges = (suppressed > high_threshold)\n        weak_edges = (suppressed >= low_threshold) & (suppressed <= high_threshold)\n\n        # Edge tracking by hysteresis\n        edges = self._edge_tracking(strong_edges, weak_edges)\n\n        return edges.astype(np.uint8) * 255\n\n    def _non_maximum_suppression(self, magnitude, direction):\n        """Non-maximum suppression for edge thinning"""\n        suppressed = np.zeros_like(magnitude)\n        angle = np.rad2deg(direction) % 180\n\n        for i in range(1, magnitude.shape[0] - 1):\n            for j in range(1, magnitude.shape[1] - 1):\n                # Determine gradient direction\n                if (0 <= angle[i, j] < 22.5) or (157.5 <= angle[i, j] <= 180):\n                    q, r = magnitude[i, j + 1], magnitude[i, j - 1]\n                elif 22.5 <= angle[i, j] < 67.5:\n                    q, r = magnitude[i + 1, j - 1], magnitude[i - 1, j + 1]\n                elif 67.5 <= angle[i, j] < 112.5:\n                    q, r = magnitude[i + 1, j], magnitude[i - 1, j]\n                elif 112.5 <= angle[i, j] < 157.5:\n                    q, r = magnitude[i - 1, j - 1], magnitude[i + 1, j + 1]\n\n                # Suppress non-maximum pixels\n                if (magnitude[i, j] >= q) and (magnitude[i, j] >= r):\n                    suppressed[i, j] = magnitude[i, j]\n\n        return suppressed\n```\n\n### 13.2.2 Feature Detection and Description\n\nExtracting stable features is crucial for visual odometry and object recognition:\n\n```python\nclass FeatureExtractor:\n    def __init__(self):\n        self.feature_detectors = {\n            \'harris\': self._harris_corner_detector,\n            \'shi_tomasi\': self._shi_tomasi_corner_detector,\n            \'sift\': self._sift_detector,\n            \'orb\': self._orb_detector,\n            \'surf\': self._surf_detector\n        }\n\n    def extract_features(self, image, method=\'orb\', max_features=500):\n        """Extract features from image"""\n        if method not in self.feature_detectors:\n            raise ValueError(f"Unsupported feature detection method: {method}")\n\n        keypoints, descriptors = self.feature_detectors[method](image, max_features)\n\n        return keypoints, descriptors\n\n    def _orb_detector(self, image, max_features):\n        """ORB feature detector implementation"""\n        import cv2\n\n        # Initialize ORB detector\n        orb = cv2.ORB_create(nfeatures=max_features)\n\n        # Find keypoints and compute descriptors\n        keypoints, descriptors = orb.detectAndCompute(image, None)\n\n        return keypoints, descriptors\n\n    def _harris_corner_detector(self, image, max_features):\n        """Harris corner detector"""\n        import cv2\n\n        # Convert to grayscale\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        # Harris corner detection\n        corners = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=max_features,\n            qualityLevel=0.01,\n            minDistance=10,\n            blockSize=3\n        )\n\n        # Convert to keypoints\n        keypoints = []\n        if corners is not None:\n            for corner in corners:\n                x, y = corner.ravel()\n                kp = cv2.KeyPoint(x, y, 1)\n                keypoints.append(kp)\n\n        # Extract simple descriptors (patch around each keypoint)\n        descriptors = []\n        patch_size = 9\n\n        for kp in keypoints:\n            x, y = int(kp.pt[0]), int(kp.pt[1])\n\n            # Extract patch\n            patch = self._extract_patch(gray, x, y, patch_size)\n            descriptor = patch.flatten()\n            descriptors.append(descriptor)\n\n        return keypoints, np.array(descriptors) if descriptors else None\n\n    def match_features(self, desc1, desc2, method=\'bfm\', ratio_threshold=0.75):\n        """Match features between two descriptor sets"""\n        if desc1 is None or desc2 is None:\n            return []\n\n        if method == \'bfm\':\n            return self._brute_force_matcher(desc1, desc2, ratio_threshold)\n        elif method == \'flann\':\n            return self._flann_matcher(desc1, desc2, ratio_threshold)\n\n    def _brute_force_matcher(self, desc1, desc2, ratio_threshold):\n        """Brute force feature matching"""\n        import cv2\n\n        # Create BFMatcher\n        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # Match descriptors\n        matches = bf.knnMatch(desc1, desc2, k=2)\n\n        # Apply ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < ratio_threshold * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\nclass VisualOdometry:\n    def __init__(self, camera_parameters):\n        self.camera = PinholeCamera(\n            camera_parameters[\'focal_length\'],\n            camera_parameters[\'image_width\'],\n            camera_parameters[\'image_height\']\n        )\n        self.feature_extractor = FeatureExtractor()\n        self.prev_image = None\n        self.prev_keypoints = None\n        self.prev_descriptors = None\n        self.pose = np.eye(4)  # Initial pose\n\n    def estimate_motion(self, current_image):\n        """Estimate camera motion from consecutive images"""\n        if self.prev_image is None:\n            # First frame\n            self.prev_image = current_image\n            self.prev_keypoints, self.prev_descriptors = self.feature_extractor.extract_features(current_image)\n            return self.pose\n\n        # Extract features from current frame\n        curr_keypoints, curr_descriptors = self.feature_extractor.extract_features(current_image)\n\n        # Match features\n        matches = self.feature_extractor.match_features(\n            self.prev_descriptors,\n            curr_descriptors\n        )\n\n        if len(matches) < 10:  # Not enough matches\n            return self.pose\n\n        # Extract matched keypoints\n        prev_pts = np.array([self.prev_keypoints[m.queryIdx].pt for m in matches])\n        curr_pts = np.array([curr_keypoints[m.trainIdx].pt for m in matches])\n\n        # Estimate motion using Essential matrix\n        E, mask = cv2.findEssentialMat(prev_pts, curr_pts, self.camera.intrinsic_matrix, method=cv2.RANSAC)\n\n        # Recover relative pose\n        _, R, t, mask = cv2.recoverPose(E, prev_pts, curr_pts, self.camera.intrinsic_matrix)\n\n        # Update pose\n        T = np.eye(4)\n        T[:3, :3] = R\n        T[:3, 3] = t.flatten()\n\n        self.pose = self.pose @ T\n\n        # Update previous frame\n        self.prev_image = current_image\n        self.prev_keypoints = curr_keypoints\n        self.prev_descriptors = curr_descriptors\n\n        return self.pose\n```\n\n## 13.3 Object Detection and Recognition\n\n### 13.3.1 Deep Learning for Object Detection\n\nModern object detection uses deep neural networks:\n\n```python\nclass YOLODetector:\n    def __init__(self, config_path, weights_path, class_names):\n        self.config_path = config_path\n        self.weights_path = weights_path\n        self.class_names = class_names\n        self.net = None\n        self.load_model()\n\n    def load_model(self):\n        """Load YOLO model"""\n        import cv2\n        import numpy as np\n\n        # Load network\n        self.net = cv2.dnn.readNetFromDarknet(self.config_path, self.weights_path)\n\n        # Use GPU if available\n        if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n\n    def detect_objects(self, image, confidence_threshold=0.5, nms_threshold=0.4):\n        """Detect objects in image"""\n        # Get output layer names\n        layer_names = self.net.getLayerNames()\n        output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n\n        # Prepare image\n        blob = cv2.dnn.blobFromImage(\n            image,\n            1/255.0,\n            (416, 416),\n            swapRB=True,\n            crop=False\n        )\n\n        # Forward pass\n        self.net.setInput(blob)\n        outputs = self.net.forward(output_layers)\n\n        # Process detections\n        boxes = []\n        confidences = []\n        class_ids = []\n\n        for output in outputs:\n            for detection in output:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > confidence_threshold:\n                    # Object detected\n                    center_x = int(detection[0] * image.shape[1])\n                    center_y = int(detection[1] * image.shape[0])\n                    width = int(detection[2] * image.shape[1])\n                    height = int(detection[3] * image.shape[0])\n\n                    # Rectangle coordinates\n                    x = int(center_x - width / 2)\n                    y = int(center_y - height / 2)\n\n                    boxes.append([x, y, width, height])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        # Apply Non-Maximum Suppression\n        indices = cv2.dnn.NMSBoxes(\n            boxes,\n            confidences,\n            confidence_threshold,\n            nms_threshold\n        )\n\n        detections = []\n        if len(indices) > 0:\n            for i in indices.flatten():\n                x, y, w, h = boxes[i]\n                confidence = confidences[i]\n                class_id = class_ids[i]\n                class_name = self.class_names[class_id]\n\n                detection = {\n                    \'bbox\': [x, y, x + w, y + h],\n                    \'confidence\': confidence,\n                    \'class_id\': class_id,\n                    \'class_name\': class_name\n                }\n                detections.append(detection)\n\n        return detections\n\nclass ObjectTracker:\n    def __init__(self, max_disappeared=50, max_distance=50):\n        self.next_object_id = 0\n        self.objects = {}\n        self.disappeared = {}\n        self.max_disappeared = max_disappeared\n        self.max_distance = max_distance\n\n    def register(self, centroid):\n        """Register new object"""\n        self.objects[self.next_object_id] = {\n            \'centroid\': centroid,\n            \'bbox\': None,\n            \'history\': [centroid],\n            \'class_name\': None\n        }\n        self.disappeared[self.next_object_id] = 0\n        self.next_object_id += 1\n\n    def deregister(self, object_id):\n        """Deregister object"""\n        del self.objects[object_id]\n        del self.disappeared[object_id]\n\n    def update(self, detections):\n        """Update tracking with new detections"""\n        if len(detections) == 0:\n            # Mark all existing objects as disappeared\n            for object_id in list(self.disappeared.keys()):\n                self.disappeared[object_id] += 1\n\n                if self.disappeared[object_id] > self.max_disappeared:\n                    self.deregister(object_id)\n\n            return self.objects\n\n        # Initialize new centroids\n        input_centroids = np.zeros((len(detections), 2), dtype="int")\n        input_bboxes = []\n\n        for (i, detection) in enumerate(detections):\n            x1, y1, x2, y2 = detection[\'bbox\']\n            cX = int((x1 + x2) / 2.0)\n            cY = int((y1 + y2) / 2.0)\n            input_centroids[i] = (cX, cY)\n            input_bboxes.append(detection[\'bbox\'])\n\n        # If no objects currently tracked, register all detections\n        if len(self.objects) == 0:\n            for i in range(len(input_centroids)):\n                self.register(input_centroids[i])\n\n        else:\n            # Get current object centroids\n            object_centroids = list(\n                obj[\'centroid\'] for obj in self.objects.values()\n            )\n\n            # Compute distance between each pair\n            D = dist.cdist(np.array(object_centroids), input_centroids)\n\n            # Find minimum distance for each object\n            rows = D.min(axis=1).argsort()\n            cols = D.argmin(axis=1)[rows]\n\n            # Keep track of used row and column indices\n            used_row_idxs = set()\n            used_col_idxs = set()\n\n            # Loop over the combination of the (row, column) index tuples\n            for (row, col) in zip(rows, cols):\n                # If we have already examined either the row or column, ignore it\n                if row in used_row_idxs or col in used_col_idxs:\n                    continue\n\n                # If distance is greater than maximum distance, do not associate\n                if D[row, col] > self.max_distance:\n                    continue\n\n                # Get object ID\n                object_id = list(self.objects.keys())[row]\n\n                # Update object centroid and bbox\n                self.objects[object_id][\'centroid\'] = input_centroids[col]\n                self.objects[object_id][\'bbox\'] = input_bboxes[col]\n                self.objects[object_id][\'history\'].append(input_centroids[col])\n\n                # Reset disappeared counter\n                self.disappeared[object_id] = 0\n\n                # Mark as used\n                used_row_idxs.add(row)\n                used_col_idxs.add(col)\n\n            # Compute unused row and column indices\n            unused_row_idxs = set(range(0, D.shape[0])).difference(used_row_idxs)\n            unused_col_idxs = set(range(0, D.shape[1])).difference(used_col_idxs)\n\n            # If objects disappeared > disappeared threshold, deregister them\n            if D.shape[0] >= D.shape[1]:\n                for row in unused_row_idxs:\n                    object_id = list(self.objects.keys())[row]\n                    self.disappeared[object_id] += 1\n\n                    if self.disappeared[object_id] > self.max_disappeared:\n                        self.deregister(object_id)\n\n            # Register new objects as needed\n            for col in unused_col_idxs:\n                self.register(input_centroids[col])\n\n        return self.objects\n```\n\n### 13.3.2 Semantic Segmentation\n\nPixel-level understanding of the scene:\n\n```python\nclass SemanticSegmentationNet:\n    def __init__(self, model_path, num_classes):\n        self.model_path = model_path\n        self.num_classes = num_classes\n        self.model = None\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.load_model()\n\n    def load_model(self):\n        """Load segmentation model"""\n        import torch\n        import torch.nn as nn\n\n        # Define model architecture (example: DeepLabV3)\n        self.model = models.segmentation.deeplabv3_resnet101(pretrained=False)\n        self.model.classifier[4] = nn.Conv2d(256, self.num_classes, kernel_size=1)\n        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))\n        self.model.to(self.device)\n        self.model.eval()\n\n    def predict(self, image):\n        """Perform semantic segmentation"""\n        import torch\n        import torchvision.transforms as transforms\n\n        # Preprocess image\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((512, 512)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        input_tensor = transform(image).unsqueeze(0).to(self.device)\n\n        # Predict\n        with torch.no_grad():\n            output = self.model(input_tensor)\n            predictions = output[\'out\'][0]\n\n        # Get segmentation mask\n        segmentation_mask = predictions.argmax(0).cpu().numpy()\n\n        return segmentation_mask\n\n    def visualize_segmentation(self, image, segmentation_mask, class_colors):\n        """Visualize segmentation results"""\n        import cv2\n        import numpy as np\n\n        # Resize mask to original image size\n        mask_resized = cv2.resize(segmentation_mask, (image.shape[1], image.shape[0]), interpolation=cv2.NEAREST)\n\n        # Create colored segmentation\n        colored_mask = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n\n        for class_id, color in class_colors.items():\n            colored_mask[mask_resized == class_id] = color\n\n        # Blend with original image\n        alpha = 0.5\n        result = cv2.addWeighted(image, 1 - alpha, colored_mask, alpha, 0)\n\n        return result\n```\n\n## 13.4 Visual SLAM\n\n### 13.4.1 SLAM Fundamentals\n\nSimultaneous Localization and Mapping (SLAM) estimates camera pose while building a map:\n\n```python\nclass VisualSLAM:\n    def __init__(self, camera_parameters):\n        self.camera = PinholeCamera(\n            camera_parameters[\'focal_length\'],\n            camera_parameters[\'image_width\'],\n            camera_parameters[\'image_height\']\n        )\n        self.feature_extractor = FeatureExtractor()\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # Map data structures\n        self.map_points = []\n        self.keyframes = []\n        self.current_pose = np.eye(4)\n\n        # SLAM parameters\n        self.min_matches = 10\n        self.min_triangulation_angle = 1.0  # degrees\n        self.reprojection_threshold = 3.0  # pixels\n\n    def process_frame(self, image, timestamp):\n        """Process new frame for SLAM"""\n        # Detect features\n        keypoints, descriptors = self.feature_extractor.extract_features(image, \'orb\', 1000)\n\n        if not self.keyframes:\n            # First frame - initialize map\n            self._initialize_map(image, keypoints, descriptors, timestamp)\n            return self.current_pose, self.map_points\n\n        # Match with last keyframe\n        last_keyframe = self.keyframes[-1]\n        matches = self._match_features(last_keyframe[\'descriptors\'], descriptors)\n\n        if len(matches) < self.min_matches:\n            # Tracking lost - relocalize\n            return self._relocalize(image, keypoints, descriptors)\n\n        # Estimate pose\n        pose, inliers = self._estimate_pose(\n            last_keyframe[\'keypoints\'],\n            keypoints,\n            matches\n        )\n\n        if pose is None:\n            return self._relocalize(image, keypoints, descriptors)\n\n        self.current_pose = pose\n\n        # Triangulate new map points\n        new_map_points = self._triangulate_points(\n            last_keyframe[\'keypoints\'],\n            keypoints,\n            matches,\n            last_keyframe[\'pose\'],\n            pose,\n            inliers\n        )\n\n        self.map_points.extend(new_map_points)\n\n        # Check if new keyframe is needed\n        if self._should_add_keyframe(image, keypoints, pose):\n            self._add_keyframe(image, keypoints, descriptors, pose, timestamp)\n\n        # Bundle adjustment (simplified)\n        if len(self.keyframes) % 10 == 0:\n            self._local_bundle_adjustment()\n\n        return self.current_pose, self.map_points\n\n    def _initialize_map(self, image, keypoints, descriptors, timestamp):\n        """Initialize map with first frame"""\n        # Create initial keyframe at origin\n        initial_pose = np.eye(4)\n\n        self.keyframes.append({\n            \'image\': image.copy(),\n            \'keypoints\': keypoints,\n            \'descriptors\': descriptors,\n            \'pose\': initial_pose,\n            \'timestamp\': timestamp\n        })\n\n        self.current_pose = initial_pose\n\n    def _match_features(self, desc1, desc2, ratio_threshold=0.75):\n        """Match features between two sets of descriptors"""\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < ratio_threshold * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\n    def _estimate_pose(self, kp1, kp2, matches):\n        """Estimate camera pose using PnP"""\n        if len(matches) < 4:\n            return None, None\n\n        # Extract matched keypoints\n        obj_points = []\n        img_points = []\n\n        for match in matches:\n            # Get corresponding 3D point if available\n            pt3d = self._get_3d_point_for_match(match)\n            if pt3d is not None:\n                obj_points.append(pt3d)\n                img_points.append(kp2[match.trainIdx].pt)\n\n        if len(obj_points) < 4:\n            return None, None\n\n        # Convert to numpy arrays\n        obj_points = np.array(obj_points, dtype=np.float32)\n        img_points = np.array(img_points, dtype=np.float32)\n\n        # Solve PnP\n        success, rvec, tvec, inliers = cv2.solvePnPRansac(\n            obj_points,\n            img_points,\n            self.camera.intrinsic_matrix,\n            None,\n            iterationsCount=1000,\n            reprojectionError=self.reprojection_threshold,\n            confidence=0.99\n        )\n\n        if success:\n            # Convert rotation vector to matrix\n            R, _ = cv2.Rodrigues(rvec)\n\n            # Create pose matrix\n            pose = np.eye(4)\n            pose[:3, :3] = R\n            pose[:3, 3] = tvec.flatten()\n\n            return pose, inliers\n\n        return None, None\n\n    def _triangulate_points(self, kp1, kp2, matches, pose1, pose2, inliers):\n        """Triangulate 3D points from matched keypoints"""\n        if inliers is not None:\n            matches = [matches[i] for i in inliers.flatten()]\n\n        new_points = []\n\n        for match in matches:\n            # Get corresponding keypoints\n            pt1 = np.array(kp1[match.queryIdx].pt)\n            pt2 = np.array(kp2[match.trainIdx].pt)\n\n            # Triangulate\n            point_3d = self._triangulate_point(pt1, pt2, pose1, pose2)\n\n            if point_3d is not None:\n                new_points.append({\n                    \'position\': point_3d,\n                    \'observations\': [\n                        {\n                            \'keyframe_id\': len(self.keyframes) - 1,\n                            \'keypoint_idx\': match.queryIdx,\n                            \'pixel_coord\': pt1\n                        },\n                        {\n                            \'keyframe_id\': len(self.keyframes),  # Current frame\n                            \'keypoint_idx\': match.trainIdx,\n                            \'pixel_coord\': pt2\n                        }\n                    ]\n                })\n\n        return new_points\n\n    def _triangulate_point(self, pt1, pt2, pose1, pose2):\n        """Triangulate a single 3D point"""\n        # Camera matrices\n        P1 = self.camera.intrinsic_matrix @ pose1[:3, :]\n        P2 = self.camera.intrinsic_matrix @ pose2[:3, :]\n\n        # DLT triangulation\n        A = np.zeros((4, 4))\n\n        for i in range(2):\n            A[i, :] = pt1[i] * P1[2, :] - P1[i, :]\n            A[i + 2, :] = pt2[i] * P2[2, :] - P2[i, :]\n\n        # Solve using SVD\n        _, _, Vt = np.linalg.svd(A)\n        X = Vt[-1, :]\n\n        # Convert from homogeneous to 3D\n        if X[3] != 0:\n            X = X[:3] / X[3]\n\n            # Check triangulation angle\n            ray1 = pose1[:3, 3] - X\n            ray2 = pose2[:3, 3] - X\n\n            cos_angle = np.dot(ray1, ray2) / (np.linalg.norm(ray1) * np.linalg.norm(ray2))\n            angle = np.arccos(np.clip(cos_angle, -1, 1))\n\n            if angle > np.radians(self.min_triangulation_angle):\n                # Check reprojection error\n                error1 = self._calculate_reprojection_error(X, pt1, P1)\n                error2 = self._calculate_reprojection_error(X, pt2, P2)\n\n                if error1 < self.reprojection_threshold and error2 < self.reprojection_threshold:\n                    return X\n\n        return None\n\n    def _calculate_reprojection_error(self, point_3d, point_2d, P):\n        """Calculate reprojection error"""\n        # Project 3D point to image\n        projected = P @ np.append(point_3d, 1)\n        projected = projected[:2] / projected[2]\n\n        # Calculate error\n        error = np.linalg.norm(projected - point_2d)\n        return error\n\n    def _local_bundle_adjustment(self):\n        """Perform local bundle adjustment"""\n        # This is a simplified version\n        # In practice, use g2o or Ceres Solver for full BA\n\n        recent_keyframes = self.keyframes[-10:]  # Last 10 keyframes\n\n        print(f"Performing local bundle adjustment on {len(recent_keyframes)} keyframes")\n\n        # Update poses using recent observations\n        for keyframe in recent_keyframes:\n            # Simple pose refinement based on 3D-2D correspondences\n            refined_pose = self._refine_pose(keyframe)\n            keyframe[\'pose\'] = refined_pose\n\n    def _refine_pose(self, keyframe):\n        """Refine keyframe pose"""\n        # Get 3D-2D correspondences\n        obj_points = []\n        img_points = []\n\n        for map_point in self.map_points:\n            for obs in map_point[\'observations\']:\n                if obs[\'keyframe_id\'] == keyframe[\'timestamp\']:\n                    obj_points.append(map_point[\'position\'])\n                    img_points.append(obs[\'pixel_coord\'])\n\n        if len(obj_points) >= 6:\n            obj_points = np.array(obj_points, dtype=np.float32)\n            img_points = np.array(img_points, dtype=np.float32)\n\n            # Refine pose\n            success, rvec, tvec = cv2.solvePnP(\n                obj_points,\n                img_points,\n                self.camera.intrinsic_matrix,\n                None,\n                flags=cv2.SOLVEPNP_ITERATIVE\n            )\n\n            if success:\n                R, _ = cv2.Rodrigues(rvec)\n                pose = np.eye(4)\n                pose[:3, :3] = R\n                pose[:3, 3] = tvec.flatten()\n\n                return pose\n\n        return keyframe[\'pose\']\n```\n\n## Chapter Summary\n\nThis chapter covered comprehensive computer vision techniques for robotic applications:\n\n### Key Concepts Covered\n1. **Image Formation**: Pinhole camera model and lens distortion correction\n2. **Image Processing**: Preprocessing, edge detection, and feature extraction\n3. **Object Detection**: YOLO-based detection with tracking capabilities\n4. **Semantic Segmentation**: Pixel-level scene understanding\n5. **Visual SLAM**: Simultaneous localization and mapping\n6. **Feature Matching**: Robust feature detection and matching algorithms\n\n### Practical Implementations\n- Complete camera model with distortion correction\n- ORB feature extractor with descriptor matching\n- Visual odometry using essential matrix decomposition\n- YOLO object detector with multi-object tracking\n- Semantic segmentation with DeepLabV3\n- Complete Visual SLAM pipeline with bundle adjustment\n\n### Next Steps\nWith computer vision mastery, you\'re ready for:\n- Chapter 14: Sensor Fusion and State Estimation\n- Chapter 15: SLAM, VSLAM, and Navigation\n- Chapter 16: Path Planning Algorithms\n\n---\n\n## Glossary Terms\n\n**Term**: **Pinhole Camera Model**\n**Definition**: Mathematical model describing how 3D points in the world are projected onto a 2D image plane through a single point (the pinhole)\n**Related**: **Camera Calibration**, **Extrinsic Parameters**\n\n**Term**: **Essential Matrix**\n**Definition**: 3x3 matrix that relates corresponding points in two images taken by cameras with known intrinsic parameters\n**Related**: **Fundamental Matrix**, **Five-Point Algorithm**\n\n**Term**: **Non-Maximum Suppression**\n**Definition**: Algorithm used in edge detection to thin edges by keeping only local maxima in gradient magnitude\n**Related**: **Edge Detection**, **Canny Operator**\n\n**Term**: **Visual Odometry**\n**Definition**: Process of estimating camera pose motion by analyzing sequential images and tracking feature movements\n**Related**: **SLAM**, **Structure from Motion**\n\n**Term**: **Bundle Adjustment**\n**Definition**: Optimization technique that simultaneously refines 3D structure and camera poses to minimize reprojection error\n**Related**: **SLAM**, **Photogrammetry**\n\n---\n\n## Exercises\n\n### Exercise 13.1: Camera Calibration\nImplement camera calibration system:\n- Capture calibration pattern images\n- Detect chessboard corners\n- Calculate intrinsic and extrinsic parameters\n- Validate calibration accuracy\n\n### Exercise 13.2: Feature Detection Pipeline\nBuild complete feature detection system:\n- Implement Harris corner detector\n- Add SIFT feature extraction\n- Create robust feature matching\n- Test with different image conditions\n\n### Exercise 13.3: Object Detection System\nCreate object detection and tracking:\n- Train YOLO model for custom objects\n- Implement multi-object tracking\n- Handle occlusions and disappearances\n- Evaluate detection performance\n\n### Exercise 13.4: Visual SLAM Implementation\nBuild basic visual SLAM system:\n- Implement feature-based SLAM\n- Add loop closure detection\n- Create map visualization\n- Test in real environments\n\n### Exercise 13.5: Semantic Segmentation\nDevelop semantic segmentation for robotics:\n- Implement pixel-wise classification\n- Create class-specific color mapping\n- Integrate with navigation system\n- Evaluate segmentation accuracy'})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);